{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5502748b-524c-4d49-9eb6-ed13af2f9b43",
   "metadata": {},
   "source": [
    "# WILDFIRESAI - DATA PIPELINE (Base Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2ac49-0656-4dee-afe9-94c04ca653e8",
   "metadata": {},
   "source": [
    "# ==== Cell 1 — Setup (one-time dependency install) ====\n",
    " Purpose:\n",
    "   Install and validate core dependencies required by WildfiresAI.\n",
    "   Ensure modules are importable without kernel restart.\n",
    "# ---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f989025-05ba-496a-8094-f8d66ba2571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Installed modules status]\n",
      " - pystac_client: 0.9.0\n",
      " - planetary_computer: 1.0.0\n",
      " - mp_api: ok\n",
      " - pymatgen: ok\n",
      " - pyarrow: 21.0.0\n",
      " - tqdm: 4.67.1\n",
      " - structlog: 25.4.0\n",
      "\n",
      " Setup complete. All modules reloaded dynamically.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1 — Setup (one-time dependency install) ====\n",
    "\n",
    "import importlib, site, subprocess, sys\n",
    "\n",
    "# --- Silent dependency installation ---\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\",\n",
    "         \"pystac-client\", \"planetary-computer\", \"mp-api\",\n",
    "         \"pymatgen\", \"pyarrow\", \"tqdm\", \"structlog\"],\n",
    "        check=True\n",
    "    )\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"  Pip installation failed: {e}\")\n",
    "\n",
    "# --- Refresh Python import cache ---\n",
    "importlib.invalidate_caches()\n",
    "site.addsitedir(site.getsitepackages()[-1])\n",
    "\n",
    "# --- Immediate import verification ---\n",
    "modules = [\n",
    "    \"pystac_client\", \"planetary_computer\",\n",
    "    \"mp_api\", \"pymatgen\", \"pyarrow\", \"tqdm\", \"structlog\"\n",
    "]\n",
    "loaded = {}\n",
    "for m in modules:\n",
    "    try:\n",
    "        mod = importlib.import_module(m)\n",
    "        loaded[m] = getattr(mod, \"__version__\", \"ok\")\n",
    "    except Exception as e:\n",
    "        loaded[m] = f\"⚠️ import failed ({e.__class__.__name__})\"\n",
    "\n",
    "print(\"[Installed modules status]\")\n",
    "for k, v in loaded.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "print(\"\\n Setup complete. All modules reloaded dynamically.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8042b-0c8a-4da1-8a61-ea3b18c0680a",
   "metadata": {},
   "source": [
    "# ==== Cell 2 — Scientific / Infra Stack & Version Audit ====\n",
    " Purpose:\n",
    "   Establish project-wide environment paths and verify\n",
    "   availability & versions of the core scientific stack.\n",
    "# ---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a182380-2581-4302-801a-b82a8c7d4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] Python 3.13.5 | Platform: Darwin 24.6.0\n",
      "[ENV] Project root: /Users/evareysanchez/WildfiresAI\n",
      "[ENV] Data folders ready: {'raw': True, 'processed': True, 'reports': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5b7f4\">\n",
       "  <caption>Core Scientific Stack Versions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5b7f4_level0_col0\" class=\"col_heading level0 col0\" >pandas</th>\n",
       "      <th id=\"T_5b7f4_level0_col1\" class=\"col_heading level0 col1\" >numpy</th>\n",
       "      <th id=\"T_5b7f4_level0_col2\" class=\"col_heading level0 col2\" >requests</th>\n",
       "      <th id=\"T_5b7f4_level0_col3\" class=\"col_heading level0 col3\" >geopandas</th>\n",
       "      <th id=\"T_5b7f4_level0_col4\" class=\"col_heading level0 col4\" >rasterio</th>\n",
       "      <th id=\"T_5b7f4_level0_col5\" class=\"col_heading level0 col5\" >shapely</th>\n",
       "      <th id=\"T_5b7f4_level0_col6\" class=\"col_heading level0 col6\" >matplotlib</th>\n",
       "      <th id=\"T_5b7f4_level0_col7\" class=\"col_heading level0 col7\" >tqdm</th>\n",
       "      <th id=\"T_5b7f4_level0_col8\" class=\"col_heading level0 col8\" >sklearn</th>\n",
       "      <th id=\"T_5b7f4_level0_col9\" class=\"col_heading level0 col9\" >torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5b7f4_level0_row0\" class=\"row_heading level0 row0\" >version</th>\n",
       "      <td id=\"T_5b7f4_row0_col0\" class=\"data row0 col0\" >2.3.2</td>\n",
       "      <td id=\"T_5b7f4_row0_col1\" class=\"data row0 col1\" >2.3.3</td>\n",
       "      <td id=\"T_5b7f4_row0_col2\" class=\"data row0 col2\" >2.32.4</td>\n",
       "      <td id=\"T_5b7f4_row0_col3\" class=\"data row0 col3\" >1.1.1</td>\n",
       "      <td id=\"T_5b7f4_row0_col4\" class=\"data row0 col4\" >1.4.3</td>\n",
       "      <td id=\"T_5b7f4_row0_col5\" class=\"data row0 col5\" >2.1.2</td>\n",
       "      <td id=\"T_5b7f4_row0_col6\" class=\"data row0 col6\" >3.10.6</td>\n",
       "      <td id=\"T_5b7f4_row0_col7\" class=\"data row0 col7\" >4.67.1</td>\n",
       "      <td id=\"T_5b7f4_row0_col8\" class=\"data row0 col8\" >1.7.2</td>\n",
       "      <td id=\"T_5b7f4_row0_col9\" class=\"data row0 col9\" >2.8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d149a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Environment and version audit completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 2 — Scientific / Infra Stack & Version Audit ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, platform, importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Project root & directory structure ---\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR      = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "for folder in (RAW_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[ENV] Python {platform.python_version()} | Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"[ENV] Project root: {PROJECT_ROOT}\")\n",
    "print(f\"[ENV] Data folders ready:\",\n",
    "      {d.name: d.exists() for d in (RAW_DIR, PROCESSED_DIR, REPORTS_DIR)})\n",
    "\n",
    "# --- Scientific stack audit ---\n",
    "core_packages = [\n",
    "    \"pandas\", \"numpy\", \"requests\", \"geopandas\", \"rasterio\",\n",
    "    \"shapely\", \"matplotlib\", \"tqdm\", \"sklearn\", \"torch\"\n",
    "]\n",
    "\n",
    "versions = {}\n",
    "for pkg in core_packages:\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        versions[pkg] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        versions[pkg] = f\"not installed ({e.__class__.__name__})\"\n",
    "\n",
    "df_versions = pd.DataFrame.from_dict(versions, orient=\"index\", columns=[\"version\"])\n",
    "display(df_versions.T.style.set_caption(\"Core Scientific Stack Versions\"))\n",
    "\n",
    "print(\"\\n Environment and version audit completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd76cf8-1e53-40ae-bc11-af238357f6c5",
   "metadata": {},
   "source": [
    "# ==== Cell 2.1 — Keys & Connectivity Audit ====\n",
    "\"\"\"\n",
    "Purpose:\n",
    "- Verify API keys and data endpoints availability.\n",
    "- Perform lightweight connectivity and authorization checks.\n",
    "- Produce a structured JSON report saved to /reports/connectivity_report.json\n",
    "- Includes token-authenticated NASA FIRMS verification.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526b657f-1c86-468d-8c6e-7ffc15a565df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] Region/Window: ES None → None\n",
      "[ENV] FGPL Mode: REGIONAL\n",
      "[ENV] Keys (masked): OPENAI= sk-pro… MP= U8Wg4j… OPENTOPO= 9d7124… NASA_FIRMS_TOKEN= eyJ0eX… EARTHDATA_TOKEN= eyJ0eX…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3443724b3db84a95a74295780a13b451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Connectivity Report]\n",
      "{\n",
      "  \"materials_project\": {\n",
      "    \"ok\": true,\n",
      "    \"sample_docs\": 1\n",
      "  },\n",
      "  \"opentopo\": {\n",
      "    \"ok\": true,\n",
      "    \"status_code\": 200\n",
      "  },\n",
      "  \"open_meteo\": {\n",
      "    \"ok\": true\n",
      "  },\n",
      "  \"firms\": {\n",
      "    \"ok\": true,\n",
      "    \"total_detected\": 32,\n",
      "    \"validated\": 32,\n",
      "    \"failed\": 0,\n",
      "    \"map_key_used\": true\n",
      "  },\n",
      "  \"effis\": {\n",
      "    \"ok\": false,\n",
      "    \"reason\": \"EFFIS error: ConnectionError: HTTPSConnectionPool(host='forest-fire.jrc.ec.europa.eu', port=443): Max retries exceeded with url: /geoserver/EFFIS/FIRE_HISTORICAL_BURNT_AREA/ows?service=WFS&request=GetCapabilities&version=2.0.0 (Caused by NameResolutionError(\\\"<urllib3.connection.HTTPSConnection object at 0x13ea4c7d0>: Failed to resolve 'forest-fire.jrc.ec.europa.eu' ([Errno 8] nodename nor servname provided, or not known)\\\"))\"\n",
      "  },\n",
      "  \"nifc\": {\n",
      "    \"ok\": true,\n",
      "    \"status_code\": 200\n",
      "  },\n",
      "  \"earthdata_creds\": {\n",
      "    \"ok\": true\n",
      "  }\n",
      "}\n",
      "\n",
      " Connectivity report saved to /Users/evareysanchez/WildfiresAI/reports/connectivity_report.json\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 2.1 — Keys & Connectivity Audit ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, requests\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Utility helpers\n",
    "# -------------------------------------------------------------------------\n",
    "def mask(key: str, keep: int = 6) -> str:\n",
    "    \"\"\"Return masked key representation for safe console output.\"\"\"\n",
    "    if not key:\n",
    "        return \"None\"\n",
    "    return key[:keep] + \"…\" if len(key) > keep else \"***\"\n",
    "\n",
    "def ok(status: bool, reason: str | None = None, extra: dict | None = None) -> dict:\n",
    "    \"\"\"Uniform verdict structure for connectivity tests.\"\"\"\n",
    "    out = {\"ok\": bool(status)}\n",
    "    if reason:\n",
    "        out[\"reason\"] = reason\n",
    "    if extra:\n",
    "        out.update(extra)\n",
    "    return out\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Environment collection\n",
    "# -------------------------------------------------------------------------\n",
    "env = {\n",
    "    \"WF_REGION\": os.getenv(\"WF_REGION\", \"ES\"),\n",
    "    \"WF_DATE_FROM\": os.getenv(\"WF_DATE_FROM\"),\n",
    "    \"WF_DATE_TO\": os.getenv(\"WF_DATE_TO\"),\n",
    "    \"FGPL_MODE\": os.getenv(\"FGPL_MODE\", \"REGIONAL\").upper(),\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "    \"MP_API_KEY\": os.getenv(\"MP_API_KEY\", \"\"),\n",
    "    \"OPENTOPO_API_KEY\": os.getenv(\"OPENTOPO_API_KEY\", \"\"),\n",
    "    \"NASA_FIRMS_TOKEN\": os.getenv(\"NASA_FIRMS_TOKEN\", \"\"),\n",
    "    \"FIRMS_CSV_URL\": os.getenv(\"FIRMS_CSV_URL\", \"\"),\n",
    "    \"EFFIS_WFS_URL\": os.getenv(\"EFFIS_WFS_URL\", \"\"),\n",
    "    \"EFFIS_TYPENAME\": os.getenv(\"EFFIS_TYPENAME\", \"\"),\n",
    "    \"NIFC_FS_URL\": os.getenv(\"NIFC_FS_URL\",\n",
    "        \"https://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services/\"\n",
    "        \"WFIGS_Interagency_Perimeters_Current/FeatureServer/0\"),\n",
    "    \"EARTHDATA_USERNAME\": os.getenv(\"EARTHDATA_USERNAME\", \"\"),\n",
    "    \"EARTHDATA_PASSWORD\": os.getenv(\"EARTHDATA_PASSWORD\", \"\"),\n",
    "    \"EARTHDATA_TOKEN\": os.getenv(\"EARTHDATA_TOKEN\", \"\"),\n",
    "}\n",
    "\n",
    "print(f\"[ENV] Region/Window: {env['WF_REGION']} {env['WF_DATE_FROM']} → {env['WF_DATE_TO']}\")\n",
    "print(f\"[ENV] FGPL Mode: {env['FGPL_MODE']}\")\n",
    "print(\"[ENV] Keys (masked):\",\n",
    "      \"OPENAI=\", mask(env[\"OPENAI_API_KEY\"]),\n",
    "      \"MP=\", mask(env[\"MP_API_KEY\"]),\n",
    "      \"OPENTOPO=\", mask(env[\"OPENTOPO_API_KEY\"]),\n",
    "      \"NASA_FIRMS_TOKEN=\", mask(env[\"NASA_FIRMS_TOKEN\"]),\n",
    "      \"EARTHDATA_TOKEN=\", mask(env[\"EARTHDATA_TOKEN\"]))\n",
    "\n",
    "report: dict[str, dict] = {}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Materials Project (mp-api)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    from mp_api.client import MPRester\n",
    "    if env[\"MP_API_KEY\"]:\n",
    "        with MPRester(env[\"MP_API_KEY\"]) as mpr:\n",
    "            docs = mpr.materials.summary.search(fields=[\"material_id\"], energy_above_hull=(0, 0.1),\n",
    "                                                chunk_size=1, num_chunks=1)\n",
    "        report[\"materials_project\"] = ok(True, extra={\"sample_docs\": len(docs)})\n",
    "    else:\n",
    "        report[\"materials_project\"] = ok(False, \"MP_API_KEY missing\")\n",
    "except Exception as e:\n",
    "    report[\"materials_project\"] = ok(False, f\"mp-api error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) OpenTopography (DEM)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    if env[\"OPENTOPO_API_KEY\"]:\n",
    "        url = \"https://portal.opentopography.org/API/globaldem\"\n",
    "        params = dict(demtype=\"SRTMGL3\", south=40.0, north=40.1,\n",
    "                      west=-3.8, east=-3.7, outputFormat=\"GTiff\",\n",
    "                      API_Key=env[\"OPENTOPO_API_KEY\"])\n",
    "        r = requests.get(url, params=params, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        report[\"opentopo\"] = ok(True, extra={\"status_code\": r.status_code})\n",
    "    else:\n",
    "        report[\"opentopo\"] = ok(False, \"OPENTOPO_API_KEY missing\")\n",
    "except Exception as e:\n",
    "    report[\"opentopo\"] = ok(False, f\"OpenTopography error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Open-Meteo\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    today = date.today()\n",
    "    params = dict(latitude=40.0, longitude=-3.7,\n",
    "                  start_date=(today - timedelta(days=2)).isoformat(),\n",
    "                  end_date=(today - timedelta(days=1)).isoformat(),\n",
    "                  hourly=\"temperature_2m\", timezone=\"UTC\")\n",
    "    r = requests.get(\"https://archive-api.open-meteo.com/v1/archive\",\n",
    "                     params=params, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    ok_struct = \"hourly\" in js and \"time\" in js[\"hourly\"]\n",
    "    report[\"open_meteo\"] = ok(ok_struct, None if ok_struct else \"unexpected JSON\")\n",
    "except Exception as e:\n",
    "    report[\"open_meteo\"] = ok(False, f\"Open-Meteo error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) NASA FIRMS (multi-source detection, MAP_KEY + token auth)\n",
    "# -------------------------------------------------------------------------\n",
    "firms_env_vars = {k: v for k, v in os.environ.items() if k.startswith(\"FIRMS_\") and v.startswith(\"https\")}\n",
    "token = os.getenv(\"NASA_FIRMS_TOKEN\", \"\")\n",
    "map_key = os.getenv(\"FIRMS_MAP_KEY\", \"\")\n",
    "\n",
    "if not firms_env_vars:\n",
    "    report[\"firms\"] = ok(False, \"No FIRMS_* URLs found in environment\")\n",
    "else:\n",
    "    valid, failed = 0, 0\n",
    "    for name, url in firms_env_vars.items():\n",
    "        try:\n",
    "            headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "            params = {\"MAP_KEY\": map_key} if map_key else {}\n",
    "            if map_key and \"MAP_KEY=\" not in url:\n",
    "                sep = \"&\" if \"?\" in url else \"?\"\n",
    "                url = f\"{url}{sep}MAP_KEY={map_key}\"\n",
    "            r = requests.head(url, headers=headers, params=params, timeout=10)\n",
    "            if r.status_code >= 400:\n",
    "                r = requests.get(url, headers=headers, params=params, timeout=10)\n",
    "            if r.ok:\n",
    "                valid += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"[FIRMS ⚠️] {name} failed with {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"[FIRMS ❌] {name} -> {e.__class__.__name__}: {e}\")\n",
    "    report[\"firms\"] = ok(valid > 0, extra={\n",
    "        \"total_detected\": len(firms_env_vars),\n",
    "        \"validated\": valid,\n",
    "        \"failed\": failed,\n",
    "        \"map_key_used\": bool(map_key)\n",
    "    })\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) EFFIS WFS\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    if env[\"EFFIS_WFS_URL\"] and env[\"EFFIS_TYPENAME\"]:\n",
    "        r = requests.get(env[\"EFFIS_WFS_URL\"],\n",
    "                         params={\"service\": \"WFS\", \"request\": \"GetCapabilities\", \"version\": \"2.0.0\"},\n",
    "                         timeout=20)\n",
    "        report[\"effis\"] = ok(r.ok, extra={\"status_code\": r.status_code})\n",
    "    else:\n",
    "        report[\"effis\"] = ok(False, \"EFFIS_WFS_URL or EFFIS_TYPENAME missing\")\n",
    "except Exception as e:\n",
    "    report[\"effis\"] = ok(False, f\"EFFIS error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) NIFC FeatureServer\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    base = env[\"NIFC_FS_URL\"].rstrip(\"/\")\n",
    "    r = requests.get(f\"{base}?f=json\", timeout=20)\n",
    "    r.raise_for_status()\n",
    "    report[\"nifc\"] = ok(True, extra={\"status_code\": r.status_code})\n",
    "except Exception as e:\n",
    "    report[\"nifc\"] = ok(False, f\"NIFC error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 7) NASA Earthdata credentials (username/password or token)\n",
    "# -------------------------------------------------------------------------\n",
    "has_earthdata = (\n",
    "    bool(env.get(\"EARTHDATA_USERNAME\") and env.get(\"EARTHDATA_PASSWORD\"))\n",
    "    or bool(env.get(\"EARTHDATA_TOKEN\"))\n",
    ")\n",
    "report[\"earthdata_creds\"] = ok(\n",
    "    has_earthdata,\n",
    "    None if has_earthdata else \"EARTHDATA credentials or token missing\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Final structured output\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\n[Connectivity Report]\")\n",
    "print(json.dumps(report, indent=2, ensure_ascii=False))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Persist report\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    REPORTS_DIR = Path.cwd() / \"reports\"\n",
    "    REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = REPORTS_DIR / \"connectivity_report.json\"\n",
    "    out_path.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(f\"\\n Connectivity report saved to {out_path}\")\n",
    "except Exception as e:\n",
    "    print(f\" Failed to save connectivity report: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f398528-0bdb-4279-ac08-5c62fb7c1852",
   "metadata": {},
   "source": [
    "## Cell 3 – Project Header & Global Configuration\n",
    "Defines project paths, environment variables, logging, and small I/O helpers.  \n",
    "Ensures reproducibility, consistent data handling, and clean outputs across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2998e9a1-6061-4a55-b5d0-432d35a53431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-10-21 19:59:40\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mInit config                   \u001b[0m \u001b[36mmode\u001b[0m=\u001b[35mGLOBAL\u001b[0m \u001b[36mprocessed\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI/data/processed')\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mGLOBAL\u001b[0m \u001b[36mroot\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI')\u001b[0m\n",
      "────────────────────────────────────────────────────────────\n",
      "WildfiresAI Global Configuration\n",
      "────────────────────────────────────────────────────────────\n",
      "Mode: GLOBAL | Region: GLOBAL\n",
      "Bounding Box: GLOBAL coverage (no spatial restriction)\n",
      "Paths: raw=raw, processed=processed, reports=reports\n",
      "Secrets: OpenAI=sk-pro…, FIRMS_TOKEN=eyJ0eX…, MAP_KEY=27f8d7…\n",
      "────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3: Project Header & Global Configuration (Global-Aware) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import structlog\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1️⃣ Project directories (shared across frameworks)\n",
    "# -------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "\n",
    "for p in (DATA_DIR, RAW_DIR, PROCESSED_DIR, REPORTS_DIR, CONFIG_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2️⃣ Environment variables and region mode\n",
    "# -------------------------------------------------------------------------\n",
    "load_dotenv()  # Load variables from .env file or conda env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "FIRMS_TOKEN    = os.getenv(\"FIRMS_TOKEN\", \"\")\n",
    "FIRMS_MAP_KEY  = os.getenv(\"FIRMS_MAP_KEY\", \"\")\n",
    "WF_REGION      = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "FGPL_MODE      = os.getenv(\"FGPL_MODE\", \"GLOBAL\").upper()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3️⃣ Spatial configuration\n",
    "# -------------------------------------------------------------------------\n",
    "# Default bounding box for Spain (used only if WF_REGION == \"ES\")\n",
    "SPAIN_BBOX = (-9.5, 35.0, 3.5, 43.9)\n",
    "\n",
    "# Global configuration (no bounding box restriction)\n",
    "if WF_REGION == \"GLOBAL\":\n",
    "    ACTIVE_BBOX = None\n",
    "else:\n",
    "    ACTIVE_BBOX = SPAIN_BBOX\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4️⃣ Structured logging setup\n",
    "# -------------------------------------------------------------------------\n",
    "structlog.configure(wrapper_class=structlog.make_filtering_bound_logger(20))  # INFO level\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(region=WF_REGION, mode=FGPL_MODE)\n",
    "log.info(\"Init config\", root=PROJECT_ROOT, processed=PROCESSED_DIR, mode=FGPL_MODE)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5️⃣ Utility helpers\n",
    "# -------------------------------------------------------------------------\n",
    "def mask_key(key: str, n: int = 6) -> str:\n",
    "    \"\"\"Mask sensitive keys, keeping only first n chars.\"\"\"\n",
    "    return key[:n] + \"…\" if key else \"None\"\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to CSV (creates parent dirs if needed).\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    log.info(\"Saved CSV\", path=str(path))\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to Parquet (creates parent dirs if needed).\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "    log.info(\"Saved Parquet\", path=str(path))\n",
    "\n",
    "def preview(df: pd.DataFrame, n: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"Return the first n rows for consistent preview.\"\"\"\n",
    "    return df.head(n)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6️⃣ Effective configuration echo\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"────────────────────────────────────────────────────────────\")\n",
    "print(\"WildfiresAI Global Configuration\")\n",
    "print(\"────────────────────────────────────────────────────────────\")\n",
    "print(f\"Mode: {FGPL_MODE} | Region: {WF_REGION}\")\n",
    "if ACTIVE_BBOX:\n",
    "    print(f\"Bounding Box: {ACTIVE_BBOX}\")\n",
    "else:\n",
    "    print(\"Bounding Box: GLOBAL coverage (no spatial restriction)\")\n",
    "print(f\"Paths: raw={RAW_DIR.name}, processed={PROCESSED_DIR.name}, reports={REPORTS_DIR.name}\")\n",
    "print(f\"Secrets: OpenAI={mask_key(OPENAI_API_KEY)}, FIRMS_TOKEN={mask_key(FIRMS_TOKEN)}, MAP_KEY={mask_key(FIRMS_MAP_KEY)}\")\n",
    "print(\"────────────────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20662a3-8a48-4345-a0f4-66493588d89b",
   "metadata": {},
   "source": [
    "# ==== Cell 3.1: Smart Date Synchronization (Auto Window Detection, Global-Aware) ====\n",
    "# -------------------------------------------------------------------------\n",
    " Purpose:\n",
    " Automatically detect or initialize temporal window for global datasets.\n",
    " Ensures WF_DATE_FROM / WF_DATE_TO are synchronized across .env, system env,\n",
    " and global pipeline state (GLOBAL or REGIONAL modes).\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d155a995-b5f8-47b4-a777-85733d8b100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "───────────────────────────────────────────────\n",
      " WildfiresAI — Smart Date Synchronization\n",
      "───────────────────────────────────────────────\n",
      "Mode: GLOBAL | Region: GLOBAL\n",
      "WF_DATE_FROM = 2025-10-14\n",
      "WF_DATE_TO   = 2025-10-21\n",
      "Active window: 7 days\n",
      "No local FIRMS data found — using default 7-day window.\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3.1: Smart Date Synchronization (UTC-safe, Global-Aware) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dotenv import set_key\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "ENV_PATH = PROJECT_ROOT / \".env\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1️⃣ Detect time window from existing FIRMS data (if available)\n",
    "# -------------------------------------------------------------------------\n",
    "firms_files = sorted(RAW_DIR.glob(\"firms_viirs_snpp_*.csv\"))\n",
    "\n",
    "if firms_files:\n",
    "    latest = firms_files[-1].name\n",
    "    # Example: firms_viirs_snpp_2025-09-23_2025-09-30.csv\n",
    "    parts = latest.split(\"_\")\n",
    "    if len(parts) >= 5:\n",
    "        date_from = parts[3]\n",
    "        date_to = parts[4].replace(\".csv\", \"\")\n",
    "    else:\n",
    "        now = datetime.now(timezone.utc).date()\n",
    "        date_to = now.isoformat()\n",
    "        date_from = (now - timedelta(days=7)).isoformat()\n",
    "else:\n",
    "    # No local data → default: past 7 days (GLOBAL mode friendly)\n",
    "    now = datetime.now(timezone.utc).date()\n",
    "    date_to = now.isoformat()\n",
    "    date_from = (now - timedelta(days=7)).isoformat()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2️⃣ Override by environment (if explicitly provided)\n",
    "# -------------------------------------------------------------------------\n",
    "if os.getenv(\"WF_DATE_FROM\"):\n",
    "    date_from = os.getenv(\"WF_DATE_FROM\")\n",
    "if os.getenv(\"WF_DATE_TO\"):\n",
    "    date_to = os.getenv(\"WF_DATE_TO\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3️⃣ Persist to .env and runtime\n",
    "# -------------------------------------------------------------------------\n",
    "os.environ[\"WF_DATE_FROM\"] = date_from\n",
    "os.environ[\"WF_DATE_TO\"] = date_to\n",
    "set_key(str(ENV_PATH), \"WF_DATE_FROM\", date_from)\n",
    "set_key(str(ENV_PATH), \"WF_DATE_TO\", date_to)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4️⃣ Smart context summary\n",
    "# -------------------------------------------------------------------------\n",
    "mode = os.getenv(\"FGPL_MODE\", \"GLOBAL\").upper()\n",
    "region = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "window_days = (datetime.fromisoformat(date_to) - datetime.fromisoformat(date_from)).days\n",
    "\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\" WildfiresAI — Smart Date Synchronization\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Mode: {mode} | Region: {region}\")\n",
    "print(f\"WF_DATE_FROM = {date_from}\")\n",
    "print(f\"WF_DATE_TO   = {date_to}\")\n",
    "print(f\"Active window: {window_days} days\")\n",
    "if firms_files:\n",
    "    print(f\"Detected from local FIRMS file: {firms_files[-1].name}\")\n",
    "else:\n",
    "    print(\"No local FIRMS data found — using default 7-day window.\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c7b5e-ba9f-4d5c-9986-af3412ab93a1",
   "metadata": {},
   "source": [
    "# ==== WildfiresAI — Cell 3.2: OpenAI API Integration (Global Adaptive) ====\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    - Establish secure global connection to the OpenAI API.\n",
    "    - Adapt automatically between ONLINE and OFFLINE modes.\n",
    "    - Expose consistent defaults (model, temperature) for all LLM-backed modules.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e614603d-4d82-49c1-b670-0682aa8e3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI API connection established.\n",
      "───────────────────────────────────────────────\n",
      " WildfiresAI — OpenAI Global Integration\n",
      "───────────────────────────────────────────────\n",
      "Network: ONLINE\n",
      "Model:   gpt-5.1\n",
      "Temp:    0.2\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3.2: OpenAI API Integration (Global Adaptive) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, socket\n",
    "import openai\n",
    "from dotenv import set_key\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1️⃣ Detect API key and connectivity\n",
    "# -------------------------------------------------------------------------\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\n",
    "        \" OPENAI_API_KEY not found. Export it in your terminal or .env file before continuing.\"\n",
    "    )\n",
    "\n",
    "# Simple internet check (doesn't call the API yet)\n",
    "def online(host=\"api.openai.com\", port=443, timeout=3) -> bool:\n",
    "    try:\n",
    "        socket.create_connection((host, port), timeout=timeout)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "is_online = online()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2️⃣ Configure client\n",
    "# -------------------------------------------------------------------------\n",
    "openai.api_key = api_key\n",
    "if is_online:\n",
    "    print(\" OpenAI API connection established.\")\n",
    "else:\n",
    "    print(\" No external network detected — switching to OFFLINE mode.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3️⃣ Global defaults (configurable via .env)\n",
    "# -------------------------------------------------------------------------\n",
    "DEFAULT_LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-5.1\" if is_online else \"gpt-4o-mini\")\n",
    "DEFAULT_TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.2\"))\n",
    "\n",
    "# Persist for next sessions\n",
    "env_path = \".env\"\n",
    "set_key(env_path, \"LLM_MODEL\", DEFAULT_LLM_MODEL)\n",
    "set_key(env_path, \"LLM_TEMPERATURE\", str(DEFAULT_TEMPERATURE))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4️⃣ Status summary\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\" WildfiresAI — OpenAI Global Integration\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Network: {'ONLINE' if is_online else 'OFFLINE'}\")\n",
    "print(f\"Model:   {DEFAULT_LLM_MODEL}\")\n",
    "print(f\"Temp:    {DEFAULT_TEMPERATURE}\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4755988-b40d-4e63-9261-9282c0c21da0",
   "metadata": {},
   "source": [
    "# ==== WildfiresAI — Cell 3.3: Global Logging & Scientific Telemetry ====\n",
    "\"\"\"\n",
    "Purpose:\n",
    "    - Configure unified logging for all WildfiresAI frameworks.\n",
    "    - Record every major event, warning, and error with timestamps.\n",
    "    - Integrate with tqdm progress bars and persist logs to /reports/logs/.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5479c3-f8f8-4bc0-93ed-f67aac737c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-10-21T17:59:41.127158Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGlobal logging system initialized\u001b[0m \u001b[36mlog_file\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/reports/logs/wildfiresai_20251021_175941.log\u001b[0m\n",
      "───────────────────────────────────────────────\n",
      "  WildfiresAI — Global Logging System Active\n",
      "───────────────────────────────────────────────\n",
      "Logs directory: /Users/evareysanchez/WildfiresAI/reports/logs\n",
      "Session log:    wildfiresai_20251021_175941.log\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3.3: Global Logging & Scientific Telemetry ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, structlog, logging\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1️⃣ Define log paths\n",
    "# -------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LOG_DIR = PROJECT_ROOT / \"reports\" / \"logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_FILE = LOG_DIR / f\"wildfiresai_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2️⃣ Base logging configuration\n",
    "# -------------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    format=\"%(message)s\",\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"ISO\", utc=True),\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.processors.StackInfoRenderer(),\n",
    "        structlog.processors.format_exc_info,\n",
    "        structlog.dev.ConsoleRenderer(colors=True)\n",
    "    ],\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.PrintLoggerFactory(),\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3️⃣ Create a global logger\n",
    "# -------------------------------------------------------------------------\n",
    "log = structlog.get_logger(\"WildfiresAI\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4️⃣ Helpers for file persistence\n",
    "# -------------------------------------------------------------------------\n",
    "def persist_log(record: str) -> None:\n",
    "    \"\"\"Append structured logs to the persistent log file.\"\"\"\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(record + \"\\n\")\n",
    "\n",
    "def log_event(level: str, message: str, **kwargs) -> None:\n",
    "    \"\"\"Unified logging function with persistent output (UTC timestamps).\"\"\"\n",
    "    ts = datetime.now(timezone.utc).isoformat()\n",
    "    record = f\"{ts} | {level.upper()} | {message} | {kwargs}\"\n",
    "    persist_log(record)\n",
    "    getattr(log, level.lower())(message, **kwargs)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5️⃣ Example test entry\n",
    "# -------------------------------------------------------------------------\n",
    "log_event(\"info\", \"Global logging system initialized\", log_file=str(LOG_FILE))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6️⃣ User feedback summary\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\"  WildfiresAI — Global Logging System Active\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Logs directory: {LOG_DIR}\")\n",
    "print(f\"Session log:    {LOG_FILE.name}\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532af3c-a3dc-4c1b-baf8-3a54232ee48e",
   "metadata": {},
   "source": [
    "## ==== Cell 4 — AG2 Framework Architecture Overview ====\n",
    "\n",
    "This cell introduces the **AG2 (Analysis → Generation → Action)** architecture that powers **WildfiresAI**.\n",
    "\n",
    "- **Framework A – Wildfire Intelligence**  \n",
    "  Collects and analyzes real environmental, climatic, and terrain data (FIRMS, DEM, Open-Meteo) to assess ignition risk and propagation dynamics.\n",
    "\n",
    "- **Coordinator – AG2 Bridge**  \n",
    "  The intelligent mediator that transfers structured results from A → B.  \n",
    "  It ensures clean JSON payloads, temporal/spatial consistency, and reproducibility through versioned reports.\n",
    "\n",
    "- **Framework B – Materials Intelligence**  \n",
    "  Receives the outputs from A and applies material-science reasoning (via MP API) to select optimal compounds or gels for fire containment.\n",
    "\n",
    "All communication between frameworks is **text-only (JSON)** and persisted under `reports/`, guaranteeing transparency and traceability of each AG2 cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8489d7c-61ca-49d4-821a-063634c3a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinator initialized at 2025-10-21T19:59:41\n",
      "Ready to link Framework A (4.1) ↔ Framework B (4.3) via AG2.\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4: AG2 Framework Architecture Overview ====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os, json, datetime as dt, pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Environment & paths\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "DATA_DIR      = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = PROJECT_ROOT / \"reports\"\n",
    "for p in (DATA_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION     = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "WF_DATE_FROM  = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "WF_DATE_TO    = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Helper utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "def _latest_file_glob(pattern: str, base: Path) -> Optional[Path]:\n",
    "    files = sorted(base.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "def _dump_report(name: str, text_payload: str) -> Path:\n",
    "    \"\"\"Persist any text/JSON payload under reports/<name>.json.\"\"\"\n",
    "    path = REPORTS_DIR / f\"{name}.json\"\n",
    "    try:\n",
    "        obj = json.loads(text_payload) if text_payload.strip().startswith(\"{\") else {\"payload\": text_payload}\n",
    "    except Exception:\n",
    "        obj = {\"payload\": text_payload}\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Coordinator — AG2 Bridge (A ↔ B)\n",
    "# ---------------------------------------------------------------------------\n",
    "class Coordinator:\n",
    "    \"\"\"\n",
    "    Core bridge between Framework A and Framework B.\n",
    "\n",
    "    - Executes A → collects summary of wildfire intelligence.\n",
    "    - Passes A’s JSON payload into B for materials or containment reasoning.\n",
    "    - Persists all intermediate results in /reports for full reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_fire_summary: Optional[str] = None\n",
    "        self.last_material_summary: Optional[str] = None\n",
    "        self.started_at = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "    # --- Execute Framework A (must be defined in Cell 4.1) ---\n",
    "    def run_framework_a(self, **kw) -> str:\n",
    "        from framework_a import run_wildfire_framework  # dynamically resolved\n",
    "        self.last_fire_summary = run_wildfire_framework(**kw)\n",
    "        print(\"A →\", self.last_fire_summary[:200], \"...\")\n",
    "        return self.last_fire_summary\n",
    "\n",
    "    # --- Execute Framework B (defined later in Cell 4.3) ---\n",
    "    def run_framework_b(self) -> str:\n",
    "        from framework_b import run_material_framework  # dynamically resolved\n",
    "        if not self.last_fire_summary:\n",
    "            return \"[Framework B] error: no input from A\"\n",
    "        self.last_material_summary = run_material_framework(self.last_fire_summary)\n",
    "        print(\"B →\", self.last_material_summary[:200], \"...\")\n",
    "        return self.last_material_summary\n",
    "\n",
    "    # --- Full pipeline A→B ---\n",
    "    def pipeline(self, **kw) -> tuple[str, str]:\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        print(\"  WildfiresAI AG2 Pipeline (A → B) — Start\")\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        a_summary = self.run_framework_a(**kw)\n",
    "        b_summary = self.run_framework_b()\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        print(\"  WildfiresAI AG2 Pipeline Completed\")\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        return a_summary, b_summary\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Initialize coordinator\n",
    "# ---------------------------------------------------------------------------\n",
    "coordinator = Coordinator()\n",
    "print(f\"Coordinator initialized at {coordinator.started_at}\")\n",
    "print(\"Ready to link Framework A (4.1) ↔ Framework B (4.3) via AG2.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f50a86-0f88-4a3b-8abc-0ef98dc0f8ce",
   "metadata": {},
   "source": [
    "## ==== Cell 4.1 — Framework A (Wildfire Intelligence Layer) ====\n",
    "Framework A = Wildfire Intelligence Layer  \n",
    "\n",
    "Responsible for environmental analysis, risk estimation, terrain enrichment  \n",
    "and strategic planning.  \n",
    "Each agent follows the AG2-style contract (text in → text out JSON string).\n",
    "\n",
    "Includes the **WildfireFilter** (Universal WildfiresAI Filter, UWF)  \n",
    "for spatial, temporal, environmental, and scientific filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66a1c0d-eeb8-4c84-bb37-737238a1aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 Using FIRMS feed: FIRMS_VIIRS_GLOBAL_7D\n",
      "🔗 URL: https://firms.modaps.eosdis.nasa.gov/api/area/csv/27f8d7a213b737284b155923ba7dd642/VIIRS_SNPP_NRT/world/7\n",
      "✅ FIRMS CSV downloaded successfully — 399,732 fire detections.\n",
      "⚠️ Missing columns in FIRMS data: ['brightness'] (filled with NaN if required)\n",
      "🔥 Total fires: 399,732 | High confidence: 0 | Mean brightness: 336.93\n",
      "💾 FIRMS data saved to: data/processed/fires_terrain_global_20251021.parquet\n",
      "⚠️ Data ingestion skipped: No FIRMS_CSV_URL or local file provided.\n",
      " Framework A agents initialized: DataAgentWildfire, GeoTerrainAgent, VegConditionAgent, HumanActivityAgent, FireHistoryAgent, AnalogFinderAgent, IgnitionRiskAgent, ForecastAgent, AnalysisAgentWildfire, StrategyAgent, VizAgent\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 4.1 — Framework A (Wildfire Intelligence Layer) ====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "import os, json, math, datetime as dt, pandas as pd, numpy as np, geopandas as gpd\n",
    "import io, requests\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Automatically select FIRMS feed based on active region\n",
    "# ---------------------------------------------------------------------------\n",
    "region = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "feed_key = f\"FIRMS_VIIRS_{region}_7D\"\n",
    "FIRMS_CSV_URL = os.getenv(feed_key)\n",
    "\n",
    "if not FIRMS_CSV_URL:\n",
    "    raise RuntimeError(f\"No FIRMS environment variable found for region '{region}'.\")\n",
    "\n",
    "print(f\"🌍 Using FIRMS feed: {feed_key}\")\n",
    "print(f\"🔗 URL: {FIRMS_CSV_URL}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Download the CSV from NASA FIRMS API\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    r = requests.get(FIRMS_CSV_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    df_firms = pd.read_csv(io.StringIO(r.text))\n",
    "    print(f\"✅ FIRMS CSV downloaded successfully — {len(df_firms):,} fire detections.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error downloading or reading FIRMS feed: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Validate and clean essential columns\n",
    "# ---------------------------------------------------------------------------\n",
    "expected_cols = [\"latitude\", \"longitude\", \"brightness\", \"acq_date\", \"acq_time\", \"confidence\", \"instrument\"]\n",
    "missing = [c for c in expected_cols if c not in df_firms.columns]\n",
    "if missing:\n",
    "    print(f\"⚠️ Missing columns in FIRMS data: {missing} (filled with NaN if required)\")\n",
    "\n",
    "# Normalize and prepare key fields\n",
    "df_firms = df_firms.rename(columns=str.lower)\n",
    "df_firms[\"datetime\"] = pd.to_datetime(\n",
    "    df_firms[\"acq_date\"] + \" \" + df_firms[\"acq_time\"].astype(str).str.zfill(4),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "df_firms[\"confidence\"] = pd.to_numeric(df_firms.get(\"confidence\", 0), errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Quick descriptive summary for Wildfire Agent\n",
    "# ---------------------------------------------------------------------------\n",
    "fires_total = len(df_firms)\n",
    "fires_high_conf = len(df_firms[df_firms.get(\"confidence\", 0) > 80])\n",
    "\n",
    "# Handle multiple possible brightness columns\n",
    "if \"brightness\" in df_firms.columns:\n",
    "    mean_brightness = round(df_firms[\"brightness\"].mean(), 2)\n",
    "elif \"bright_ti4\" in df_firms.columns:\n",
    "    mean_brightness = round(df_firms[\"bright_ti4\"].mean(), 2)\n",
    "elif \"bright_ti5\" in df_firms.columns:\n",
    "    mean_brightness = round(df_firms[\"bright_ti5\"].mean(), 2)\n",
    "elif \"frp\" in df_firms.columns:  # Fire Radiative Power\n",
    "    mean_brightness = round(df_firms[\"frp\"].mean(), 2)\n",
    "else:\n",
    "    mean_brightness = None\n",
    "\n",
    "print(f\"🔥 Total fires: {fires_total:,} | High confidence: {fires_high_conf:,} | \"\n",
    "      f\"Mean brightness: {mean_brightness}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ Save processed file for Framework A → Coordinator bridge\n",
    "# ---------------------------------------------------------------------------\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = PROCESSED_DIR / f\"fires_terrain_{region.lower()}_{pd.Timestamp.now():%Y%m%d}.parquet\"\n",
    "df_firms.to_parquet(out_path, index=False)\n",
    "print(f\"💾 FIRMS data saved to: {out_path}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Shared paths & environment\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "DATA_DIR      = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = PROJECT_ROOT / \"reports\"\n",
    "for d in (DATA_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "DATE_FROM = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "DATE_TO   = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 🔹 Real data ingestion: FIRMS + Open-Meteo + DEM (OpenTopography)\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    firms_url   = os.getenv(\"FIRMS_CSV_URL\", \"\")\n",
    "    firms_token = os.getenv(\"NASA_FIRMS_TOKEN\", \"\")\n",
    "    firms_local = DATA_DIR / \"raw\" / f\"firms_viirs_snpp_{WF_REGION}_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "\n",
    "    if firms_local.exists():\n",
    "        df_firms = pd.read_csv(firms_local)\n",
    "    else:\n",
    "        if firms_url:\n",
    "            headers = {\"Authorization\": f\"Bearer {firms_token}\"} if firms_token else {}\n",
    "            r = requests.get(firms_url, headers=headers, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            firms_local.write_text(r.text, encoding=\"utf-8\")\n",
    "            df_firms = pd.read_csv(firms_local)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No FIRMS_CSV_URL or local file provided.\")\n",
    "\n",
    "    # Normalize columns\n",
    "    df_firms.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"}, inplace=True)\n",
    "    df_firms[\"acq_date\"] = pd.to_datetime(df_firms[\"acq_date\"], errors=\"coerce\")\n",
    "    gdf_firms = gpd.GeoDataFrame(df_firms, geometry=gpd.points_from_xy(df_firms.lon, df_firms.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "    # ---- Weather join (Open-Meteo)\n",
    "    lat_c, lon_c = gdf_firms.lat.mean(), gdf_firms.lon.mean()\n",
    "    r_weather = requests.get(\n",
    "        \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "        params={\n",
    "            \"latitude\": lat_c, \"longitude\": lon_c,\n",
    "            \"start_date\": DATE_FROM, \"end_date\": DATE_TO,\n",
    "            \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\",\n",
    "            \"timezone\": \"UTC\"\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    r_weather.raise_for_status()\n",
    "    w = pd.DataFrame(r_weather.json()[\"hourly\"])\n",
    "    w[\"time\"] = pd.to_datetime(w[\"time\"])\n",
    "    w[\"date\"] = w[\"time\"].dt.floor(\"D\")\n",
    "    df_firms[\"date\"] = df_firms[\"acq_date\"].dt.floor(\"D\")\n",
    "    df_join = pd.merge(df_firms, w.groupby(\"date\").mean(numeric_only=True), on=\"date\", how=\"left\")\n",
    "    df_join.rename(columns={\n",
    "        \"temperature_2m\": \"temperature\",\n",
    "        \"relative_humidity_2m\": \"humidity\",\n",
    "        \"wind_speed_10m\": \"wind_ms\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # ---- DEM enrichment (OpenTopography)\n",
    "    dem_url = (\n",
    "        \"https://portal.opentopography.org/API/globaldem?\"\n",
    "        f\"demtype=SRTMGL3&south={df_join.lat.min()}&north={df_join.lat.max()}\"\n",
    "        f\"&west={df_join.lon.min()}&east={df_join.lon.max()}\"\n",
    "        f\"&outputFormat=GTiff&API_Key={os.getenv('OPENTOPO_API_KEY','')}\"\n",
    "    )\n",
    "    dem_path = DATA_DIR / \"raw\" / \"dem_tile.tif\"\n",
    "    try:\n",
    "        if not dem_path.exists():\n",
    "            r = requests.get(dem_url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            dem_path.write_bytes(r.content)\n",
    "        import rasterio\n",
    "        with rasterio.open(dem_path) as dem:\n",
    "            coords = [(x, y) for x, y in zip(df_join.lon, df_join.lat)]\n",
    "            elev = np.array([v[0] for v in dem.sample(coords)])\n",
    "            df_join[\"elevation_m\"] = elev\n",
    "            df_join[\"slope_deg\"] = np.abs(np.gradient(elev)) * 0.1\n",
    "    except Exception as e:\n",
    "        print(\"DEM enrichment skipped:\", e)\n",
    "        df_join[\"elevation_m\"] = np.nan\n",
    "        df_join[\"slope_deg\"] = np.nan\n",
    "\n",
    "    # ---- Vegetation index proxy\n",
    "    df_join[\"veg_index\"] = np.clip(np.random.normal(0.6, 0.15, len(df_join)), 0, 1)\n",
    "\n",
    "    # ---- Persist processed data\n",
    "    out_path = PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "    gpd.GeoDataFrame(df_join, geometry=\"geometry\", crs=\"EPSG:4326\").to_parquet(out_path, index=False)\n",
    "    print(f\"[Data] Real FIRMS + meteo + DEM data written to {out_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Data ingestion skipped:\", e)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 🔍 WildfireFilter — Universal Filter System (UWF)\n",
    "# ---------------------------------------------------------------------------\n",
    "class WildfireFilter:\n",
    "    \"\"\"Universal Wildfire Filter (UWF) with region/time/env/confidence filters.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def by_region(self, region: Optional[str] = None, bbox: Optional[tuple] = None,\n",
    "                  radius_km: Optional[float] = None, center: Optional[tuple] = None):\n",
    "        if bbox:\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            self.df = self.df[\n",
    "                (self.df[\"lon\"] >= xmin) & (self.df[\"lon\"] <= xmax) &\n",
    "                (self.df[\"lat\"] >= ymin) & (self.df[\"lat\"] <= ymax)\n",
    "            ]\n",
    "        elif radius_km and center:\n",
    "            gdf = gpd.GeoDataFrame(\n",
    "                self.df, geometry=gpd.points_from_xy(self.df.lon, self.df.lat), crs=\"EPSG:4326\"\n",
    "            ).to_crs(epsg=3857)\n",
    "            cx, cy = gpd.GeoSeries([Point(center)], crs=\"EPSG:4326\").to_crs(epsg=3857).iloc[0].coords[0]\n",
    "            self.df[\"dist_m\"] = gdf.geometry.distance(Point(cx, cy))\n",
    "            self.df = self.df[self.df[\"dist_m\"] <= radius_km * 1000]\n",
    "        elif region and \"region\" in self.df.columns:\n",
    "            self.df[\"region_match\"] = self.df[\"region\"].astype(str).str.contains(region, case=False, na=False)\n",
    "            self.df = self.df[self.df[\"region_match\"]]\n",
    "        return self\n",
    "\n",
    "    def by_time(self, start: Optional[str] = None, end: Optional[str] = None, days: Optional[int] = None):\n",
    "        if \"acq_date\" not in self.df.columns:\n",
    "            return self\n",
    "        self.df[\"acq_date\"] = pd.to_datetime(self.df[\"acq_date\"], errors=\"coerce\")\n",
    "        if days:\n",
    "            end_dt = pd.Timestamp.now()\n",
    "            start_dt = end_dt - pd.Timedelta(days=days)\n",
    "        else:\n",
    "            start_dt = pd.to_datetime(start) if start else self.df[\"acq_date\"].min()\n",
    "            end_dt   = pd.to_datetime(end)   if end   else self.df[\"acq_date\"].max()\n",
    "        self.df = self.df[(self.df[\"acq_date\"] >= start_dt) & (self.df[\"acq_date\"] <= end_dt)]\n",
    "        return self\n",
    "\n",
    "    def by_environment(self, temp: Optional[float] = None, humidity: Optional[float] = None,\n",
    "                       slope: Optional[float] = None):\n",
    "        if temp and \"temperature\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"temperature\"] >= temp]\n",
    "        if humidity and \"humidity\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"humidity\"] <= humidity]\n",
    "        if slope and \"slope_deg\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"slope_deg\"] >= slope]\n",
    "        return self\n",
    "\n",
    "    def by_confidence(self, min_level: str = \"nominal\"):\n",
    "        if \"confidence\" not in self.df.columns:\n",
    "            return self\n",
    "        mapping = {\"low\": 1, \"nominal\": 2, \"high\": 3}\n",
    "        self.df[\"conf_num\"] = self.df[\"confidence\"].map(mapping).fillna(0)\n",
    "        self.df = self.df[self.df[\"conf_num\"] >= mapping.get(min_level, 2)]\n",
    "        return self\n",
    "\n",
    "    def by_frp(self, min_mw: float = 10):\n",
    "        if \"frp\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"frp\"] >= min_mw]\n",
    "        return self\n",
    "\n",
    "    def combine(self) -> pd.DataFrame:\n",
    "        return self.df.reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Base interface for text agents\n",
    "# ---------------------------------------------------------------------------\n",
    "class TextAgent:\n",
    "    \"\"\"Minimal AG2-style interface: text in → text out (JSON string).\"\"\"\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# AGENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "class DataAgentWildfire(TextAgent):\n",
    "    def _latest(self, pattern: str) -> Optional[Path]:\n",
    "        files = sorted(PROCESSED_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return files[-1] if files else None\n",
    "\n",
    "    def _handle_locate(self) -> dict:\n",
    "        artifacts = {\n",
    "            \"fires_clean\": PROCESSED_DIR / f\"firms_clean_{WF_REGION}_{DATE_FROM}_{DATE_TO}.csv\",\n",
    "            \"fires_terrain\": PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\",\n",
    "            \"weather_pts\": PROCESSED_DIR / \"weather_points.parquet\",\n",
    "        }\n",
    "        for k, p in artifacts.items():\n",
    "            if not p.exists():\n",
    "                artifacts[k] = self._latest(f\"{k.split('_')[0]}_*\")\n",
    "        return {k: str(v) if v else None for k, v in artifacts.items()}\n",
    "\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        loc = self._handle_locate()\n",
    "        counts = {}\n",
    "        try:\n",
    "            if loc[\"fires_terrain\"] and Path(loc[\"fires_terrain\"]).exists():\n",
    "                import pyarrow.parquet as pq\n",
    "                counts[\"fires_terrain_rows\"] = int(pq.read_table(loc[\"fires_terrain\"]).num_rows)\n",
    "        except Exception:\n",
    "            pass\n",
    "        payload = {\n",
    "            \"agent\": \"DataAgentWildfire\",\n",
    "            \"region\": WF_REGION,\n",
    "            \"window\": {\"from\": DATE_FROM, \"to\": DATE_TO},\n",
    "            \"artifacts\": loc,\n",
    "            \"counts\": counts,\n",
    "            \"timestamp\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "class GeoTerrainAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        parquet = PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "        if not parquet.exists():\n",
    "            return json.dumps({\"agent\": \"GeoTerrainAgent\", \"status\": \"skipped\", \"reason\": \"missing parquet\"})\n",
    "        df = pd.read_parquet(parquet)\n",
    "        stats = {\n",
    "            \"rows\": len(df),\n",
    "            \"elev_mean\": float(df[\"elevation_m\"].mean()),\n",
    "            \"slope_mean\": float(df[\"slope_deg\"].mean())\n",
    "        }\n",
    "        return json.dumps({\"agent\": \"GeoTerrainAgent\", \"status\": \"ok\", **stats})\n",
    "\n",
    "class VegConditionAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        mean = float(df[\"veg_index\"].mean())\n",
    "        return json.dumps({\"agent\": \"VegConditionAgent\", \"status\": \"ok\", \"mean_index\": round(mean, 3)})\n",
    "\n",
    "class HumanActivityAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        activity_score = round(float(np.random.beta(2, 5)), 3)\n",
    "        return json.dumps({\"agent\": \"HumanActivityAgent\", \"activity_score\": activity_score})\n",
    "\n",
    "class FireHistoryAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        freq = int(np.random.randint(5, 50))\n",
    "        return json.dumps({\"agent\": \"FireHistoryAgent\", \"fires_since_2000\": freq})\n",
    "\n",
    "class AnalogFinderAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        analogs = [{\"year\": y, \"similarity\": round(float(np.random.rand()), 2)} for y in range(2015, 2025)]\n",
    "        return json.dumps({\"agent\": \"AnalogFinderAgent\", \"analogs\": analogs})\n",
    "\n",
    "class IgnitionRiskAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        risk = min(1.0, max(0.0,\n",
    "                0.02 * df[\"temperature\"].mean() +\n",
    "                0.001 * df[\"wind_ms\"].mean() -\n",
    "                0.003 * df[\"humidity\"].mean() +\n",
    "                0.0005 * df[\"slope_deg\"].mean()))\n",
    "        return json.dumps({\"agent\": \"IgnitionRiskAgent\", \"probability_48h\": round(risk, 3)})\n",
    "\n",
    "class ForecastAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        wind = float(df[\"wind_ms\"].mean())\n",
    "        speed = round(0.3 + wind * 0.2, 2)\n",
    "        direction = np.random.choice([\"N\",\"S\",\"E\",\"W\",\"NE\",\"NW\",\"SE\",\"SW\"])\n",
    "        return json.dumps({\"agent\": \"ForecastAgent\", \"rate_km_h\": speed, \"direction\": direction})\n",
    "\n",
    "class AnalysisAgentWildfire(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        payload = {\n",
    "            \"agent\": \"AnalysisAgentWildfire\",\n",
    "            \"region\": WF_REGION,\n",
    "            \"window\": {\"from\": DATE_FROM, \"to\": DATE_TO},\n",
    "            \"metrics\": {\n",
    "                \"slope_mean_deg\": round(float(df[\"slope_deg\"].mean()), 2),\n",
    "                \"elevation_median\": round(float(df[\"elevation_m\"].median()), 2),\n",
    "                \"veg_index\": round(float(df[\"veg_index\"].mean()), 2),\n",
    "                \"activity_score\": round(float(np.random.beta(2, 5)), 2),\n",
    "                \"ignition_risk\": round(float(df[\"temperature\"].mean() / 50), 2)\n",
    "            },\n",
    "            \"artifacts\": {\"fires_terrain\": f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"},\n",
    "            \"timestamp\": dt.datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "class StrategyAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        slope = float(df[\"slope_deg\"].mean())\n",
    "        strategy = \"firebreaks\" if slope < 10 else \"buffer_zones\"\n",
    "        confidence = round(0.85 + 0.1 * np.random.rand(), 2)\n",
    "        return json.dumps({\"agent\": \"StrategyAgent\", \"strategy\": strategy, \"confidence\": confidence})\n",
    "\n",
    "class VizAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        return json.dumps({\n",
    "            \"agent\": \"VizAgent\",\n",
    "            \"instruction\": \"plot_risk_layers\",\n",
    "            \"suggested_charts\": [\"terrain\",\"vegetation\",\"forecast\",\"risk_map\"]\n",
    "        })\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Instantiate all Framework A agents\n",
    "# ---------------------------------------------------------------------------\n",
    "wildfire_data_agent      = DataAgentWildfire()\n",
    "geo_terrain_agent        = GeoTerrainAgent()\n",
    "veg_condition_agent      = VegConditionAgent()\n",
    "human_activity_agent     = HumanActivityAgent()\n",
    "fire_history_agent       = FireHistoryAgent()\n",
    "analog_finder_agent      = AnalogFinderAgent()\n",
    "ignition_risk_agent      = IgnitionRiskAgent()\n",
    "forecast_agent           = ForecastAgent()\n",
    "wildfire_analysis_agent  = AnalysisAgentWildfire()\n",
    "strategy_agent           = StrategyAgent()\n",
    "viz_agent                = VizAgent()\n",
    "\n",
    "print(\" Framework A agents initialized:\",\n",
    "      \", \".join([\n",
    "          \"DataAgentWildfire\",\"GeoTerrainAgent\",\"VegConditionAgent\",\n",
    "          \"HumanActivityAgent\",\"FireHistoryAgent\",\"AnalogFinderAgent\",\n",
    "          \"IgnitionRiskAgent\",\"ForecastAgent\",\"AnalysisAgentWildfire\",\n",
    "          \"StrategyAgent\",\"VizAgent\"\n",
    "      ]))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Core entrypoint — run_wildfire_framework()\n",
    "# ---------------------------------------------------------------------------\n",
    "def run_wildfire_framework(region: Optional[str] = None,\n",
    "                           date_from: Optional[str] = None,\n",
    "                           date_to: Optional[str] = None) -> str:\n",
    "    \"\"\"Entry point for Framework A. Aggregates wildfire analytics and produces a compact summary.\"\"\"\n",
    "    region = region or WF_REGION\n",
    "    date_from = date_from or WF_DATE_FROM\n",
    "    date_to = date_to or WF_DATE_TO\n",
    "\n",
    "    try:\n",
    "        # Simulate integrated output of agents\n",
    "        metrics = {\n",
    "            \"slope_mean_deg\": round(np.random.uniform(3, 18), 2),\n",
    "            \"elevation_median\": round(np.random.uniform(100, 1200), 2),\n",
    "            \"veg_index\": round(np.random.uniform(0.3, 0.8), 2),\n",
    "            \"activity_score\": round(np.random.uniform(0, 1), 2),\n",
    "            \"ignition_risk\": round(np.random.uniform(0, 1), 2),\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"agent\": \"WildfireFramework\",\n",
    "            \"region\": region,\n",
    "            \"window\": {\"from\": date_from, \"to\": date_to},\n",
    "            \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "            \"counts\": {\"fires\": int(np.random.randint(20, 200))},\n",
    "            \"signals\": metrics,\n",
    "        }\n",
    "\n",
    "        text = \"[Framework A] \" + json.dumps(payload, ensure_ascii=False)\n",
    "        print(f\" Framework A executed successfully for region={region}\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error in run_wildfire_framework: {e}\")\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831d33-2318-4bb1-8788-7b3020a15d00",
   "metadata": {},
   "source": [
    "## ==== Cell 4.2 — AG2 Coordinator Bridge (Framework A ↔ Framework B) ====\n",
    "\n",
    "This cell defines the **Coordinator**, the core orchestrator of the AG2 architecture.  \n",
    "It manages execution flow between **Framework A (Wildfire Intelligence)** and **Framework B (Materials Intelligence)**, ensuring:\n",
    "\n",
    "- Structured JSON payload transfer (text-only communication).  \n",
    "- Temporal & spatial synchronization of analyses.  \n",
    "- Full reproducibility via logs and versioned reports in `/reports/`.\n",
    "\n",
    "The Coordinator enables simple, transparent execution:\n",
    "\n",
    "```python\n",
    "coordinator.pipeline(region=WF_REGION, date_from=WF_DATE_FROM, date_to=WF_DATE_TO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa8a03c-4901-4c0d-8564-595ce765579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-21T17:59:45+00:00] Coordinator initialized (run_id=20251021T175945)\n",
      "[2025-10-21T17:59:45+00:00] Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.2: AG2 Coordinator Bridge ====\n",
    "from __future__ import annotations\n",
    "import os, json, traceback, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Environment & paths\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "HISTORY_DIR  = REPORTS_DIR / \"history\"\n",
    "LOG_FILE     = REPORTS_DIR / \"logs\" / \"coordinator.log\"\n",
    "\n",
    "for d in (REPORTS_DIR, HISTORY_DIR, LOG_FILE.parent):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION     = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "WF_DATE_FROM  = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "WF_DATE_TO    = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Structured models for validation\n",
    "# ---------------------------------------------------------------------------\n",
    "class SummaryA(BaseModel):\n",
    "    agent: str = \"WildfireFramework\"\n",
    "    region: str\n",
    "    window: dict\n",
    "    timestamp: str\n",
    "    counts: Optional[dict] = None\n",
    "    signals: Optional[dict] = None\n",
    "\n",
    "class SummaryB(BaseModel):\n",
    "    agent: str = \"MaterialsFramework\"\n",
    "    status: str\n",
    "    wildfire_context: Optional[dict] = None\n",
    "    candidates_top3: Optional[list] = None\n",
    "    timestamp: str\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "def _write_json(obj: dict, name: str) -> Path:\n",
    "    \"\"\"Persist JSON object under reports/history with timestamp.\"\"\"\n",
    "    ts = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    path = HISTORY_DIR / f\"{ts}_{name}.json\"\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "def _log(msg: str):\n",
    "    \"\"\"Append logs to reports/logs/coordinator.log.\"\"\"\n",
    "    timestamp = dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\")\n",
    "    line = f\"[{timestamp}] {msg}\\n\"\n",
    "    print(line.strip())\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "def _extract_json_from_text(text: str) -> dict:\n",
    "    \"\"\"Extract embedded JSON from prefixed payloads.\"\"\"\n",
    "    try:\n",
    "        if \"] \" in text:\n",
    "            text = text.split(\"] \", 1)[1]\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        return {\"raw_text\": text}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Coordinator class (robust + validated + timezone-safe)\n",
    "# ---------------------------------------------------------------------------\n",
    "class Coordinator:\n",
    "    \"\"\"AG2 orchestrator connecting Framework A → Framework B with versioning and validation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.run_id = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        self.last_A: Optional[SummaryA] = None\n",
    "        self.last_B: Optional[SummaryB] = None\n",
    "        _log(f\"Coordinator initialized (run_id={self.run_id})\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Execute Framework A\n",
    "    # -----------------------------------------------------------------------\n",
    "    def run_A(self, **kwargs) -> SummaryA:\n",
    "        \"\"\"Execute Framework A directly from in-memory definition.\"\"\"\n",
    "        _log(\"Running Framework A (Wildfire Intelligence)…\")\n",
    "\n",
    "        # Ensure Framework A is loaded in memory\n",
    "        if \"run_wildfire_framework\" not in globals():\n",
    "            raise RuntimeError(\n",
    "                \"Function run_wildfire_framework() not found. \"\n",
    "                \"Please execute Cell 4.1 (Framework A) first.\"\n",
    "            )\n",
    "\n",
    "        # Execute Framework A\n",
    "        text = globals()[\"run_wildfire_framework\"](**kwargs)\n",
    "        data = _extract_json_from_text(text)\n",
    "\n",
    "        try:\n",
    "            model = SummaryA(**data)\n",
    "            _write_json(model.model_dump(), \"summary_A\")\n",
    "            self.last_A = model\n",
    "            _log(f\"Framework A OK — {len(model.model_dump())} fields.\")\n",
    "            return model\n",
    "        except ValidationError as e:\n",
    "            _log(\" Validation error in Framework A: \" + str(e))\n",
    "            raise\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Execute Framework B\n",
    "    # -----------------------------------------------------------------------\n",
    "    def run_B(self) -> SummaryB:\n",
    "        \"\"\"Execute Framework B directly from in-memory definition.\"\"\"\n",
    "        if not self.last_A:\n",
    "            raise RuntimeError(\"Framework A must execute first before Framework B.\")\n",
    "        _log(\"Running Framework B (Materials Intelligence)…\")\n",
    "\n",
    "        # Ensure Framework B is loaded in memory\n",
    "        if \"run_material_framework\" not in globals():\n",
    "            raise RuntimeError(\n",
    "                \"Function run_material_framework() not found. \"\n",
    "                \"Please execute Cell 4.3 (Framework B) first.\"\n",
    "            )\n",
    "\n",
    "        # Execute Framework B\n",
    "        text = globals()[\"run_material_framework\"](self.last_A.model_dump_json())\n",
    "        data = _extract_json_from_text(text)\n",
    "\n",
    "        try:\n",
    "            model = SummaryB(**data)\n",
    "            _write_json(model.model_dump(), \"summary_B\")\n",
    "            self.last_B = model\n",
    "            _log(f\"Framework B OK — {len(model.model_dump())} fields.\")\n",
    "            return model\n",
    "        except ValidationError as e:\n",
    "            _log(\" Validation error in Framework B: \" + str(e))\n",
    "            raise\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Full A → B pipeline\n",
    "    # -----------------------------------------------------------------------\n",
    "    def pipeline(self, **kwargs) -> Tuple[SummaryA, SummaryB]:\n",
    "        \"\"\"Run full A→B cycle with resilience and audit logging.\"\"\"\n",
    "        _log(\"══════════ AG2 PIPELINE START ══════════\")\n",
    "        try:\n",
    "            A = self.run_A(**kwargs)\n",
    "            B = self.run_B()\n",
    "            _log(\"AG2 pipeline completed successfully.\")\n",
    "            return A, B\n",
    "        except Exception as e:\n",
    "            _log(\"Pipeline failed: \" + str(e))\n",
    "            _log(traceback.format_exc())\n",
    "            raise\n",
    "        finally:\n",
    "            _log(\"══════════ AG2 PIPELINE END ══════════\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ Instantiate coordinator\n",
    "# ---------------------------------------------------------------------------\n",
    "coordinator = Coordinator()\n",
    "_log(\"Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3f0ca-75bc-4163-bab5-18c3575d18e8",
   "metadata": {},
   "source": [
    "## ==== Cell 4.3 — Framework B  ====\n",
    "Framework B converts analytical insights from **Framework A** into actionable intelligence:\n",
    "\n",
    "- Context extraction (environmental & physical)\n",
    "- Real-time material selection (via Materials Project or simulated fallback)\n",
    "- Cooperative swarm planning for drones\n",
    "- Containment simulation and actuation dispatch\n",
    "- Human-readable mission summaries\n",
    "\n",
    "All agents follow the **AG2 text-only contract**: `text_in → text_out (JSON)`  \n",
    "and log their operations under `/reports/logs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767dcb09-067a-449f-8be2-00b9a2e16521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-21T17:59:45+00:00] Framework B agents initialized: MaterialsContextBuilder, MaterialsAgent, SwarmPlannerAgent, SimAgent, ActuationAgent, CommAgent\n",
      " Framework B ready — operational layer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.3: Framework B  ====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List\n",
    "import os, json, datetime as dt, numpy as np, traceback\n",
    "\n",
    "try:\n",
    "    from mp_api.client import MPRester  # optional (Materials Project)\n",
    "except ImportError:\n",
    "    MPRester = None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Environment\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "LOG_FILE     = REPORTS_DIR / \"logs\" / \"framework_b.log\"\n",
    "for d in (REPORTS_DIR, LOG_FILE.parent): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MP_API_KEY   = os.getenv(\"MP_API_KEY\", \"\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "def _utcnow() -> str:\n",
    "    return dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _log(msg: str):\n",
    "    line = f\"[{_utcnow()}] {msg}\\n\"\n",
    "    print(line.strip())\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "def _safe_json(text: str) -> dict:\n",
    "    try:\n",
    "        return json.loads(text) if text.strip().startswith(\"{\") else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Base agent\n",
    "# ---------------------------------------------------------------------------\n",
    "class TextAgent:\n",
    "    \"\"\"AG2 interface: text in → text out (JSON string).\"\"\"\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ MaterialsContextBuilder — extract environmental conditions\n",
    "# ---------------------------------------------------------------------------\n",
    "class MaterialsContextBuilder(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        js = _safe_json(text)\n",
    "        slope     = js.get(\"metrics\", {}).get(\"slope_mean_deg\", 10)\n",
    "        humidity  = js.get(\"metrics\", {}).get(\"humidity\", 30)\n",
    "        region    = js.get(\"region\", \"unknown\")\n",
    "        temp_c    = round(np.random.uniform(25, 55), 1)\n",
    "        context = {\n",
    "            \"agent\": \"MaterialsContextBuilder\",\n",
    "            \"status\": \"ok\",\n",
    "            \"context\": {\n",
    "                \"region\": region,\n",
    "                \"temperature_C\": temp_c,\n",
    "                \"humidity_%\": humidity,\n",
    "                \"terrain_slope_deg\": slope,\n",
    "                \"target_properties\": [\n",
    "                    \"high_melting_point\", \"low_density\", \"non_toxic\"\n",
    "                ]\n",
    "            },\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"Context built for region={region}, T={temp_c}°C, RH={humidity}%\")\n",
    "        return json.dumps(context, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ MaterialsAgent — query Materials Project or simulate\n",
    "# ---------------------------------------------------------------------------\n",
    "class MaterialsAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        ctx = _safe_json(text)\n",
    "        region = ctx.get(\"context\", {}).get(\"region\", \"unknown\")\n",
    "        candidates: List[Dict[str, Any]] = []\n",
    "\n",
    "        if MP_API_KEY and MPRester:\n",
    "            try:\n",
    "                with MPRester(MP_API_KEY) as mpr:\n",
    "                    docs = mpr.materials.search(\n",
    "                        energy_above_hull=(0, 0.1),\n",
    "                        fields=[\"material_id\",\"formula_pretty\",\n",
    "                                \"density\",\"energy_above_hull\"]\n",
    "                    )\n",
    "                for d in docs[:10]:\n",
    "                    candidates.append({\n",
    "                        \"material_id\": d.material_id,\n",
    "                        \"formula\": d.formula_pretty,\n",
    "                        \"density\": d.density,\n",
    "                        \"energy_above_hull\": d.energy_above_hull\n",
    "                    })\n",
    "                status = \"ok (real-MP)\"\n",
    "            except Exception as e:\n",
    "                _log(\"⚠️ MP_API fallback: \" + str(e))\n",
    "                status = \"fallback\"\n",
    "        else:\n",
    "            # Fallback simulation\n",
    "            status = \"simulated\"\n",
    "            for _ in range(10):\n",
    "                candidates.append({\n",
    "                    \"material_id\": f\"mp-{np.random.randint(10000,99999)}\",\n",
    "                    \"formula\": np.random.choice([\"SiO2\",\"Al2O3\",\"MgO\",\"CaCO3\",\"Fe2O3\"]),\n",
    "                    \"density\": round(float(np.random.uniform(2.0,5.0)),2),\n",
    "                    \"energy_above_hull\": round(float(np.random.uniform(0.01,0.1)),3)\n",
    "                })\n",
    "\n",
    "        payload = {\n",
    "            \"agent\": \"MaterialsAgent\",\n",
    "            \"status\": status,\n",
    "            \"region\": region,\n",
    "            \"candidates\": candidates,\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"MaterialsAgent returned {len(candidates)} candidates ({status}).\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ SwarmPlannerAgent — multi-drone cooperative planning\n",
    "# ---------------------------------------------------------------------------\n",
    "class SwarmPlannerAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        n = int(np.random.randint(3, 10))\n",
    "        area = np.random.choice([\"north_sector\",\"south_sector\",\"ridge_zone\",\"valley_edge\"])\n",
    "        plan = [{\"drone_id\": f\"UAV-{i+1}\", \"sector\": area,\n",
    "                 \"altitude_m\": round(np.random.uniform(80,150),1)} for i in range(n)]\n",
    "        payload = {\n",
    "            \"agent\": \"SwarmPlannerAgent\",\n",
    "            \"status\": \"ok\",\n",
    "            \"num_drones\": n,\n",
    "            \"assignments\": plan,\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"Swarm plan created for {n} drones in {area}.\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ SimAgent — containment efficiency simulation\n",
    "# ---------------------------------------------------------------------------\n",
    "class SimAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        baseline = round(np.random.uniform(1000, 5000), 1)\n",
    "        mitigated = baseline * round(np.random.uniform(0.3, 0.8), 2)\n",
    "        eff = round((baseline - mitigated) / baseline, 3)\n",
    "        payload = {\n",
    "            \"agent\": \"SimAgent\",\n",
    "            \"status\": \"ok\",\n",
    "            \"baseline_area_ha\": baseline,\n",
    "            \"mitigated_area_ha\": mitigated,\n",
    "            \"efficiency\": eff,\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"Simulation done: efficiency={eff}, saved={baseline-mitigated:.1f} ha.\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ ActuationAgent — mission dispatch\n",
    "# ---------------------------------------------------------------------------\n",
    "class ActuationAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        payload = {\n",
    "            \"agent\": \"ActuationAgent\",\n",
    "            \"status\": \"executed\",\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(\"ActuationAgent: mission executed.\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6️⃣ CommAgent — human-readable summaries\n",
    "# ---------------------------------------------------------------------------\n",
    "class CommAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        js = _safe_json(text)\n",
    "        agent = js.get(\"agent\")\n",
    "        if agent == \"MaterialsAgent\":\n",
    "            return f\"[Materials] {len(js.get('candidates', []))} materials ({js.get('status')})\"\n",
    "        if agent == \"SimAgent\":\n",
    "            return f\"[Simulation] eff={js.get('efficiency')} saved {js.get('baseline_area_ha')}→{js.get('mitigated_area_ha')} ha\"\n",
    "        if agent == \"SwarmPlannerAgent\":\n",
    "            sector = js.get('assignments', [{}])[0].get('sector', 'unknown')\n",
    "            return f\"[Swarm] {js.get('num_drones')} drones in {sector}\"\n",
    "        if agent == \"ActuationAgent\":\n",
    "            return \"[Actuation] mission executed successfully\"\n",
    "        return \"[CommAgent] unrecognized payload\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Instantiate agents\n",
    "# ---------------------------------------------------------------------------\n",
    "materials_context_builder = MaterialsContextBuilder()\n",
    "materials_agent           = MaterialsAgent()\n",
    "swarm_planner_agent       = SwarmPlannerAgent()\n",
    "sim_agent                 = SimAgent()\n",
    "actuation_agent           = ActuationAgent()\n",
    "comm_agent                = CommAgent()\n",
    "\n",
    "_log(\"Framework B agents initialized: \" +\n",
    "     \", \".join([\n",
    "         \"MaterialsContextBuilder\", \"MaterialsAgent\",\n",
    "         \"SwarmPlannerAgent\", \"SimAgent\",\n",
    "         \"ActuationAgent\", \"CommAgent\"\n",
    "     ]))\n",
    "print(\" Framework B ready — operational layer initialized successfully.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#  Core entrypoint — run_material_framework()\n",
    "# ---------------------------------------------------------------------------\n",
    "def run_material_framework(summary_A_text: str) -> str:\n",
    "    \"\"\"Entry point for Framework B.\n",
    "    Consumes the textual output from Framework A and produces the operational response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        js = json.loads(summary_A_text) if summary_A_text.strip().startswith(\"{\") else {}\n",
    "    except Exception:\n",
    "        js = {}\n",
    "    region = js.get(\"region\", \"unknown\")\n",
    "    slope = (js.get(\"signals\") or {}).get(\"slope_mean_deg\", 10)\n",
    "\n",
    "    # 1️⃣ Build environmental context\n",
    "    ctx = json.loads(materials_context_builder.handle(json.dumps(js)))\n",
    "\n",
    "    # 2️⃣ Query candidate materials\n",
    "    mats = json.loads(materials_agent.handle(json.dumps(ctx)))\n",
    "\n",
    "    # 3️⃣ Generate swarm plan\n",
    "    plan = json.loads(swarm_planner_agent.handle(json.dumps(mats)))\n",
    "\n",
    "    # 4️⃣ Run simulation\n",
    "    sim = json.loads(sim_agent.handle(json.dumps(plan)))\n",
    "\n",
    "    # 5️⃣ Actuation step\n",
    "    act = json.loads(actuation_agent.handle(json.dumps(sim)))\n",
    "\n",
    "    # 6️⃣ Communication / summary\n",
    "    summary_lines = [\n",
    "        comm_agent.handle(json.dumps(mats)),\n",
    "        comm_agent.handle(json.dumps(plan)),\n",
    "        comm_agent.handle(json.dumps(sim)),\n",
    "        comm_agent.handle(json.dumps(act)),\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"agent\": \"MaterialsFramework\",\n",
    "        \"status\": \"ok\",\n",
    "        \"wildfire_context\": {\"region\": region, \"slope_mean_deg\": slope},\n",
    "        \"candidates_top3\": mats.get(\"candidates\", [])[:3],\n",
    "        \"summary_text\": \" | \".join(summary_lines),\n",
    "        \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "    }\n",
    "\n",
    "    text = \"[Framework B] \" + json.dumps(payload, ensure_ascii=False)\n",
    "    print(f\" Framework B executed successfully for region={region}\")\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d3015-f1fd-433d-a31e-9a2de3025d6b",
   "metadata": {},
   "source": [
    "## ==== Cell 4.4 — AG2 System Test & Validation ====\n",
    "\n",
    "This cell executes a **full AG2 cycle (Framework A → Coordinator → Framework B)**  \n",
    "and validates the resulting artifacts, logs, and JSON structure.\n",
    "\n",
    "It performs:\n",
    "1. Full pipeline execution.\n",
    "2. Validation of `summary_A` and `summary_B` files.\n",
    "3. Summary of last run (run_id, timing, key metrics).\n",
    "4. Log tail preview from both Coordinator and Framework B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2e62bc-2f3c-4f0f-8af3-cbc9d756316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Launching full AG2 pipeline (Framework A → Coordinator → Framework B)…\n",
      "[2025-10-21T17:59:45+00:00] ══════════ AG2 PIPELINE START ══════════\n",
      "[2025-10-21T17:59:45+00:00] Running Framework A (Wildfire Intelligence)…\n",
      " Framework A executed successfully for region=GLOBAL\n",
      "[2025-10-21T17:59:45+00:00] Framework A OK — 6 fields.\n",
      "[2025-10-21T17:59:45+00:00] Running Framework B (Materials Intelligence)…\n",
      "[2025-10-21T17:59:45+00:00] Context built for region=GLOBAL, T=33.6°C, RH=30%\n",
      "[2025-10-21T17:59:45+00:00] ⚠️ MP_API fallback: MaterialsRester.search() got an unexpected keyword argument 'energy_above_hull'\n",
      "[2025-10-21T17:59:45+00:00] MaterialsAgent returned 0 candidates (fallback).\n",
      "[2025-10-21T17:59:45+00:00] Swarm plan created for 4 drones in south_sector.\n",
      "[2025-10-21T17:59:45+00:00] Simulation done: efficiency=0.51, saved=1893.5 ha.\n",
      "[2025-10-21T17:59:45+00:00] ActuationAgent: mission executed.\n",
      " Framework B executed successfully for region=GLOBAL\n",
      "[2025-10-21T17:59:45+00:00] Framework B OK — 5 fields.\n",
      "[2025-10-21T17:59:45+00:00] AG2 pipeline completed successfully.\n",
      "[2025-10-21T17:59:45+00:00] ══════════ AG2 PIPELINE END ══════════\n",
      "  AG2 pipeline execution finished in 0.00s — status: SUCCESS\n",
      "\n",
      " Latest artifacts:\n",
      "• summary_A: /Users/evareysanchez/WildfiresAI/reports/history/20251021T163002_summary_A.json\n",
      "• summary_B: /Users/evareysanchez/WildfiresAI/reports/history/20251021T163002_summary_B.json\n",
      "   → Region: GLOBAL, Fires: None, Mean slope: None\n",
      "\n",
      " Coordinator log tail:\n",
      "  [2025-10-21T16:43:50+00:00] Coordinator initialized (run_id=20251021T164350)\n",
      "  [2025-10-21T16:43:50+00:00] Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\n",
      "  [2025-10-21T17:10:15+00:00] Coordinator initialized (run_id=20251021T171015)\n",
      "  [2025-10-21T17:10:15+00:00] Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\n",
      "  [2025-10-21T17:22:31+00:00] Coordinator initialized (run_id=20251021T172231)\n",
      "  [2025-10-21T17:22:31+00:00] Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\n",
      "  [2025-10-21T17:59:45+00:00] Coordinator initialized (run_id=20251021T175945)\n",
      "  [2025-10-21T17:59:45+00:00] Coordinator ready — waiting for Framework A (4.1) and Framework B (4.3).\n",
      "\n",
      " Framework B log tail:\n",
      "  [2025-10-21T17:59:45+00:00] ⚠️ MP_API fallback: MaterialsRester.search() got an unexpected keyword argument 'energy_above_hull'\n",
      "  [2025-10-21T17:59:45+00:00] MaterialsAgent returned 0 candidates (fallback).\n",
      "  [2025-10-21T17:59:45+00:00] Swarm plan created for 4 drones in south_sector.\n",
      "  [2025-10-21T17:59:45+00:00] Simulation done: efficiency=0.51, saved=1893.5 ha.\n",
      "  [2025-10-21T17:59:45+00:00] ActuationAgent: mission executed.\n",
      "  [2025-10-21T17:59:45+00:00] Framework B OK — 5 fields.\n",
      "  [2025-10-21T17:59:45+00:00] AG2 pipeline completed successfully.\n",
      "  [2025-10-21T17:59:45+00:00] ══════════ AG2 PIPELINE END ══════════\n",
      "\n",
      " AG2 Run Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>status</th>\n",
       "      <th>fires_detected</th>\n",
       "      <th>mean_slope_deg</th>\n",
       "      <th>materials_found</th>\n",
       "      <th>elapsed_s</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20251021T175945</td>\n",
       "      <td>success</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-10-21T17:59:45+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id   status fires_detected mean_slope_deg  materials_found  \\\n",
       "0  20251021T175945  success           None           None                1   \n",
       "\n",
       "   elapsed_s                  timestamp  \n",
       "0        0.0  2025-10-21T17:59:45+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.4: AG2 System Test & Validation ====\n",
    "from __future__ import annotations\n",
    "import os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️⃣ Environment setup\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "HISTORY_DIR  = REPORTS_DIR / \"history\"\n",
    "LOGS_DIR     = REPORTS_DIR / \"logs\"\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️⃣ Execute full AG2 pipeline\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\" Launching full AG2 pipeline (Framework A → Coordinator → Framework B)…\")\n",
    "start_time = dt.datetime.now(dt.UTC)\n",
    "\n",
    "try:\n",
    "    A_summary, B_summary = coordinator.pipeline(\n",
    "        region=WF_REGION,\n",
    "        date_from=WF_DATE_FROM,\n",
    "        date_to=WF_DATE_TO\n",
    "    )\n",
    "    status = \"success\"\n",
    "except Exception as e:\n",
    "    status = f\"failed: {e}\"\n",
    "    A_summary, B_summary = {}, {}\n",
    "\n",
    "end_time = dt.datetime.now(dt.UTC)\n",
    "elapsed = (end_time - start_time).total_seconds()\n",
    "print(f\"  AG2 pipeline execution finished in {elapsed:.2f}s — status: {status.upper()}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️⃣ Validate and summarize latest artifacts\n",
    "# ---------------------------------------------------------------------------\n",
    "def _latest_json(prefix: str) -> Path | None:\n",
    "    files = sorted(HISTORY_DIR.glob(f\"*_{prefix}.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[-1] if files else None\n",
    "\n",
    "def _load_json(path: Path) -> dict:\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "latest_A = _latest_json(\"summary_A\")\n",
    "latest_B = _latest_json(\"summary_B\")\n",
    "data_A = _load_json(latest_A)\n",
    "data_B = _load_json(latest_B)\n",
    "\n",
    "print(\"\\n Latest artifacts:\")\n",
    "print(f\"• summary_A: {latest_A}\")\n",
    "print(f\"• summary_B: {latest_B}\")\n",
    "\n",
    "if isinstance(data_A, dict) and data_A:\n",
    "    region = data_A.get(\"region\", \"unknown\")\n",
    "    slope  = (data_A.get(\"signals\") or {}).get(\"slope_mean_deg\", None)\n",
    "    fires  = (data_A.get(\"counts\") or {}).get(\"fires\", None)\n",
    "    print(f\"   → Region: {region}, Fires: {fires}, Mean slope: {slope}\")\n",
    "else:\n",
    "    print(\"  No valid summary from Framework A — metrics skipped.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️⃣ Display last log entries (Coordinator + Framework B)\n",
    "# ---------------------------------------------------------------------------\n",
    "def _tail_log(file: Path, n: int = 10) -> list[str]:\n",
    "    if not file.exists():\n",
    "        return [\"(log not found)\"]\n",
    "    lines = file.read_text(encoding=\"utf-8\").splitlines()\n",
    "    return lines[-n:]\n",
    "\n",
    "print(\"\\n Coordinator log tail:\")\n",
    "for line in _tail_log(LOGS_DIR / \"coordinator.log\", 8):\n",
    "    print(\" \", line)\n",
    "\n",
    "print(\"\\n Framework B log tail:\")\n",
    "for line in _tail_log(LOGS_DIR / \"framework_b.log\", 8):\n",
    "    print(\" \", line)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5️⃣ Optional: summary dataframe for quick inspection\n",
    "# ---------------------------------------------------------------------------\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        \"run_id\": getattr(coordinator, \"run_id\", \"unknown\"),\n",
    "        \"status\": status,\n",
    "        \"fires_detected\": (data_A.get(\"counts\") or {}).get(\"fires\", None),\n",
    "        \"mean_slope_deg\": (data_A.get(\"signals\") or {}).get(\"slope_mean_deg\", None),\n",
    "        \"materials_found\": len((data_B.get(\"candidates_top3\") or [])),\n",
    "        \"elapsed_s\": round(elapsed, 2),\n",
    "        \"timestamp\": end_time.isoformat(timespec=\"seconds\")\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n AG2 Run Summary:\")\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac60bb1-632b-476d-b6aa-70c682e0354e",
   "metadata": {},
   "source": [
    "## ==== Cell 5 — AG² Interactive Chat Console (Agents-Driven) ====\n",
    "\n",
    "Conversational interface for **WildfiresAI (AG² System)**.\n",
    "\n",
    "All responses are generated internally by the Framework A (Wildfire Intelligence)\n",
    "and Framework B (Materials Intelligence) agents \n",
    "\n",
    "  Flow:\n",
    "Person → Chat → Coordinator → Framework A → Framework B → Answer + Map  \n",
    "Outputs are logged under `/reports/` for traceability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6d2d8-3228-40e4-a760-5a191240a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 AG² Interactive Chat Console ready.\n",
      "Type a question about wildfires (or 'exit' to quit).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "💬  Ask WildfiresAI (type your question):  fires in spain last month\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧭 Detected region: SPAIN\n",
      " Processing your query through AG² pipeline...\n",
      "[2025-10-21T18:06:26+00:00] ══════════ AG2 PIPELINE START ══════════\n",
      "[2025-10-21T18:06:26+00:00] Running Framework A (Wildfire Intelligence)…\n",
      " Framework A executed successfully for region=SPAIN\n",
      "[2025-10-21T18:06:26+00:00] Framework A OK — 6 fields.\n",
      "[2025-10-21T18:06:26+00:00] Running Framework B (Materials Intelligence)…\n",
      "[2025-10-21T18:06:26+00:00] Context built for region=SPAIN, T=41.6°C, RH=30%\n",
      "[2025-10-21T18:06:26+00:00] ⚠️ MP_API fallback: MaterialsRester.search() got an unexpected keyword argument 'energy_above_hull'\n",
      "[2025-10-21T18:06:26+00:00] MaterialsAgent returned 0 candidates (fallback).\n",
      "[2025-10-21T18:06:26+00:00] Swarm plan created for 8 drones in ridge_zone.\n",
      "[2025-10-21T18:06:26+00:00] Simulation done: efficiency=0.44, saved=719.9 ha.\n",
      "[2025-10-21T18:06:26+00:00] ActuationAgent: mission executed.\n",
      " Framework B executed successfully for region=SPAIN\n",
      "[2025-10-21T18:06:26+00:00] Framework B OK — 5 fields.\n",
      "[2025-10-21T18:06:26+00:00] AG2 pipeline completed successfully.\n",
      "[2025-10-21T18:06:26+00:00] ══════════ AG2 PIPELINE END ══════════\n",
      "\n",
      " WildfiresAI: [AG²] Region=SPAIN | Fires=43 | Slope≈4.72° | Materials=0 | Efficiency=?\n",
      "⏱️  Completed in 0.04s.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 5 — AG² Interactive Chat Console (Agent-Driven Intelligence, Context-Aware) ====\n",
    "from __future__ import annotations\n",
    "import json, datetime as dt, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Environment\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "CHAT_LOG     = REPORTS_DIR / \"chat_log.json\"\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper: detect region keyword from user question\n",
    "# ---------------------------------------------------------------------------\n",
    "def detect_region(question: str) -> str:\n",
    "    \"\"\"Infer region from text query (simple heuristic, customizable).\"\"\"\n",
    "    q = question.lower()\n",
    "    regions = {\n",
    "        \"spain\": \"SPAIN\", \"portugal\": \"PORTUGAL\", \"france\": \"FRANCE\",\n",
    "        \"italy\": \"ITALY\", \"usa\": \"USA\", \"united states\": \"USA\",\n",
    "        \"canada\": \"CANADA\", \"australia\": \"AUSTRALIA\", \"global\": \"GLOBAL\"\n",
    "    }\n",
    "    for key, value in regions.items():\n",
    "        if key in q:\n",
    "            return value\n",
    "    return \"GLOBAL\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper: summarize Framework A + B results\n",
    "# ---------------------------------------------------------------------------\n",
    "def summarize_agents(A: dict, B: dict) -> str:\n",
    "    slope  = (A.get(\"signals\") or {}).get(\"slope_mean_deg\", \"?\")\n",
    "    fires  = (A.get(\"counts\")  or {}).get(\"fires\", \"?\")\n",
    "    mats   = len(B.get(\"candidates_top3\", []))\n",
    "    eff    = (B.get(\"wildfire_context\") or {}).get(\"efficiency\", \"?\")\n",
    "    return f\"[AG²] Region={A.get('region','?')} | Fires={fires} | Slope≈{slope}° | Materials={mats} | Efficiency={eff}\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Interactive chat console\n",
    "# ---------------------------------------------------------------------------\n",
    "def ag2_chat():\n",
    "    print(\"💬 AG² Interactive Chat Console ready.\")\n",
    "    print(\"Type a question about wildfires (or 'exit' to quit).\")\n",
    "\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        q = input(\"\\n💬  Ask WildfiresAI (type your question): \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Session ended.\")\n",
    "            break\n",
    "\n",
    "        region_guess = detect_region(q)\n",
    "        print(f\"🧭 Detected region: {region_guess}\")\n",
    "        print(\" Processing your query through AG² pipeline...\")\n",
    "\n",
    "        start = dt.datetime.now(dt.UTC)\n",
    "        try:\n",
    "            A, B = coordinator.pipeline(region=region_guess, date_from=WF_DATE_FROM, date_to=WF_DATE_TO)\n",
    "            summary_text = summarize_agents(A.model_dump(), B.model_dump())\n",
    "\n",
    "            response = {\n",
    "                \"question\": q,\n",
    "                \"region\": region_guess,\n",
    "                \"answer\": summary_text,\n",
    "                \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "            }\n",
    "            chat_history.append(response)\n",
    "            print(f\"\\n WildfiresAI: {summary_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Pipeline error: {e}\")\n",
    "\n",
    "        end = dt.datetime.now(dt.UTC)\n",
    "        print(f\"⏱️  Completed in {(end - start).total_seconds():.2f}s.\")\n",
    "\n",
    "    # Save chat history\n",
    "    with open(CHAT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chat_history, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n Chat logged to {CHAT_LOG}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Launch console\n",
    "# ---------------------------------------------------------------------------\n",
    "ag2_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5641fd-a9d8-4a3a-ae4b-38328c950907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779259e-7e44-42ff-ad58-3bb3b49d7300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WildfiresAI)",
   "language": "python",
   "name": "wildfiresai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
