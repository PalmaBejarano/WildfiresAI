{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5502748b-524c-4d49-9eb6-ed13af2f9b43",
   "metadata": {},
   "source": [
    "# WILDFIRESAI - DATA PIPELINE (Base Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2ac49-0656-4dee-afe9-94c04ca653e8",
   "metadata": {},
   "source": [
    "## Cell 1 — Setup (one-time dependency install)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f989025-05ba-496a-8094-f8d66ba2571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet pystac-client planetary-computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1ea666-20a9-4d08-9e28-ae7edea1a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pystac-client: 0.9.0\n",
      "planetary-computer: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "print(\"pystac-client:\", importlib.import_module(\"pystac_client\").__version__)\n",
    "print(\"planetary-computer:\", importlib.import_module(\"planetary_computer\").__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebbc1c-ab5b-4176-9d84-83f47393c4b4",
   "metadata": {},
   "source": [
    "## Cell 2 – Scientific/Infra Stack & Version Audit\n",
    "Load the full geospatial + scientific + infra stack once.  \n",
    "Print versions for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d3c510-8753-4d01-9185-f591a962e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5\n",
      "NumPy: 2.3.3 | Pandas: 2.3.2 | Matplotlib: 3.10.6\n",
      "Requests: 2.32.4 | OpenAI SDK: 1.107.1\n",
      "GeoPandas: 1.1.1 | Rasterio: 1.4.3 | Xarray: 2025.9.1\n",
      "rioxarray: 0.19.0 | pyproj: 3.7.2 | Shapely: 2.1.2\n",
      "Fiona: 1.10.1 | Contextily: 1.6.2\n",
      "pystac-client: 0.9.0 | Planetary Computer module: planetary_computer\n",
      "structlog: 25.4.0 | pyarrow: 21.0.0\n",
      "tqdm: 4.67.1 | PyYAML: 6.0.3\n"
     ]
    }
   ],
   "source": [
    "## Cell 2 — Imports & Version Audit (centralized)\n",
    "\n",
    "# ---- Core Python / Utilities ----\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "\n",
    "# ---- Scientific ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "# ---- Geospatial & Remote Sensing ----\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.merge import merge as rio_merge      # used to mosaic rasters\n",
    "from rasterio.windows import Window                # used to window/clip rasters\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import pyproj\n",
    "import shapely\n",
    "import fiona\n",
    "\n",
    "# ---- Mapping / Visualization ----\n",
    "import folium\n",
    "import contextily as ctx\n",
    "\n",
    "# ---- STAC / Planetary (for ESA WorldCover) ----\n",
    "from pystac_client import Client                   # STAC search\n",
    "import planetary_computer as pc                    # signed asset URLs\n",
    "\n",
    "# ---- Infra / Project Support ----\n",
    "from dotenv import load_dotenv\n",
    "import structlog\n",
    "import tqdm                                        # module-level import\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import yaml                                        # pyyaml\n",
    "import importlib.metadata as im\n",
    "\n",
    "# ---- Version Audit ----\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__, \"| Pandas:\", pd.__version__, \"| Matplotlib:\", plt.matplotlib.__version__)\n",
    "print(\"Requests:\", requests.__version__, \"| OpenAI SDK:\", getattr(openai, \"__version__\", \"unknown\"))\n",
    "print(\"GeoPandas:\", gpd.__version__, \"| Rasterio:\", rasterio.__version__, \"| Xarray:\", xr.__version__)\n",
    "print(\"rioxarray:\", rioxarray.__version__, \"| pyproj:\", pyproj.__version__, \"| Shapely:\", shapely.__version__)\n",
    "print(\"Fiona:\", fiona.__version__, \"| Contextily:\", ctx.__version__)\n",
    "print(\"pystac-client:\", im.version(\"pystac-client\"), \"| Planetary Computer module:\", pc.__name__)\n",
    "print(\"structlog:\", getattr(structlog, '__version__', 'ok'), \"| pyarrow:\", pa.__version__)\n",
    "print(\"tqdm:\", im.version(\"tqdm\"), \"| PyYAML:\", yaml.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f398528-0bdb-4279-ac08-5c62fb7c1852",
   "metadata": {},
   "source": [
    "## Cell 3 – Project Header & Global Configuration\n",
    "Defines project paths, environment variables, logging, and small I/O helpers.  \n",
    "Ensures reproducibility, consistent data handling, and clean outputs across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2d9096-74b4-45f6-9b77-17d190d20567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-30 20:22:44\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mInit config                   \u001b[0m \u001b[36mprocessed\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI/data/processed')\u001b[0m \u001b[36mraw\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI/data/raw')\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36mreports\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI/reports')\u001b[0m \u001b[36mroot\u001b[0m=\u001b[35mPosixPath('/Users/evareysanchez/WildfiresAI')\u001b[0m\n",
      "Paths: raw=raw, processed=processed, reports=reports\n",
      "Dates: 2025-09-23 → 2025-09-30, Window=7 days\n",
      "Region: ES | BBOX: (-9.5, 35.0, 3.5, 43.9)\n",
      "Secrets: OpenAI=sk-pro…, FIRMS_TOKEN=eyJ0eX…, MAP_KEY=27f8d7…\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3: Project Header & Global Configuration ====\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import structlog\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "\n",
    "# ---- Project paths (shared) ----\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "\n",
    "for p in (DATA_DIR, RAW_DIR, PROCESSED_DIR, REPORTS_DIR, CONFIG_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Spatial / temporal defaults ----\n",
    "SPAIN_BBOX = (-9.5, 35.0, 3.5, 43.9)  # WGS84 (EPSG:4326)\n",
    "\n",
    "TODAY = date.today()\n",
    "DAYS_BACK = 7  # default temporal window\n",
    "DATE_FROM = os.getenv(\"WF_DATE_FROM\", str(TODAY - timedelta(days=DAYS_BACK)))\n",
    "DATE_TO   = os.getenv(\"WF_DATE_TO\", str(TODAY))\n",
    "\n",
    "# ---- Secrets (credentials if available) ----\n",
    "load_dotenv()  # Load from .env at project root\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "FIRMS_TOKEN    = os.getenv(\"FIRMS_TOKEN\", \"\")   # optional (Bearer endpoints)\n",
    "FIRMS_MAP_KEY  = os.getenv(\"FIRMS_MAP_KEY\", \"\") # required for FIRMS CSV API if used\n",
    "WF_REGION      = os.getenv(\"WF_REGION\", \"ES\")\n",
    "\n",
    "# ---- Lightweight logging (structured) ----\n",
    "structlog.configure(\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(20)  # INFO\n",
    ")\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(region=WF_REGION)\n",
    "log.info(\"Init config\",\n",
    "         root=PROJECT_ROOT,\n",
    "         raw=RAW_DIR,\n",
    "         processed=PROCESSED_DIR,\n",
    "         reports=REPORTS_DIR)\n",
    "\n",
    "# ---- Small helpers (I/O, display, masking) ----\n",
    "def mask_key(key: str, n: int = 6) -> str:\n",
    "    \"\"\"Mask sensitive keys, keeping only first n chars.\"\"\"\n",
    "    return key[:n] + \"…\" if key else \"None\"\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to CSV at the given path (parents created if needed).\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    log.info(\"Saved CSV\", path=str(path))\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to Parquet at the given path.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "    log.info(\"Saved Parquet\", path=str(path))\n",
    "\n",
    "def preview(df: pd.DataFrame, n: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"Return the first n rows for consistent preview across the notebook.\"\"\"\n",
    "    return df.head(n)\n",
    "\n",
    "# ---- Echo effective configuration (concise) ----\n",
    "print(\"Paths:\", f\"raw={RAW_DIR.name}, processed={PROCESSED_DIR.name}, reports={REPORTS_DIR.name}\")\n",
    "print(\"Dates:\", f\"{DATE_FROM} → {DATE_TO}, Window={DAYS_BACK} days\")\n",
    "print(\"Region:\", WF_REGION, \"| BBOX:\", SPAIN_BBOX)\n",
    "print(\"Secrets:\", f\"OpenAI={mask_key(OPENAI_API_KEY)}, \"\n",
    "                 f\"FIRMS_TOKEN={mask_key(FIRMS_TOKEN)}, \"\n",
    "                 f\"MAP_KEY={mask_key(FIRMS_MAP_KEY)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d4a3a-c849-4a6c-b8ab-6256304aaa66",
   "metadata": {},
   "source": [
    "## Cell 4 – Unified Source Configuration\n",
    "Defines all external data sources for fires, weather, climate, hydrology, and terrain.  \n",
    "Centralized registry ensures consistency, reproducibility, and easy updates across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25ce2be-ed97-46db-8ca5-3052ed91a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active region: ES | BBOX: (-9.5, 35.0, 3.5, 43.9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>name</th>\n",
       "      <th>enabled</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fires</td>\n",
       "      <td>FIRMS</td>\n",
       "      <td>True</td>\n",
       "      <td>Global near real-time fire detections (VIIRS/M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fires</td>\n",
       "      <td>EFFIS</td>\n",
       "      <td>True</td>\n",
       "      <td>Europe fire perimeters and burned area (via Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fires</td>\n",
       "      <td>NIFC</td>\n",
       "      <td>True</td>\n",
       "      <td>United States wildfire perimeters/incidents (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>OPEN_METEO_FORECAST</td>\n",
       "      <td>True</td>\n",
       "      <td>Global short-term forecast (GFS-backed when mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weather</td>\n",
       "      <td>OPEN_METEO_ARCHIVE</td>\n",
       "      <td>True</td>\n",
       "      <td>Historical archive (ERA5-backed hourly data).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weather</td>\n",
       "      <td>ERA5_ALIAS</td>\n",
       "      <td>True</td>\n",
       "      <td>Handled via OPEN_METEO_ARCHIVE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weather</td>\n",
       "      <td>GFS_ALIAS</td>\n",
       "      <td>True</td>\n",
       "      <td>Handled via OPEN_METEO_FORECAST.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hydrology</td>\n",
       "      <td>GRACE</td>\n",
       "      <td>True</td>\n",
       "      <td>Groundwater/drought indicators (NASA GRACE).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>terrain</td>\n",
       "      <td>GLOBAL_BASELINE</td>\n",
       "      <td>True</td>\n",
       "      <td>Global baseline terrain and land cover.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>terrain</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>True</td>\n",
       "      <td>European terrain and land cover products.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>terrain</td>\n",
       "      <td>USA</td>\n",
       "      <td>True</td>\n",
       "      <td>High-resolution DEM for the United States.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        group                 name  enabled  \\\n",
       "0       fires                FIRMS     True   \n",
       "1       fires                EFFIS     True   \n",
       "2       fires                 NIFC     True   \n",
       "3     weather  OPEN_METEO_FORECAST     True   \n",
       "4     weather   OPEN_METEO_ARCHIVE     True   \n",
       "5     weather           ERA5_ALIAS     True   \n",
       "6     weather            GFS_ALIAS     True   \n",
       "7   hydrology                GRACE     True   \n",
       "8     terrain      GLOBAL_BASELINE     True   \n",
       "9     terrain               EUROPE     True   \n",
       "10    terrain                  USA     True   \n",
       "\n",
       "                                               detail  \n",
       "0   Global near real-time fire detections (VIIRS/M...  \n",
       "1   Europe fire perimeters and burned area (via Co...  \n",
       "2   United States wildfire perimeters/incidents (N...  \n",
       "3   Global short-term forecast (GFS-backed when mo...  \n",
       "4       Historical archive (ERA5-backed hourly data).  \n",
       "5                     Handled via OPEN_METEO_ARCHIVE.  \n",
       "6                    Handled via OPEN_METEO_FORECAST.  \n",
       "7        Groundwater/drought indicators (NASA GRACE).  \n",
       "8             Global baseline terrain and land cover.  \n",
       "9           European terrain and land cover products.  \n",
       "10         High-resolution DEM for the United States.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4: Unified Source Configuration ====\n",
    "# Sources are declared here and controlled via environment variables when required.\n",
    "\n",
    "# ---- Fire sources ----\n",
    "SRC_FIRES = {\n",
    "    \"FIRMS\": {\n",
    "        \"enabled\": True,\n",
    "        \"endpoint\": os.getenv(\"FIRMS_CSV_URL\", \"\"),\n",
    "        \"token\": os.getenv(\"FIRMS_TOKEN\", \"\"),\n",
    "        \"map_key\": os.getenv(\"FIRMS_MAP_KEY\", \"\"),\n",
    "        \"notes\": \"Global near real-time fire detections (VIIRS/MODIS).\"\n",
    "    },\n",
    "    \"EFFIS\": {\n",
    "        \"enabled\": True,\n",
    "        \"wfs_url\": os.getenv(\"EFFIS_WFS_URL\", \"\"),\n",
    "        \"notes\": \"Europe fire perimeters and burned area (via Copernicus Emergency).\"\n",
    "    },\n",
    "    \"NIFC\": {\n",
    "        \"enabled\": True,\n",
    "        \"featureserver_url\": os.getenv(\"NIFC_FEATURESERVER_URL\", \"\"),\n",
    "        \"notes\": \"United States wildfire perimeters/incidents (NIFC FeatureServer).\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# ---- Weather & climate (via Open-Meteo) ----\n",
    "SRC_WEATHER = {\n",
    "    \"OPEN_METEO_FORECAST\": {\n",
    "        \"enabled\": True,\n",
    "        \"base_url\": \"https://api.open-meteo.com/v1/forecast\",\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\", \"relative_humidity_2m\", \"windspeed_10m\",\n",
    "            \"winddirection_10m\", \"windgusts_10m\", \"dew_point_2m\",\n",
    "            \"precipitation\", \"surface_pressure\"\n",
    "        ],\n",
    "        \"models\": os.getenv(\"OM_MODELS\", \"gfs_seamless\"),\n",
    "        \"notes\": \"Global short-term forecast (GFS-backed when models=gfs_*).\"\n",
    "    },\n",
    "    \"OPEN_METEO_ARCHIVE\": {\n",
    "        \"enabled\": True,\n",
    "        \"base_url\": \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\", \"relative_humidity_2m\", \"windspeed_10m\",\n",
    "            \"winddirection_10m\", \"dew_point_2m\", \"precipitation\",\n",
    "            \"surface_pressure\"\n",
    "        ],\n",
    "        \"notes\": \"Historical archive (ERA5-backed hourly data).\"\n",
    "    },\n",
    "    \"ERA5_ALIAS\": {\"enabled\": True, \"notes\": \"Handled via OPEN_METEO_ARCHIVE.\"},\n",
    "    \"GFS_ALIAS\": {\"enabled\": True, \"notes\": \"Handled via OPEN_METEO_FORECAST.\"},\n",
    "}\n",
    "\n",
    "# ---- Hydrology / drought ----\n",
    "SRC_HYDRO = {\n",
    "    \"GRACE\": {\n",
    "        \"enabled\": True,\n",
    "        \"url\": os.getenv(\"GRACE_URL\", \"\"),\n",
    "        \"earthdata_user\": os.getenv(\"EARTHDATA_USERNAME\", \"\"),\n",
    "        \"earthdata_pass\": os.getenv(\"EARTHDATA_PASSWORD\", \"\"),\n",
    "        \"notes\": \"Groundwater/drought indicators (NASA GRACE).\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ---- Terrain & land cover ----\n",
    "SRC_TERRAIN = {\n",
    "    \"GLOBAL_BASELINE\": {\n",
    "        \"enabled\": True,\n",
    "        \"dem\": \"Copernicus DEM GLO-30\",\n",
    "        \"landcover\": \"ESA WorldCover 10m\",\n",
    "        \"notes\": \"Global baseline terrain and land cover.\"\n",
    "    },\n",
    "    \"EUROPE\": {\n",
    "        \"enabled\": True,\n",
    "        \"dem\": \"EU-DEM 25m\",\n",
    "        \"landcover\": \"CORINE Land Cover\",\n",
    "        \"notes\": \"European terrain and land cover products.\"\n",
    "    },\n",
    "    \"USA\": {\n",
    "        \"enabled\": True,\n",
    "        \"dem\": \"USGS 3DEP 1m\",\n",
    "        \"notes\": \"High-resolution DEM for the United States.\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# ---- Region windows (BBOX) ----\n",
    "REGION_CFG = {\n",
    "    \"ES\": {\"bbox\": (-9.5, 35.0, 3.5, 43.9)},        # Spain\n",
    "    \"US\": {\"bbox\": (-125.0, 24.4, -66.9, 49.4)},    # Conterminous US\n",
    "    \"EU\": {\"bbox\": (-25.0, 34.0, 45.0, 72.0)},      # Europe\n",
    "}\n",
    "ACTIVE_BBOX = REGION_CFG.get(WF_REGION, REGION_CFG[\"ES\"])[\"bbox\"]\n",
    "\n",
    "# ---- Central registry ----\n",
    "CFG_SOURCES = {\n",
    "    \"region\": WF_REGION,\n",
    "    \"bbox\": ACTIVE_BBOX,\n",
    "    \"window\": {\"from\": DATE_FROM, \"to\": DATE_TO},\n",
    "    \"fires\": SRC_FIRES,\n",
    "    \"weather\": SRC_WEATHER,\n",
    "    \"hydrology\": SRC_HYDRO,\n",
    "    \"terrain\": SRC_TERRAIN,\n",
    "}\n",
    "\n",
    "# ---- Summary table ----\n",
    "def summarize_sources(cfg: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for group, entries in cfg.items():\n",
    "        if isinstance(entries, dict):\n",
    "            for name, meta in entries.items():\n",
    "                if isinstance(meta, dict) and \"enabled\" in meta:\n",
    "                    rows.append({\n",
    "                        \"group\": group,\n",
    "                        \"name\": name,\n",
    "                        \"enabled\": meta[\"enabled\"],\n",
    "                        \"detail\": meta.get(\"notes\", \"\")\n",
    "                    })\n",
    "    return pd.DataFrame(rows, columns=[\"group\",\"name\",\"enabled\",\"detail\"])\n",
    "\n",
    "print(\"Active region:\", CFG_SOURCES[\"region\"], \"| BBOX:\", CFG_SOURCES[\"bbox\"])\n",
    "display(summarize_sources(CFG_SOURCES))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67514803-b34e-40a6-a960-b1fa2828f221",
   "metadata": {},
   "source": [
    "## Cell 5 – IngestAgent (Fires, Weather, Hydrology)\n",
    "Downloads and caches raw datasets: FIRMS, EFFIS (WFS), NIFC (FeatureServer), Open-Meteo (forecast/archive), and GRACE.\n",
    "Outputs are stored under `data/raw/<source>/...` with a concise ingest report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687f63fd-ad29-4c6e-be31-e8aa4f9fd6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-30 20:22:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mfirms_csv_downloaded          \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/raw/firms_viirs_snpp_2025-09-23_2025-09-30_20250930T182251Z.csv\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mopenmeteo_downloaded          \u001b[0m \u001b[36mlabel\u001b[0m=\u001b[35marchive\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/raw/openmeteo_archive/archive_39.450_-3.000_2025-09-23_2025-09-30_20250930T182251Z.json\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:51\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mopenmeteo_downloaded          \u001b[0m \u001b[36mlabel\u001b[0m=\u001b[35mforecast\u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/raw/openmeteo_forecast/forecast_39.450_-3.000_2025-09-23_2025-09-30_20250930T182251Z.json\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mnifc_geojson_downloaded       \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/raw/nifc/perimeters_2025-09-23_2025-09-30_20250930T182251Z.geojson\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "Ingest report: /Users/evareysanchez/WildfiresAI/reports/ingest_report_ES_2025-09-23_2025-09-30.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>artifact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>firms</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/raw/firm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>om_archive</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/raw/open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>om_forecast</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/raw/open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nifc_perimeters</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/raw/nifc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source                                           artifact\n",
       "0            firms  /Users/evareysanchez/WildfiresAI/data/raw/firm...\n",
       "1       om_archive  /Users/evareysanchez/WildfiresAI/data/raw/open...\n",
       "2      om_forecast  /Users/evareysanchez/WildfiresAI/data/raw/open...\n",
       "3  nifc_perimeters  /Users/evareysanchez/WildfiresAI/data/raw/nifc..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cell 5 — IngestAgent (FIRMS + Weather + hooks EFFIS/NIFC)\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import structlog\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- env & paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_dotenv()  # read .env once\n",
    "log = structlog.get_logger(\"ingest\").bind(region=os.getenv(\"WF_REGION\", \"ES\"))\n",
    "\n",
    "# --- config (from Cell 3 defaults)\n",
    "SPAIN_BBOX = globals().get(\"SPAIN_BBOX\", (-9.5, 35.0, 3.5, 43.9))\n",
    "DATE_FROM = globals().get(\"DATE_FROM\")\n",
    "DATE_TO   = globals().get(\"DATE_TO\")\n",
    "\n",
    "# --- endpoints & keys\n",
    "FIRMS_MAP_KEY  = os.getenv(\"FIRMS_MAP_KEY\")\n",
    "FIRMS_TOKEN    = os.getenv(\"FIRMS_TOKEN\")          # optional (Bearer)\n",
    "FIRMS_CSV_URL  = os.getenv(\"FIRMS_CSV_URL\")        # required for CSV mode\n",
    "\n",
    "EFFIS_WFS_URL  = os.getenv(\"EFFIS_WFS_URL\")        # optional\n",
    "NIFC_FS_URL    = os.getenv(\"NIFC_FS_URL\")          # optional\n",
    "\n",
    "def _bbox_dict(bbox: Tuple[float,float,float,float]) -> Dict[str, float]:\n",
    "    w, s, e, n = bbox\n",
    "    return {\"west\": w, \"south\": s, \"east\": e, \"north\": n}\n",
    "\n",
    "def _save_json(obj: Any, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "artifacts = []\n",
    "\n",
    "# --- 5.1 FIRMS (CSV mode with MAP_KEY) ---------------------------------\n",
    "try:\n",
    "    if not FIRMS_CSV_URL:\n",
    "        log.warning(\"firms_endpoint_missing\", hint=\"Set FIRMS_CSV_URL in env\")\n",
    "    else:\n",
    "        from datetime import timezone\n",
    "        dt_from = datetime.fromisoformat(str(DATE_FROM))\n",
    "        dt_to   = datetime.fromisoformat(str(DATE_TO))\n",
    "        window_days = max(1, min((dt_to - dt_from).days or 1, 10))\n",
    "\n",
    "        # Compose URL directly with bbox and day inside the path\n",
    "        w, s, e, n = SPAIN_BBOX\n",
    "        url = f\"{FIRMS_CSV_URL}/{w},{s},{e},{n}/{window_days}\"\n",
    "\n",
    "        r = requests.get(url, timeout=90)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        out_csv = RAW_DIR / f\"firms_viirs_snpp_{DATE_FROM}_{DATE_TO}_{ts}.csv\"\n",
    "        out_csv.write_text(r.text, encoding=\"utf-8\")\n",
    "\n",
    "        log.info(\"firms_csv_downloaded\", path=str(out_csv))\n",
    "        artifacts.append((\"firms\", str(out_csv)))\n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"firms_download_failed\", err=str(e))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 5.2 Open-Meteo (archive + forecast) --------------------------------\n",
    "try:\n",
    "    W, S, E, N = SPAIN_BBOX\n",
    "    lat_c, lon_c = (S + N) / 2.0, (W + E) / 2.0\n",
    "\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    hourly_vars = [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\",\n",
    "        \"windspeed_10m\", \"windgusts_10m\", \"winddirection_10m\",\n",
    "        \"precipitation\", \"surface_pressure\"\n",
    "    ]\n",
    "    params_common = {\n",
    "        \"latitude\": round(lat_c, 4),\n",
    "        \"longitude\": round(lon_c, 4),\n",
    "        \"start_date\": str(DATE_FROM),\n",
    "        \"end_date\": str(DATE_TO),\n",
    "        \"hourly\": \",\".join(hourly_vars),\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "\n",
    "    # Archive (ERA5-backed)\n",
    "    om_archive = os.getenv(\"OM_ARCHIVE\", \"https://archive-api.open-meteo.com/v1/archive\")\n",
    "    r = requests.get(om_archive, params=params_common, timeout=90)\n",
    "    r.raise_for_status()\n",
    "    out_archive = RAW_DIR / \"openmeteo_archive\" / f\"archive_{lat_c:.3f}_{lon_c:.3f}_{DATE_FROM}_{DATE_TO}_{ts}.json\"\n",
    "    out_archive.parent.mkdir(parents=True, exist_ok=True)\n",
    "    _save_json(r.json(), out_archive)\n",
    "    log.info(\"openmeteo_downloaded\", label=\"archive\", path=str(out_archive))\n",
    "    artifacts.append((\"om_archive\", str(out_archive)))\n",
    "\n",
    "    # Forecast (GFS-backed)\n",
    "    om_forecast = os.getenv(\"OM_FORECAST\", \"https://api.open-meteo.com/v1/forecast\")\n",
    "    r2 = requests.get(om_forecast, params={**params_common, \"models\": \"gfs_seamless\"}, timeout=90)\n",
    "    r2.raise_for_status()\n",
    "    out_forecast = RAW_DIR / \"openmeteo_forecast\" / f\"forecast_{lat_c:.3f}_{lon_c:.3f}_{DATE_FROM}_{DATE_TO}_{ts}.json\"\n",
    "    out_forecast.parent.mkdir(parents=True, exist_ok=True)\n",
    "    _save_json(r2.json(), out_forecast)\n",
    "    log.info(\"openmeteo_downloaded\", label=\"forecast\", path=str(out_forecast))\n",
    "    artifacts.append((\"om_forecast\", str(out_forecast)))\n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"openmeteo_failed\", err=str(e))\n",
    "\n",
    "# --- 5.3 EFFIS (optional; WFS) -----------------------------------------\n",
    "try:\n",
    "    effis_url = os.getenv(\"EFFIS_WFS_URL\")\n",
    "    effis_lyr = os.getenv(\"EFFIS_TYPENAME\")\n",
    "    if effis_url and effis_lyr:\n",
    "        # Ejemplo de GetFeature WFS → GeoJSON\n",
    "        w, s, e, n = SPAIN_BBOX\n",
    "        params = {\n",
    "            \"service\": \"WFS\",\n",
    "            \"request\": \"GetFeature\",\n",
    "            \"version\": \"2.0.0\",\n",
    "            \"typeNames\": effis_lyr,             \n",
    "            \"srsName\": \"EPSG:4326\",\n",
    "            \"bbox\": f\"{s},{w},{n},{e},EPSG:4326\",\n",
    "            \"outputFormat\": \"application/json\"\n",
    "        }\n",
    "        r = requests.get(effis_url, params=params, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        out_effis = RAW_DIR / \"effis\" / f\"perimeters_{DATE_FROM}_{DATE_TO}_{ts}.geojson\"\n",
    "        out_effis.parent.mkdir(parents=True, exist_ok=True)\n",
    "        _save_json(r.json(), out_effis)\n",
    "        log.info(\"effis_geojson_downloaded\", path=str(out_effis))\n",
    "        artifacts.append((\"effis_perimeters\", str(out_effis)))\n",
    "    # Sin URL/typename → silencio (no “skipped”) hasta que tengas acceso oficial.\n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"effis_failed\", err=str(e))\n",
    "\n",
    "\n",
    "# --- 5.4 NIFC (USA perimeters; ArcGIS FeatureServer → GeoJSON) ---------\n",
    "try:\n",
    "    # Usamos un valor por defecto robusto si la var no está (vista \"Current\").\n",
    "    nifc_url = os.getenv(\n",
    "        \"NIFC_FS_URL\",\n",
    "        \"https://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services/WFIGS_Interagency_Perimeters_Current/FeatureServer/0\"\n",
    "    )\n",
    "\n",
    "    if nifc_url:\n",
    "       \n",
    "        w, s, e, n = SPAIN_BBOX  # puedes usar otra bbox según región\n",
    "        geometry = {\n",
    "            \"xmin\": w, \"ymin\": s, \"xmax\": e, \"ymax\": n,\n",
    "            \"spatialReference\": {\"wkid\": 4326}\n",
    "        }\n",
    "        q = {\n",
    "            \"where\": \"1=1\",\n",
    "            \"geometryType\": \"esriGeometryEnvelope\",\n",
    "            \"geometry\": json.dumps(geometry),\n",
    "            \"inSR\": 4326,\n",
    "            \"spatialRel\": \"esriSpatialRelIntersects\",\n",
    "            \"outFields\": \"*\",\n",
    "            \"returnGeometry\": \"true\",\n",
    "            \"f\": \"geojson\"  # salida directa GeoJSON\n",
    "        }\n",
    "        r = requests.get(f\"{nifc_url}/query\", params=q, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        gj = r.json()\n",
    "\n",
    "        out_nifc = RAW_DIR / \"nifc\" / f\"perimeters_{DATE_FROM}_{DATE_TO}_{ts}.geojson\"\n",
    "        out_nifc.parent.mkdir(parents=True, exist_ok=True)\n",
    "        _save_json(gj, out_nifc)\n",
    "        log.info(\"nifc_geojson_downloaded\", path=str(out_nifc))\n",
    "        artifacts.append((\"nifc_perimeters\", str(out_nifc)))\n",
    "   \n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"nifc_failed\", err=str(e))\n",
    "\n",
    "\n",
    "# --- 5.5 Report ---------------------------------------------------------\n",
    "ingest_report = REPORTS_DIR / f\"ingest_report_{os.getenv('WF_REGION','ES')}_{DATE_FROM}_{DATE_TO}.json\"\n",
    "_save_json({\"artifacts\": artifacts}, ingest_report)\n",
    "print(f\"Ingest report: {ingest_report}\")\n",
    "\n",
    "# Display a compact table\n",
    "pd.DataFrame(artifacts, columns=[\"source\",\"artifact\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96462df1-4b6d-4621-9f15-ab537ad2365c",
   "metadata": {},
   "source": [
    "## Cell 6 – CleanerAgent (Robust Normalization)\n",
    "Normalize raw inputs into consistent, typed tables ready for terrain enrichment and spatiotemporal joins.\n",
    "\n",
    "**Outputs**\n",
    "- `data/processed/fires_clean.parquet`\n",
    "- `data/processed/effis_clean.parquet` (if available)\n",
    "- `data/processed/nifc_clean.parquet` (if available)\n",
    "- `data/processed/weather_points.parquet`\n",
    "- `reports/clean_report.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5c4bef-0006-46db-9ea9-4b4a45e36908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-30 20:22:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mfirms_clean_saved             \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/processed/firms_clean_ES_2025-09-23_2025-09-30.csv\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36mrows\u001b[0m=\u001b[35m737\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mweather_points_saved          \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/processed/weather_points.parquet\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36mrows\u001b[0m=\u001b[35m192\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:52\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mnifc_clean_empty              \u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "Clean report: /Users/evareysanchez/WildfiresAI/reports/clean_report_ES_2025-09-23_2025-09-30.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artifact</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fires</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/processe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weather_points</td>\n",
       "      <td>/Users/evareysanchez/WildfiresAI/data/processe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artifact                                              value\n",
       "0           fires  /Users/evareysanchez/WildfiresAI/data/processe...\n",
       "1  weather_points  /Users/evareysanchez/WildfiresAI/data/processe..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cell 6 — CleanerAgent (FIRMS + Open-Meteo + NIFC; robust validation & sampling)\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Paths from earlier cells\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "for p in (PROC_DIR, RAW_DIR, REPORTS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clean_artifacts: list[tuple[str,str]] = []\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def _save_report(obj: dict, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def _sniff_sep(path: Path) -> str:\n",
    "    # Delimiter sniffer between comma and semicolon\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    return \",\" if sample.count(\",\") >= sample.count(\";\") else \";\"\n",
    "\n",
    "def _find_col(colmap_lower: dict[str,str], candidates: list[str]) -> str | None:\n",
    "    # Return original-case column name for the first lower-cased match\n",
    "    for c in candidates:\n",
    "        if c in colmap_lower:\n",
    "            return colmap_lower[c]\n",
    "    return None\n",
    "\n",
    "def _build_datetime(df: pd.DataFrame, date_col: str | None, time_col: str | None, dt_col: str | None) -> pd.Series:\n",
    "    # Build timezone-aware timestamp (UTC). If missing parts, coerce to NaT.\n",
    "    if dt_col and dt_col in df.columns:\n",
    "        return pd.to_datetime(df[dt_col], errors=\"coerce\", utc=True)\n",
    "    if date_col and time_col and date_col in df.columns and time_col in df.columns:\n",
    "        t = df[time_col].astype(str).str.zfill(4)\n",
    "        return pd.to_datetime(\n",
    "            df[date_col].astype(str) + \" \" + t.str[:2] + \":\" + t.str[2:4] + \":00\",\n",
    "            errors=\"coerce\", utc=True\n",
    "        )\n",
    "    if date_col and date_col in df.columns:\n",
    "        return pd.to_datetime(df[date_col], errors=\"coerce\", utc=True)\n",
    "    return pd.to_datetime(pd.NaT)\n",
    "\n",
    "# ---------------- 6.1 Clean FIRMS (CSV) ----------------\n",
    "\n",
    "try:\n",
    "    firms_raw = sorted(RAW_DIR.glob(\"firms_viirs_snpp_*.csv\"))\n",
    "    if not firms_raw:\n",
    "        log.warning(\"clean_firms_missing\", hint=\"No FIRMS CSV found in data/raw/\")\n",
    "    else:\n",
    "        latest = firms_raw[-1]\n",
    "        sep = _sniff_sep(latest)\n",
    "\n",
    "        df_firms = pd.read_csv(latest, sep=sep, engine=\"python\", dtype={\"acq_time\": \"string\"})\n",
    "        df_firms.columns = [c.strip() for c in df_firms.columns]\n",
    "        colmap_lower = {c.lower(): c for c in df_firms.columns}\n",
    "\n",
    "        lat_col = _find_col(colmap_lower, [\"latitude\", \"lat\", \"y\", \"latitud\"])\n",
    "        lon_col = _find_col(colmap_lower, [\"longitude\", \"lon\", \"x\", \"longitud\", \"long\"])\n",
    "        if not lat_col or not lon_col:\n",
    "            raise KeyError(\"FIRMS CSV lacks latitude/longitude columns (tried latitude|lat|y and longitude|lon|x variants)\")\n",
    "\n",
    "        dt_col   = _find_col(colmap_lower, [\"acq_datetime\", \"datetime\", \"time_utc\"])\n",
    "        date_col = _find_col(colmap_lower, [\"acq_date\", \"date\", \"fecha\"])\n",
    "        time_col = _find_col(colmap_lower, [\"acq_time\", \"time\", \"hora\"])\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"lat\":  pd.to_numeric(df_firms[lat_col], errors=\"coerce\"),\n",
    "            \"lon\":  pd.to_numeric(df_firms[lon_col], errors=\"coerce\"),\n",
    "            \"datetime\": _build_datetime(df_firms, date_col, time_col, dt_col),\n",
    "            \"brightness\": pd.to_numeric(df_firms[_find_col(colmap_lower, [\"brightness\", \"bright_ti4\", \"bright_ti5\"])], errors=\"coerce\") if _find_col(colmap_lower, [\"brightness\",\"bright_ti4\",\"bright_ti5\"]) else pd.NA,\n",
    "            \"confidence\": pd.to_numeric(df_firms[_find_col(colmap_lower, [\"confidence\"])], errors=\"coerce\") if _find_col(colmap_lower, [\"confidence\"]) else pd.NA,\n",
    "            \"frp\": pd.to_numeric(df_firms[_find_col(colmap_lower, [\"frp\"])], errors=\"coerce\") if _find_col(colmap_lower, [\"frp\"]) else pd.NA,\n",
    "            \"satellite\": df_firms[_find_col(colmap_lower, [\"satellite\"])] if _find_col(colmap_lower, [\"satellite\"]) else pd.NA,\n",
    "            \"source\": \"FIRMS\"\n",
    "        })\n",
    "\n",
    "        out = out.dropna(subset=[\"lat\", \"lon\"]).drop_duplicates(subset=[\"datetime\",\"lat\",\"lon\"]).reset_index(drop=True)\n",
    "\n",
    "        ACTIVE_BBOX = globals().get(\"ACTIVE_BBOX\", globals().get(\"SPAIN_BBOX\"))\n",
    "        if ACTIVE_BBOX is not None:\n",
    "            w, s, e, n = ACTIVE_BBOX\n",
    "            out = out[(out[\"lon\"].between(w, e)) & (out[\"lat\"].between(s, n))].copy()\n",
    "\n",
    "        if \"DATE_FROM\" in globals() and \"DATE_TO\" in globals() and \"datetime\" in out.columns:\n",
    "            out = out[(out[\"datetime\"] >= pd.to_datetime(DATE_FROM, utc=True)) &\n",
    "                      (out[\"datetime\"] <= pd.to_datetime(DATE_TO,   utc=True))].copy()\n",
    "\n",
    "        region = os.getenv(\"WF_REGION\", \"ES\")\n",
    "        out_firms = PROC_DIR / f\"firms_clean_{region}_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "        out.to_csv(out_firms, index=False)\n",
    "        log.info(\"firms_clean_saved\", rows=len(out), path=str(out_firms))\n",
    "\n",
    "        sample_firms = REPORTS_DIR / f\"sample_firms_{region}_{DATE_FROM}_{DATE_TO}.json\"\n",
    "        out.head(10).to_json(sample_firms, orient=\"records\", indent=2)\n",
    "        clean_artifacts.append((\"fires\", str(out_firms)))\n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"clean_firms_fail\", err=str(e))\n",
    "\n",
    "# ---------------- 6.2 Clean Open-Meteo (archive JSON → parquet) ----------------\n",
    "\n",
    "try:\n",
    "    wx_archives = sorted((RAW_DIR / \"openmeteo_archive\").glob(\"archive_*.json\"))\n",
    "    if not wx_archives:\n",
    "        log.warning(\"clean_weather_missing\", hint=\"No Open-Meteo archive found\")\n",
    "    else:\n",
    "        js = json.load(open(wx_archives[-1], encoding=\"utf-8\"))\n",
    "        hourly = js.get(\"hourly\", {})\n",
    "        df_wx = pd.DataFrame(hourly)\n",
    "\n",
    "        if \"time\" in df_wx.columns:\n",
    "            df_wx[\"datetime\"] = pd.to_datetime(df_wx[\"time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "        out_wx = PROC_DIR / \"weather_points.parquet\"\n",
    "        df_wx.to_parquet(out_wx, index=False)\n",
    "        log.info(\"weather_points_saved\", rows=len(df_wx), path=str(out_wx))\n",
    "\n",
    "        sample_wx = REPORTS_DIR / f\"sample_weather_{os.getenv('WF_REGION','ES')}_{DATE_FROM}_{DATE_TO}.json\"\n",
    "        df_wx.head(10).to_json(sample_wx, orient=\"records\", indent=2)\n",
    "        clean_artifacts.append((\"weather_points\", str(out_wx)))\n",
    "except Exception as e:\n",
    "    log.warning(\"clean_weather_fail\", err=str(e))\n",
    "\n",
    "# ---------------- 6.3 Clean NIFC (GeoJSON perimeters → centroid points) ----------------\n",
    "try:\n",
    "    nifc_files = sorted((RAW_DIR / \"nifc\").glob(\"perimeters_*.geojson\"))\n",
    "    if nifc_files:\n",
    "        latest_nifc = nifc_files[-1]\n",
    "\n",
    "        # Prefer geopandas if available; otherwise use a robust JSON fallback\n",
    "        try:\n",
    "            import geopandas as gpd  # guarded import (no duplicate global imports)\n",
    "            gdf = gpd.read_file(latest_nifc)\n",
    "\n",
    "            # Ensure we start in geographic CRS for bbox filtering and then project to metric CRS\n",
    "            try:\n",
    "                gdf = gdf.to_crs(4326)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Compute centroids in a metric CRS for accuracy, then convert back to WGS84\n",
    "            try:\n",
    "                gdf_3857 = gdf.to_crs(3857)\n",
    "                cent_m = gdf_3857.geometry.centroid\n",
    "                cent_wgs84 = cent_m.to_crs(4326)\n",
    "                df_nifc = pd.DataFrame({\"lon\": cent_wgs84.x, \"lat\": cent_wgs84.y})\n",
    "            except Exception:\n",
    "                # Fallback if reprojection fails for some geometries\n",
    "                cent_wgs84 = gdf.to_crs(4326).geometry.centroid\n",
    "                df_nifc = pd.DataFrame({\"lon\": cent_wgs84.x, \"lat\": cent_wgs84.y})\n",
    "\n",
    "            # Try to extract a perimeter datetime from attributes if present\n",
    "            props = gdf.drop(columns=[\"geometry\"], errors=\"ignore\")\n",
    "            time_cols = [c for c in props.columns if any(k in c.lower() for k in [\"date\", \"time\", \"datetime\", \"perimeter\"])]\n",
    "            if time_cols:\n",
    "                df_nifc[\"datetime\"] = pd.to_datetime(props[time_cols[0]], errors=\"coerce\", utc=True)\n",
    "            else:\n",
    "                df_nifc[\"datetime\"] = pd.NaT\n",
    "\n",
    "            df_nifc[\"source\"] = \"NIFC\"\n",
    "\n",
    "        except Exception:\n",
    "            # JSON fallback: compute crude centroid (mean of ring vertices) per feature\n",
    "            js = json.load(open(latest_nifc, encoding=\"utf-8\"))\n",
    "            feats = js.get(\"features\", [])\n",
    "            rows = []\n",
    "            for f in feats:\n",
    "                geom = f.get(\"geometry\", {})\n",
    "                coords = []\n",
    "                if geom.get(\"type\") == \"Polygon\":\n",
    "                    for ring in geom.get(\"coordinates\", []):\n",
    "                        coords.extend(ring)\n",
    "                elif geom.get(\"type\") == \"MultiPolygon\":\n",
    "                    for poly in geom.get(\"coordinates\", []):\n",
    "                        for ring in poly:\n",
    "                            coords.extend(ring)\n",
    "                if coords:\n",
    "                    xs = [float(x) for x, y in coords]\n",
    "                    ys = [float(y) for x, y in coords]\n",
    "                    lon = sum(xs) / len(xs)\n",
    "                    lat = sum(ys) / len(ys)\n",
    "                    props = f.get(\"properties\", {})\n",
    "                    tfield = next((k for k in props.keys() if any(s in k.lower() for s in [\"date\",\"time\",\"datetime\",\"perimeter\"])), None)\n",
    "                    dt = pd.to_datetime(props.get(tfield), errors=\"coerce\", utc=True) if tfield else pd.NaT\n",
    "                    rows.append({\"lon\": lon, \"lat\": lat, \"datetime\": dt, \"source\": \"NIFC\"})\n",
    "            df_nifc = pd.DataFrame(rows)\n",
    "\n",
    "        # Optional spatial and temporal filters based on globals\n",
    "        ACTIVE_BBOX = globals().get(\"ACTIVE_BBOX\", globals().get(\"SPAIN_BBOX\"))\n",
    "        if ACTIVE_BBOX is not None and not df_nifc.empty:\n",
    "            w, s, e, n = ACTIVE_BBOX\n",
    "            df_nifc = df_nifc[(df_nifc[\"lon\"].between(w, e)) & (df_nifc[\"lat\"].between(s, n))].copy()\n",
    "\n",
    "        if \"DATE_FROM\" in globals() and \"DATE_TO\" in globals() and not df_nifc.empty and \"datetime\" in df_nifc.columns:\n",
    "            df_nifc = df_nifc[(df_nifc[\"datetime\"].isna()) |\n",
    "                              ((df_nifc[\"datetime\"] >= pd.to_datetime(DATE_FROM, utc=True)) &\n",
    "                               (df_nifc[\"datetime\"] <= pd.to_datetime(DATE_TO,   utc=True)))]\n",
    "\n",
    "        out_nifc = PROC_DIR / f\"nifc_clean_{os.getenv('WF_REGION','ES')}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "        if not df_nifc.empty:\n",
    "            df_nifc.to_parquet(out_nifc, index=False)\n",
    "            log.info(\"nifc_clean_saved\", rows=len(df_nifc), path=str(out_nifc))\n",
    "            clean_artifacts.append((\"nifc_clean\", str(out_nifc)))\n",
    "        else:\n",
    "            log.info(\"nifc_clean_empty\")\n",
    "    # If no NIFC file present, keep silent (clean pipeline)\n",
    "\n",
    "except Exception as e:\n",
    "    log.warning(\"clean_nifc_fail\", err=str(e))\n",
    "\n",
    "# ---------------- 6.4 Report ----------------\n",
    "\n",
    "clean_report = REPORTS_DIR / f\"clean_report_{os.getenv('WF_REGION','ES')}_{DATE_FROM}_{DATE_TO}.json\"\n",
    "_save_report({\"artifacts\": clean_artifacts}, clean_report)\n",
    "print(f\"Clean report: {clean_report}\")\n",
    "\n",
    "pd.DataFrame(clean_artifacts, columns=[\"artifact\",\"value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd450b9-3db1-4250-b4d0-81942609c962",
   "metadata": {},
   "source": [
    "## Cell 7 — GeoTerrainAgent (terrain enrichment with auto-fetch)\n",
    "Enrich fire detections with terrain attributes: elevation, slope, and land cover.\n",
    "- DEM: OpenTopography (requires OPENTOPO_API_KEY in .env)\n",
    "- Land cover: ESA WorldCover (via Planetary Computer STAC, fallback years)\n",
    " Output: terrain-enriched fires saved to Parquet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4dd42d1-3893-468b-bb31-756c6ef9978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-30 20:22:56\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdem_present                   \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/terrain/copernicus_dem.tif\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36msize\u001b[0m=\u001b[35m136749593\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:57\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mworldcover_found              \u001b[0m \u001b[36mcount\u001b[0m=\u001b[35m23\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36myear\u001b[0m=\u001b[35m2021\u001b[0m\n",
      "\u001b[2m2025-09-30 20:22:59\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mlandcover_clipped             \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/terrain/esa_worldcover_clip.tif\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36mshape\u001b[0m=\u001b[35m(22800, 6000)\u001b[0m\n",
      "\u001b[2m2025-09-30 20:23:00\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mdem_sample_failed             \u001b[0m \u001b[36merr\u001b[0m=\u001b[35m'Cannot convert fill_value nan to dtype int16'\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "\u001b[2m2025-09-30 20:23:00\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mlandcover_sampled             \u001b[0m \u001b[36mcount\u001b[0m=\u001b[35m737\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m\n",
      "\u001b[2m2025-09-30 20:23:00\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mfires_terrain_saved           \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/processed/fires_terrain_ES_2025-09-23_2025-09-30.parquet\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36mrows\u001b[0m=\u001b[35m737\u001b[0m\n",
      "Terrain-enriched fires saved: /Users/evareysanchez/WildfiresAI/data/processed/fires_terrain_ES_2025-09-23_2025-09-30.parquet\n"
     ]
    }
   ],
   "source": [
    "## Cell 7 — GeoTerrainAgent (terrain enrichment with auto-fetch, memory-safe)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "import structlog\n",
    "\n",
    "log = structlog.get_logger(\"geoterrain\").bind(region=os.getenv(\"WF_REGION\", \"ES\"))\n",
    "\n",
    "# --- config / paths\n",
    "REGION = os.getenv(\"WF_REGION\", \"ES\")\n",
    "W, S, E, N = globals().get(\"ACTIVE_BBOX\", globals().get(\"SPAIN_BBOX\", (-9.5, 35.0, 3.5, 43.9)))\n",
    "\n",
    "TERRAIN_DIR = PROJECT_ROOT / \"data\" / \"terrain\"\n",
    "TERRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEM_TIF = TERRAIN_DIR / \"copernicus_dem.tif\"\n",
    "LC_TIF  = TERRAIN_DIR / \"esa_worldcover_clip.tif\"\n",
    "\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fires_clean_path   = PROCESSED_DIR / f\"firms_clean_{REGION}_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "fires_terrain_out  = PROCESSED_DIR / f\"fires_terrain_{REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "\n",
    "# ----------------------- helpers: asset download/build -----------------------\n",
    "\n",
    "def _download_dem_opentopo(w: float, s: float, e: float, n: float, out_path: Path, demtype=\"SRTMGL3\") -> None:\n",
    "    \"\"\"\n",
    "    Fetch DEM GeoTIFF clipped to bbox using OpenTopography Global DEM API (SRTMGL3).\n",
    "    Requires OPENTOPO_API_KEY in .env.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    api_key = os.getenv(\"OPENTOPO_API_KEY\", \"\")\n",
    "    url = \"https://portal.opentopography.org/API/globaldem\"\n",
    "    params = {\n",
    "        \"demtype\": demtype,\n",
    "        \"south\": s, \"north\": n, \"west\": w, \"east\": e,\n",
    "        \"outputFormat\": \"GTiff\",\n",
    "        \"API_Key\": api_key\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=600); r.raise_for_status()\n",
    "    try:\n",
    "        js = r.json()\n",
    "        tif_url = None\n",
    "        if isinstance(js, dict) and \"result\" in js and \"links\" in js[\"result\"]:\n",
    "            for lnk in js[\"result\"][\"links\"]:\n",
    "                href = lnk.get(\"href\", \"\")\n",
    "                if href.endswith(\".tif\"):\n",
    "                    tif_url = href\n",
    "                    break\n",
    "        if not tif_url:\n",
    "            raise RuntimeError(\"No GeoTIFF link found in OpenTopography response.\")\n",
    "        r2 = requests.get(tif_url, timeout=1200); r2.raise_for_status()\n",
    "        out_path.write_bytes(r2.content)\n",
    "        log.info(\"dem_downloaded\", src=tif_url, path=str(out_path))\n",
    "    except Exception:\n",
    "        out_path.write_bytes(r.content)\n",
    "        log.info(\"dem_downloaded_direct\", bytes=len(r.content), path=str(out_path))\n",
    "\n",
    "def _build_worldcover_from_stac(w: float, s: float, e: float, n: float, out_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Query ESA WorldCover via Planetary Computer STAC, clip to bbox, and save to GeoTIFF.\n",
    "    Reads only the bounding window instead of entire mosaics (memory-safe).\n",
    "    \"\"\"\n",
    "    from pystac_client import Client\n",
    "    import planetary_computer as pc\n",
    "    import rasterio\n",
    "    from rasterio.windows import from_bounds\n",
    "\n",
    "    stac = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    bbox = [w, s, e, n]\n",
    "    years = [2021, 2020]\n",
    "    items = []\n",
    "    last_err = None\n",
    "\n",
    "    for yr in years:\n",
    "        try:\n",
    "            search = stac.search(\n",
    "                collections=[\"esa-worldcover\"],\n",
    "                bbox=bbox,\n",
    "                datetime=f\"{yr}-01-01/{yr}-12-31\"\n",
    "            )\n",
    "            items = list(search.items())\n",
    "            if items:\n",
    "                log.info(\"worldcover_found\", year=yr, count=len(items))\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            last_err = ex\n",
    "            continue\n",
    "\n",
    "    if not items:\n",
    "        raise RuntimeError(f\"No WorldCover found for bbox in years={years}. Last error: {last_err}\")\n",
    "\n",
    "    for it in items:\n",
    "        asset = it.assets.get(\"map\") or it.assets.get(\"Map\")\n",
    "        if not asset:\n",
    "            continue\n",
    "        signed_href = pc.sign(asset.href)\n",
    "        with rasterio.open(signed_href) as src:\n",
    "            window = from_bounds(w, s, e, n, src.transform)\n",
    "            data = src.read(1, window=window)\n",
    "            transform = src.window_transform(window)\n",
    "\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": data.shape[0],\n",
    "                \"width\": data.shape[1],\n",
    "                \"transform\": transform,\n",
    "                \"count\": 1,\n",
    "                \"dtype\": data.dtype,\n",
    "                \"compress\": \"lzw\"\n",
    "            })\n",
    "\n",
    "            out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                dst.write(data, 1)\n",
    "            log.info(\"landcover_clipped\", path=str(out_path), shape=data.shape)\n",
    "            return\n",
    "\n",
    "    raise RuntimeError(\"WorldCover items found but no usable 'map' assets.\")\n",
    "\n",
    "# ----------------------- ensure assets exist -----------------------\n",
    "\n",
    "try:\n",
    "    if not DEM_TIF.exists() or DEM_TIF.stat().st_size < 100_000:\n",
    "        _download_dem_opentopo(W, S, E, N, DEM_TIF)\n",
    "    else:\n",
    "        log.info(\"dem_present\", path=str(DEM_TIF), size=DEM_TIF.stat().st_size)\n",
    "except Exception as e:\n",
    "    log.warning(\"dem_unavailable\", err=str(e))\n",
    "\n",
    "try:\n",
    "    if not LC_TIF.exists() or LC_TIF.stat().st_size < 100_000:\n",
    "        _build_worldcover_from_stac(W, S, E, N, LC_TIF)\n",
    "    else:\n",
    "        log.info(\"landcover_present\", path=str(LC_TIF), size=LC_TIF.stat().st_size)\n",
    "except Exception as e:\n",
    "    log.warning(\"landcover_unavailable\", err=str(e))\n",
    "\n",
    "# ----------------------- load fires & sampling -----------------------\n",
    "\n",
    "try:\n",
    "    df_fires = pd.read_csv(fires_clean_path)\n",
    "    gdf_fires = gpd.GeoDataFrame(df_fires, geometry=gpd.points_from_xy(df_fires.lon, df_fires.lat), crs=\"EPSG:4326\")\n",
    "except Exception as e:\n",
    "    log.error(\"fires_load_failed\", err=str(e))\n",
    "    gdf_fires = gpd.GeoDataFrame(columns=[\"lat\",\"lon\",\"geometry\"], crs=\"EPSG:4326\")\n",
    "\n",
    "def _meters_per_degree(lat_deg: float) -> tuple[float, float]:\n",
    "    lat_rad = math.radians(lat_deg)\n",
    "    mx = 111_320.0 * math.cos(lat_rad)\n",
    "    my = 110_540.0\n",
    "    return mx, my\n",
    "\n",
    "def _sample_dem_and_slope(dem_path: Path, gdf: gpd.GeoDataFrame) -> tuple[list[float], list[float]]:\n",
    "    elevs, slopes = [], []\n",
    "    try:\n",
    "        with rasterio.open(dem_path) as src:\n",
    "            tr: Affine = src.transform\n",
    "            band1 = src.read(1, masked=True)\n",
    "            deg_based = abs(tr.a) < 1e-2 and abs(tr.e) < 1e-2\n",
    "            for pt in gdf.geometry:\n",
    "                if pt is None or pt.is_empty:\n",
    "                    elevs.append(np.nan); slopes.append(np.nan); continue\n",
    "                col, row = src.index(pt.x, pt.y)\n",
    "                if 0 <= row < src.height and 0 <= col < src.width:\n",
    "                    z = band1[row, col]\n",
    "                    z = float(z) if z is not np.ma.masked else np.nan\n",
    "                else:\n",
    "                    z = np.nan\n",
    "                r0, r1 = max(row-1, 0), min(row+2, src.height)\n",
    "                c0, c1 = max(col-1, 0), min(col+2, src.width)\n",
    "                patch = band1[r0:r1, c0:c1]\n",
    "                slope_deg = np.nan\n",
    "                if patch.shape == (3,3) and np.all(~patch.mask):\n",
    "                    if deg_based:\n",
    "                        mx, my = _meters_per_degree(pt.y)\n",
    "                        px = abs(tr.a) * mx; py = abs(tr.e) * my\n",
    "                    else:\n",
    "                        px = abs(tr.a); py = abs(tr.e) if abs(tr.e) > 0 else abs(tr.d)\n",
    "                    p = patch.filled(np.nan).astype(float)\n",
    "                    dzdx = ((p[0,2] + 2*p[1,2] + p[2,2]) - (p[0,0] + 2*p[1,0] + p[2,0])) / (8.0 * px)\n",
    "                    dzdy = ((p[2,0] + 2*p[2,1] + p[2,2]) - (p[0,0] + 2*p[0,1] + p[0,2])) / (8.0 * py)\n",
    "                    slope_rad = math.atan(math.hypot(dzdx, dzdy))\n",
    "                    slope_deg = math.degrees(slope_rad)\n",
    "                elevs.append(z); slopes.append(slope_deg)\n",
    "        log.info(\"dem_sampled\", count=len(elevs))\n",
    "    except Exception as e:\n",
    "        log.warning(\"dem_sample_failed\", err=str(e))\n",
    "        elevs = [np.nan] * len(gdf); slopes = [np.nan] * len(gdf)\n",
    "    return elevs, slopes\n",
    "\n",
    "def _sample_categorical(raster_path: Path, gdf: gpd.GeoDataFrame) -> list[float]:\n",
    "    vals: list[float] = []\n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            for pt in gdf.geometry:\n",
    "                if pt is None or pt.is_empty:\n",
    "                    vals.append(np.nan); continue\n",
    "                val = list(src.sample([(pt.x, pt.y)]))[0][0]\n",
    "                if src.nodata is not None and val == src.nodata:\n",
    "                    vals.append(np.nan)\n",
    "                else:\n",
    "                    try: vals.append(int(val))\n",
    "                    except Exception: vals.append(float(val))\n",
    "        log.info(\"landcover_sampled\", count=len(vals))\n",
    "    except Exception as e:\n",
    "        log.warning(\"landcover_sample_failed\", err=str(e))\n",
    "        vals = [np.nan] * len(gdf)\n",
    "    return vals\n",
    "\n",
    "# --- enrich fires with elevation, slope, land cover ---\n",
    "try:\n",
    "    elev, slope = _sample_dem_and_slope(DEM_TIF, gdf_fires)\n",
    "    gdf_fires[\"elevation_m\"]  = elev\n",
    "    gdf_fires[\"slope_deg\"]    = slope\n",
    "    gdf_fires[\"land_cover_code\"] = _sample_categorical(LC_TIF, gdf_fires)\n",
    "\n",
    "    gdf_fires.drop(columns=[\"geometry\"]).to_parquet(fires_terrain_out, index=False)\n",
    "    log.info(\"fires_terrain_saved\", path=str(fires_terrain_out), rows=len(gdf_fires))\n",
    "    print(f\"Terrain-enriched fires saved: {fires_terrain_out}\")\n",
    "except Exception as e:\n",
    "    log.error(\"fires_terrain_failed\", err=str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47085156-ab4a-4d65-bf38-a2f95f100796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c05ab-ea4a-4690-bfaa-eac85e5c0a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c3a09-803e-4f99-8028-f7e56f98d4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761be4a0-0686-4a7e-9b64-ce02d98d7355",
   "metadata": {},
   "source": [
    "## Cell 6 — Test OpenAI Connection (Optional, Paid)\n",
    "\n",
    "This cell performs a **sanity check** of the OpenAI client connection.  \n",
    "- It sends a lightweight test prompt to the `gpt-4o-mini` model.  \n",
    "- The model is asked to produce a short haiku on *AI and wildfire resilience*.  \n",
    "- This verifies that the authentication and request pipeline are working.  \n",
    "\n",
    "**Note**: Executing this cell will consume API credits.  \n",
    "It should be run only once to validate connectivity and left commented or skipped in regular workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b5572-f73e-42c9-b524-18375826154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional test (may consume credits). \n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    input=\"Write a 3-line haiku about AI and wildfire resilience.\"\n",
    ")\n",
    "print(\"Model response:\\n\")\n",
    "print(resp.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732940f5-b9fd-4e54-be57-8cc02a7653ec",
   "metadata": {},
   "source": [
    "## Cell 7 — Verify FIRMS Token (NASA FIRMS)\n",
    "\n",
    "This cell verifies that the **NASA FIRMS Bearer token** is available in the environment.  \n",
    "- The token is required for accessing FIRMS API endpoints (satellite fire detection data).  \n",
    "- It is loaded from the shell configuration file (`~/.zshrc`) into the environment.  \n",
    "- For security, only the first characters of the token are displayed.  \n",
    "\n",
    "If the token is missing, the user must check the shell configuration and reload the session with:\n",
    "\n",
    "```bash\n",
    "source ~/.zshrc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ef9f8-75c9-4688-90e3-bc7ec2b2c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-30 20:13:49\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdem_present                   \u001b[0m \u001b[36mpath\u001b[0m=\u001b[35m/Users/evareysanchez/WildfiresAI/data/terrain/copernicus_dem.tif\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36msize\u001b[0m=\u001b[35m136749593\u001b[0m\n",
      "\u001b[2m2025-09-30 20:13:50\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mworldcover_found              \u001b[0m \u001b[36mcount\u001b[0m=\u001b[35m23\u001b[0m \u001b[36mregion\u001b[0m=\u001b[35mES\u001b[0m \u001b[36myear\u001b[0m=\u001b[35m2021\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import Affine\n",
    "import structlog\n",
    "\n",
    "log = structlog.get_logger(\"geoterrain\").bind(region=os.getenv(\"WF_REGION\", \"ES\"))\n",
    "\n",
    "# --- config / paths\n",
    "REGION = os.getenv(\"WF_REGION\", \"ES\")\n",
    "W, S, E, N = globals().get(\"ACTIVE_BBOX\", globals().get(\"SPAIN_BBOX\", (-9.5, 35.0, 3.5, 43.9)))\n",
    "\n",
    "TERRAIN_DIR = PROJECT_ROOT / \"data\" / \"terrain\"\n",
    "TERRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEM_TIF = TERRAIN_DIR / \"copernicus_dem.tif\"\n",
    "LC_TIF  = TERRAIN_DIR / \"esa_worldcover_clip.tif\"  # clipped to bbox\n",
    "\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "fires_clean_path   = PROCESSED_DIR / f\"firms_clean_{REGION}_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "fires_terrain_out  = PROCESSED_DIR / f\"fires_terrain_{REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "\n",
    "# ----------------------- helpers: DEM download -----------------------\n",
    "\n",
    "def _download_dem_opentopo(w: float, s: float, e: float, n: float, out_path: Path, demtype=\"SRTMGL3\") -> None:\n",
    "    \"\"\"Fetch DEM GeoTIFF clipped to bbox using OpenTopography Global DEM API (requires OPENTOPO_API_KEY).\"\"\"\n",
    "    import requests\n",
    "    api_key = os.getenv(\"OPENTOPO_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Missing OPENTOPO_API_KEY in .env\")\n",
    "    url = \"https://portal.opentopography.org/API/globaldem\"\n",
    "    params = {\n",
    "        \"demtype\": demtype, \"south\": s, \"north\": n, \"west\": w, \"east\": e,\n",
    "        \"outputFormat\": \"GTiff\", \"API_Key\": api_key\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=600); r.raise_for_status()\n",
    "    try:\n",
    "        js = r.json()\n",
    "        tif_url = None\n",
    "        if isinstance(js, dict) and \"result\" in js and \"links\" in js[\"result\"]:\n",
    "            for lnk in js[\"result\"][\"links\"]:\n",
    "                href = lnk.get(\"href\", \"\")\n",
    "                if href.endswith(\".tif\") or lnk.get(\"rel\") in (\"data\", \"download\", \"self\"):\n",
    "                    tif_url = href; break\n",
    "        if not tif_url:\n",
    "            raise RuntimeError(f\"No GeoTIFF link in OpenTopography response: {js}\")\n",
    "        r2 = requests.get(tif_url, timeout=1200); r2.raise_for_status()\n",
    "        out_path.write_bytes(r2.content)\n",
    "        log.info(\"dem_downloaded\", src=tif_url, path=str(out_path))\n",
    "    except Exception:\n",
    "        # Direct binary fallback (some endpoints return TIFF directly)\n",
    "        if r.headers.get(\"content-type\",\"\").lower().startswith(\"image/tiff\"):\n",
    "            out_path.write_bytes(r.content)\n",
    "            log.info(\"dem_downloaded_direct\", bytes=len(r.content), path=str(out_path))\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# ----------------------- helpers: WorldCover build -----------------------\n",
    "\n",
    "def _build_worldcover_from_stac(w: float, s: float, e: float, n: float, out_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Query ESA WorldCover via Planetary Computer STAC, mosaic intersecting tiles and clip to bbox.\n",
    "    Writes a single GeoTIFF to `out_path`. Requires `pystac-client` and `planetary_computer`.\n",
    "    \"\"\"\n",
    "    from pystac_client import Client\n",
    "    import planetary_computer as pc\n",
    "    import rasterio\n",
    "    from rasterio.merge import merge as rio_merge\n",
    "    import numpy as np\n",
    "\n",
    "    stac = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    bbox = [w, s, e, n]\n",
    "\n",
    "    # Years available in WorldCover (as of v200: 2020, 2021)\n",
    "    years = [2021, 2020]\n",
    "    items = []\n",
    "    last_err = None\n",
    "\n",
    "    for yr in years:\n",
    "        try:\n",
    "            search = stac.search(\n",
    "                collections=[\"esa-worldcover\"],\n",
    "                bbox=bbox,\n",
    "                datetime=f\"{yr}-01-01/{yr}-12-31\"\n",
    "            )\n",
    "            items = list(search.items())\n",
    "            if items:\n",
    "                log.info(\"worldcover_found\", year=yr, count=len(items))\n",
    "                break\n",
    "        except Exception as ex:\n",
    "            last_err = ex\n",
    "            continue\n",
    "\n",
    "    if not items:\n",
    "        raise RuntimeError(f\"No WorldCover found for bbox in years={years}. Last error: {last_err}\")\n",
    "\n",
    "    datasets = []\n",
    "    try:\n",
    "        for it in items:\n",
    "            asset = it.assets.get(\"map\") or it.assets.get(\"Map\")\n",
    "            if not asset:\n",
    "                continue\n",
    "            signed_href = pc.sign(asset.href)\n",
    "            datasets.append(rasterio.open(signed_href))\n",
    "        if not datasets:\n",
    "            raise RuntimeError(\"WorldCover items found but no usable 'map' assets.\")\n",
    "\n",
    "        mosaic, transform = rio_merge(datasets)\n",
    "        profile = datasets[0].profile.copy()\n",
    "        profile.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": mosaic.shape[1],\n",
    "            \"width\": mosaic.shape[2],\n",
    "            \"transform\": transform,\n",
    "            \"count\": 1,\n",
    "            \"dtype\": mosaic.dtype,\n",
    "            \"compress\": \"lzw\"\n",
    "        })\n",
    "\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "            dst.write(mosaic[0], 1)  # categorical band\n",
    "        log.info(\"landcover_built\", tiles=len(datasets), path=str(out_path))\n",
    "    finally:\n",
    "        for ds in datasets:\n",
    "            try:\n",
    "                ds.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ----------------------- ensure assets -----------------------\n",
    "\n",
    "try:\n",
    "    if not DEM_TIF.exists() or DEM_TIF.stat().st_size < 100_000:\n",
    "        _download_dem_opentopo(W, S, E, N, DEM_TIF)\n",
    "    else:\n",
    "        log.info(\"dem_present\", path=str(DEM_TIF), size=DEM_TIF.stat().st_size)\n",
    "except Exception as e:\n",
    "    log.error(\"dem_unavailable\", err=str(e))\n",
    "\n",
    "try:\n",
    "    if not LC_TIF.exists() or LC_TIF.stat().st_size < 100_000:\n",
    "        _build_worldcover_from_stac(W, S, E, N, LC_TIF)\n",
    "    else:\n",
    "        log.info(\"landcover_present\", path=str(LC_TIF), size=LC_TIF.stat().st_size)\n",
    "except Exception as e:\n",
    "    log.error(\"landcover_unavailable\", err=str(e))\n",
    "\n",
    "# ----------------------- load fires & sampling -----------------------\n",
    "\n",
    "try:\n",
    "    df_fires = pd.read_csv(fires_clean_path)\n",
    "    gdf_fires = gpd.GeoDataFrame(df_fires,\n",
    "                                 geometry=gpd.points_from_xy(df_fires.lon, df_fires.lat),\n",
    "                                 crs=\"EPSG:4326\")\n",
    "except Exception as e:\n",
    "    log.error(\"fires_load_failed\", err=str(e))\n",
    "    gdf_fires = gpd.GeoDataFrame(columns=[\"lat\",\"lon\",\"geometry\"], crs=\"EPSG:4326\")\n",
    "\n",
    "def _meters_per_degree(lat_deg: float) -> tuple[float,float]:\n",
    "    lat_rad = math.radians(lat_deg)\n",
    "    mx = 111_320.0 * math.cos(lat_rad)\n",
    "    my = 110_540.0\n",
    "    return mx, my\n",
    "\n",
    "def _sample_dem_and_slope(dem_path: Path, gdf: gpd.GeoDataFrame) -> tuple[list[float], list[float]]:\n",
    "    \"\"\"Sample elevation and slope (Horn 3×3) for each fire point.\"\"\"\n",
    "    elevs, slopes = [], []\n",
    "    try:\n",
    "        with rasterio.open(dem_path) as src:\n",
    "            tr: Affine = src.transform\n",
    "            deg_based = abs(tr.a) < 1e-2 and abs(tr.e) < 1e-2\n",
    "            band1 = src.read(1, masked=True)\n",
    "            for pt in gdf.geometry:\n",
    "                if pt is None or pt.is_empty:\n",
    "                    elevs.append(np.nan); slopes.append(np.nan); continue\n",
    "                col, row = src.index(pt.x, pt.y)\n",
    "                if 0 <= row < src.height and 0 <= col < src.width:\n",
    "                    z = band1[row, col]\n",
    "                    z = float(z) if z is not np.ma.masked else np.nan\n",
    "                else:\n",
    "                    z = np.nan\n",
    "                r0, r1 = max(row-1,0), min(row+2,src.height)\n",
    "                c0, c1 = max(col-1,0), min(col+2,src.width)\n",
    "                patch = band1[r0:r1, c0:c1]\n",
    "                slope_deg = np.nan\n",
    "                if patch.shape==(3,3) and np.all(~patch.mask):\n",
    "                    if deg_based:\n",
    "                        mx, my = _meters_per_degree(pt.y)\n",
    "                        px, py = abs(tr.a)*mx, abs(tr.e)*my\n",
    "                    else:\n",
    "                        px, py = abs(tr.a), (abs(tr.e) if abs(tr.e)>0 else abs(tr.d))\n",
    "                    p = patch.filled(np.nan).astype(float)\n",
    "                    dzdx = ((p[0,2]+2*p[1,2]+p[2,2])-(p[0,0]+2*p[1,0]+p[2,0]))/(8.0*px)\n",
    "                    dzdy = ((p[2,0]+2*p[2,1]+p[2,2])-(p[0,0]+2*p[0,1]+p[0,2]))/(8.0*py)\n",
    "                    slope_deg = math.degrees(math.atan(math.hypot(dzdx,dzdy)))\n",
    "                elevs.append(z); slopes.append(slope_deg)\n",
    "        log.info(\"dem_sampled\", count=len(elevs))\n",
    "    except Exception as e:\n",
    "        log.warning(\"dem_sample_failed\", err=str(e))\n",
    "        elevs=[np.nan]*len(gdf); slopes=[np.nan]*len(gdf)\n",
    "    return elevs, slopes\n",
    "\n",
    "def _sample_categorical(raster_path: Path, gdf: gpd.GeoDataFrame) -> list[float]:\n",
    "    \"\"\"Nearest-neighbor sampling for categorical rasters (e.g., WorldCover).\"\"\"\n",
    "    vals=[]\n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            nodata=src.nodata\n",
    "            for pt in gdf.geometry:\n",
    "                if pt is None or pt.is_empty:\n",
    "                    vals.append(np.nan); continue\n",
    "                val = list(src.sample([(pt.x,pt.y)]))[0][0]\n",
    "                if nodata is not None and val==nodata:\n",
    "                    vals.append(np.nan)\n",
    "                else:\n",
    "                    try: vals.append(int(val))\n",
    "                    except: vals.append(float(val))\n",
    "        log.info(\"landcover_sampled\", count=len(vals))\n",
    "    except Exception as e:\n",
    "        log.warning(\"landcover_sample_failed\", err=str(e))\n",
    "        vals=[np.nan]*len(gdf)\n",
    "    return vals\n",
    "\n",
    "# --- enrich and save\n",
    "try:\n",
    "    elev, slope = _sample_dem_and_slope(DEM_TIF, gdf_fires)\n",
    "    gdf_fires[\"elevation_m\"] = elev\n",
    "    gdf_fires[\"slope_deg\"] = slope\n",
    "    gdf_fires[\"land_cover_code\"] = _sample_categorical(LC_TIF, gdf_fires)\n",
    "    gdf_fires.drop(columns=[\"geometry\"]).to_parquet(fires_terrain_out, index=False)\n",
    "    log.info(\"fires_terrain_saved\", path=str(fires_terrain_out), rows=len(gdf_fires))\n",
    "    print(f\"Terrain-enriched fires → {fires_terrain_out}\")\n",
    "except Exception as e:\n",
    "    log.error(\"fires_terrain_failed\", err=str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4503bf1-1d25-48e3-a6fa-010c23974b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19777a7-ec85-4869-b66e-6134b4d3081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6924300-4e34-49c1-9f06-4874bdd65d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e8520-5928-4f67-822a-5fe4d8ee5b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "689bedc4-26bd-421c-9b1e-ed1bb762dd2b",
   "metadata": {},
   "source": [
    "## Cell 8 — Verify FIRMS MAP_KEY (NASA FIRMS)\n",
    "\n",
    "This cell verifies that the **NASA FIRMS MAP_KEY** (legacy API key) is available in the environment.  \n",
    "- The MAP_KEY is required for the **CSV-style FIRMS API**, where the key is embedded directly in the request URL.  \n",
    "- It is stored in the shell configuration (`~/.zshrc`) and loaded into the environment for use in this notebook.  \n",
    "- For security, only the first characters of the key are displayed.  \n",
    "\n",
    "If the MAP_KEY is not found, the pipeline raises an explicit error instructing the user to update and reload their environment:\n",
    "\n",
    "```bash\n",
    "export FIRMS_MAP_KEY=\"your_key_here\"\n",
    "source ~/.zshrc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ffaa6-0f4e-4800-8371-252de9bddf49",
   "metadata": {},
   "source": [
    "## Cell 9 — Connect to FIRMS API (MAP_KEY mode)\n",
    "\n",
    "This cell establishes a connection to the **NASA FIRMS API** using the legacy **MAP_KEY** authentication scheme.  \n",
    "Key operations performed here:\n",
    "\n",
    "1. **Authentication**  \n",
    "   - The `FIRMS_MAP_KEY` is embedded directly in the API request URL.  \n",
    "   - Ensures access to FIRMS CSV endpoints even if Bearer token support is unavailable.\n",
    "\n",
    "2. **Spatial & Temporal Filtering**  \n",
    "   - The request is bounded by the Spain region (defined in `SPAIN_BBOX`).  \n",
    "   - The temporal window is restricted to the **last 7 days**, consistent with FIRMS near-real-time (NRT) availability.  \n",
    "\n",
    "3. **Data Retrieval**  \n",
    "   - Fire detections are retrieved from the **VIIRS SNPP NRT** product.  \n",
    "   - The response is parsed into a Pandas DataFrame and validated.  \n",
    "\n",
    "4. **Persistence & Preview**  \n",
    "   - The raw CSV is saved to the `data/raw/` directory with a timestamped filename.  \n",
    "   - A quick preview (`head(5)`) is displayed to confirm structure and content.\n",
    "\n",
    "This step ensures the pipeline begins with authoritative, high-resolution fire detection data, suitable for downstream cleaning, integration with weather datasets, and scientific validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da22f5-3fc6-4814-8167-558eb83fbae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO  # only needed here\n",
    "\n",
    "# Ensure MAP_KEY is available (from Cell 7)\n",
    "if not FIRMS_MAP_KEY:\n",
    "    raise RuntimeError(\"FIRMS_MAP_KEY not found. Did you export it in ~/.zshrc?\")\n",
    "\n",
    "# Spain bounding box (W, S, E, N)\n",
    "w, s, e, n = SPAIN_BBOX\n",
    "\n",
    "# FIRMS product and temporal window\n",
    "product = \"VIIRS_SNPP_NRT\"\n",
    "days = 7  # valid range: 1..10\n",
    "\n",
    "out_csv = RAW_DIR / \"firms_last7d_es_raw.csv\"\n",
    "\n",
    "# MAP_KEY style (token in path)\n",
    "url = f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{FIRMS_MAP_KEY}/{product}/world/{days}\"\n",
    "params = {\"west\": w, \"south\": s, \"east\": e, \"north\": n}\n",
    "\n",
    "r = requests.get(url, params=params, timeout=60)\n",
    "if r.status_code != 200 or \"Invalid\" in r.text[:200]:\n",
    "    raise RuntimeError(f\"FIRMS API request failed: HTTP {r.status_code} · {r.text[:200]}\")\n",
    "\n",
    "# Parse and save\n",
    "df_raw = pd.read_csv(StringIO(r.text), dtype={\"acq_time\": \"string\"})\n",
    "df_raw.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"FIRMS API connection successful (MAP_KEY mode)\")\n",
    "print(f\"Rows fetched: {len(df_raw)}\")\n",
    "print(f\"Saved raw CSV → {out_csv}\")\n",
    "\n",
    "display(df_raw.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb770545-128f-4092-955b-f53f8dc094ab",
   "metadata": {},
   "source": [
    "## Cell 10 — Open-Meteo Helper (Hourly, ERA5 Archive)\n",
    "\n",
    "This cell defines a reusable **helper function** to query the [Open-Meteo Archive API](https://open-meteo.com/en/docs/historical-weather-api) for historical weather data based on the **ERA5 reanalysis dataset**. No API key is required.  \n",
    "\n",
    "**Main features:**\n",
    "\n",
    "1. **Endpoint & Variables**  \n",
    "   - Uses the ERA5 archive endpoint (`https://archive-api.open-meteo.com/v1/archive`).  \n",
    "   - Retrieves hourly variables relevant to wildfire dynamics (temperature, humidity, wind, precipitation, and surface pressure).  \n",
    "\n",
    "2. **Function `fetch_open_meteo`**  \n",
    "   - Inputs: latitude, longitude, start date, end date, and selected variables.  \n",
    "   - Returns a **Pandas DataFrame** with hourly weather records in UTC.  \n",
    "   - Columns include `time_utc`, meteorological variables, and coordinates (`lat`, `lon`).  \n",
    "\n",
    "3. **Error Handling & Validation**  \n",
    "   - Raises explicit errors if the API response is invalid (e.g., missing hourly data).  \n",
    "   - Ensures numeric casting of meteorological fields for consistent analysis.  \n",
    "\n",
    "4. **Persistence**  \n",
    "   - Provides a lightweight `save_df` utility to persist datasets under the `data/raw/` folder.  \n",
    "   - Facilitates traceability and reproducibility of weather datasets.  \n",
    "\n",
    "This component is critical for aligning **FIRMS fire detections** with **meteorological context**, enabling joint analyses (e.g., fire spread vs. weather dynamics) and subsequent model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79df87-d298-46a7-adb2-07cc4664c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "# Endpoint correcto para histórico/reanálisis (ERA5):\n",
    "OPEN_METEO_BASE = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Variables horarias por defecto (puedes añadir/quitar)\n",
    "OPEN_METEO_HOURLY = [\n",
    "    \"temperature_2m\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"dew_point_2m\",\n",
    "    \"windspeed_10m\",\n",
    "    \"windgusts_10m\",\n",
    "    \"winddirection_10m\",\n",
    "    \"precipitation\",\n",
    "    \"surface_pressure\",\n",
    "]\n",
    "\n",
    "# Pequeño helper local por si save_df aún no existe\n",
    "if \"save_df\" not in globals():\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Saved: {path}\")\n",
    "\n",
    "def fetch_open_meteo(\n",
    "    lat: float,\n",
    "    lon: float,\n",
    "    date_from: date,\n",
    "    date_to: date,\n",
    "    hourly: Iterable[str] = OPEN_METEO_HOURLY,\n",
    "    model: str | None = \"era5\",  # \"era5\" o \"era5_land\"; None para auto\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch hourly weather (UTC) for a point and date window using ERA5 archive.\"\"\"\n",
    "    params = {\n",
    "        \"latitude\":  round(float(lat), 5),\n",
    "        \"longitude\": round(float(lon), 5),\n",
    "        \"start_date\": str(date_from),\n",
    "        \"end_date\":   str(date_to),\n",
    "        \"hourly\":     \",\".join(hourly),\n",
    "        \"timezone\":   \"UTC\",\n",
    "    }\n",
    "    if model:\n",
    "        params[\"models\"] = model\n",
    "\n",
    "    r = requests.get(OPEN_METEO_BASE, params=params, timeout=60)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"Open-Meteo error {r.status_code}: {r.text[:200]}\")\n",
    "\n",
    "    js = r.json()\n",
    "    if \"hourly\" not in js or \"time\" not in js[\"hourly\"]:\n",
    "        raise RuntimeError(\"Open-Meteo response missing hourly data.\")\n",
    "\n",
    "    df = pd.DataFrame(js[\"hourly\"]).rename(columns={\"time\": \"time_utc\"})\n",
    "    df[\"time_utc\"] = pd.to_datetime(df[\"time_utc\"], utc=True, errors=\"coerce\")\n",
    "    df[\"lat\"] = round(float(lat), 5)\n",
    "    df[\"lon\"] = round(float(lon), 5)\n",
    "\n",
    "    # Asegurar tipos numéricos\n",
    "    for c in df.columns:\n",
    "        if c not in (\"time_utc\", \"lat\", \"lon\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e24ed-b300-4076-a915-8eb64e09ded9",
   "metadata": {},
   "source": [
    "## Cell 11 — Open-Meteo Smoke Test (Spain Centroid)\n",
    "\n",
    "This cell performs a **sanity check** of the Open-Meteo integration by querying historical weather data at the **centroid of Spain’s bounding box**. The goal is to validate that the pipeline can fetch, persist, and visualize meteorological information.  \n",
    "\n",
    "**Main steps:**\n",
    "\n",
    "1. **Centroid Calculation**  \n",
    "   - Latitude and longitude are derived from the midpoint of Spain’s bounding box (`SPAIN_BBOX`, defined in Cell 3).  \n",
    "\n",
    "2. **Weather Fetching**  \n",
    "   - Calls `fetch_open_meteo` (Cell 10) to retrieve ERA5 reanalysis data between `DATE_FROM` and `DATE_TO`.  \n",
    "   - Produces an **hourly-resolution DataFrame** with variables such as temperature, humidity, wind, and precipitation.  \n",
    "\n",
    "3. **Persistence**  \n",
    "   - Saves raw hourly data under `data/raw/`.  \n",
    "   - Computes a **daily aggregated summary** (mean temperature, mean humidity, mean windspeed, total precipitation) and stores it under `data/processed/`.  \n",
    "\n",
    "4. **Visualization**  \n",
    "   - Generates a **time-series plot of temperature** at the centroid (°C vs UTC time).  \n",
    "   - Saves the figure under the `reports/` directory for reproducibility and reporting.  \n",
    "\n",
    "This smoke test confirms that **Open-Meteo is properly integrated**, and it sets the foundation for linking weather conditions with FIRMS fire detections in later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205a049-d9d8-4aed-8df4-1166c421a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid of Spain bbox (W, S, E, N) from Global Configuration (Cell 3)\n",
    "W, S, E, N = SPAIN_BBOX\n",
    "lat_c = (S + N) / 2.0\n",
    "lon_c = (W + E) / 2.0\n",
    "\n",
    "# Fetch hourly weather (UTC)\n",
    "df_weather_centroid = fetch_open_meteo(lat_c, lon_c, DATE_FROM, DATE_TO, model=\"era5\")\n",
    "\n",
    "# Save raw weather CSV under data/raw/\n",
    "raw_weather_path = RAW_DIR / f\"openmeteo_es_centroid_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "save_df(df_weather_centroid, raw_weather_path)\n",
    "\n",
    "# Display quick preview\n",
    "print(f\"Rows (hourly): {len(df_weather_centroid)}\")\n",
    "display(df_weather_centroid.head(10))\n",
    "\n",
    "# Also save a daily summary (mean temperature, etc.) to processed/\n",
    "daily_summary = (\n",
    "    df_weather_centroid.assign(day=df_weather_centroid[\"time_utc\"].dt.floor(\"D\"))\n",
    "    .groupby(\"day\")\n",
    "    .agg(\n",
    "        temp_mean=(\"temperature_2m\", \"mean\"),\n",
    "        rh_mean=(\"relative_humidity_2m\", \"mean\"),\n",
    "        wind_mean=(\"windspeed_10m\", \"mean\"),\n",
    "        precip_sum=(\"precipitation\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "daily_out = PROCESSED_DIR / f\"openmeteo_es_centroid_daily_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "save_df(daily_summary, daily_out)\n",
    "\n",
    "# Plot: Temperature vs time (and save figure)\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df_weather_centroid[\"time_utc\"], df_weather_centroid[\"temperature_2m\"])\n",
    "plt.title(\"Open-Meteo — Temperature at Spain centroid (UTC)\")\n",
    "plt.xlabel(\"Time (UTC)\")\n",
    "plt.ylabel(\"Temperature 2m (°C)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure to reports/\n",
    "fig_path = REPORTS_DIR / f\"openmeteo_temp_centroid_{DATE_FROM}_{DATE_TO}.png\"\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(df_weather_centroid[\"time_utc\"], df_weather_centroid[\"temperature_2m\"])\n",
    "plt.title(\"Open-Meteo — Temperature at Spain centroid (UTC)\")\n",
    "plt.xlabel(\"Time (UTC)\")\n",
    "plt.ylabel(\"Temperature 2m (°C)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=160)\n",
    "plt.close()\n",
    "print(f\"Saved figure: {fig_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce69e0-eb9d-4761-89a0-0d1ebe64f441",
   "metadata": {},
   "source": [
    "## Cell 12 — Open-Meteo at First Fire Location (Optional)\n",
    "\n",
    "This cell enriches the FIRMS detections by fetching weather conditions at the **geographic location of the first fire point** available. It is optional, but critical for validating the integration of fire and meteorological data.\n",
    "\n",
    "**Main steps:**\n",
    "\n",
    "1. **Candidate Fire Location**  \n",
    "   - Attempts to extract latitude/longitude from `df_raw` (raw FIRMS) or `df_fires` (processed FIRMS).  \n",
    "   - Accepts flexible column names (`latitude/longitude`, `lat/lon`, `x/y`).  \n",
    "\n",
    "2. **Weather Retrieval**  \n",
    "   - If a valid fire location exists, queries Open-Meteo’s ERA5 archive for hourly conditions.  \n",
    "   - Produces a DataFrame (`df_weather_point`) with temperature, humidity, windspeed, precipitation, and more.  \n",
    "\n",
    "3. **Persistence**  \n",
    "   - Saves raw hourly weather data under `data/raw/`.  \n",
    "   - Aggregates to **daily summaries** (mean temperature, mean humidity, mean windspeed, precipitation sum) and saves under `data/processed/`.  \n",
    "\n",
    "4. **Visualization**  \n",
    "   - Generates and displays a **time-series chart** of temperature at the fire location.  \n",
    "   - Saves the chart under `reports/` for later reporting and analysis.  \n",
    "\n",
    "This optional step serves as a **prototype for future joins**: linking individual fire detections with their local weather context. It provides a first look at how meteorology and active fire events intersect in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef0108-d65c-4a0e-8b64-f9b86a9f4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lat, candidate_lon = None, None\n",
    "\n",
    "if \"df_raw\" in globals() and isinstance(df_raw, pd.DataFrame) and not df_raw.empty:\n",
    "    cand = df_raw.rename(columns=str.lower)\n",
    "    for a, b in ((\"latitude\",\"longitude\"), (\"lat\",\"lon\"), (\"y\",\"x\")):\n",
    "        if a in cand.columns and b in cand.columns:\n",
    "            candidate_lat = float(cand.iloc[0][a])\n",
    "            candidate_lon = float(cand.iloc[0][b])\n",
    "            break\n",
    "\n",
    "elif \"df_fires\" in globals() and isinstance(df_fires, pd.DataFrame) and not df_fires.empty:\n",
    "    candidate_lat = float(df_fires.iloc[0][\"latitude\"])\n",
    "    candidate_lon = float(df_fires.iloc[0][\"longitude\"])\n",
    "\n",
    "if candidate_lat is not None and candidate_lon is not None:\n",
    "    df_weather_point = fetch_open_meteo(candidate_lat, candidate_lon, DATE_FROM, DATE_TO, model=\"era5\")\n",
    "\n",
    "    # Save raw weather CSV\n",
    "    raw_pt_path = RAW_DIR / f\"openmeteo_at_fire_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "    save_df(df_weather_point, raw_pt_path)\n",
    "\n",
    "    # Daily summary → processed\n",
    "    daily_summary_fire = (\n",
    "        df_weather_point.assign(day=df_weather_point[\"time_utc\"].dt.floor(\"D\"))\n",
    "        .groupby(\"day\")\n",
    "        .agg(\n",
    "            temp_mean=(\"temperature_2m\", \"mean\"),\n",
    "            rh_mean=(\"relative_humidity_2m\", \"mean\"),\n",
    "            wind_mean=(\"windspeed_10m\", \"mean\"),\n",
    "            precip_sum=(\"precipitation\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    daily_pt_out = PROCESSED_DIR / f\"openmeteo_at_fire_daily_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "    save_df(daily_summary_fire, daily_pt_out)\n",
    "\n",
    "    print(f\"Weather fetched at fire point lat={candidate_lat:.4f}, lon={candidate_lon:.4f}\")\n",
    "    preview(df_weather_point)\n",
    "\n",
    "    # Plot: Temperature vs time\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.plot(df_weather_point[\"time_utc\"], df_weather_point[\"temperature_2m\"])\n",
    "    plt.title(f\"Open-Meteo — Temperature at fire point ({candidate_lat:.2f},{candidate_lon:.2f})\")\n",
    "    plt.xlabel(\"Time (UTC)\")\n",
    "    plt.ylabel(\"Temperature 2m (°C)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save figure\n",
    "    fig_path = REPORTS_DIR / f\"openmeteo_firepoint_temp_{DATE_FROM}_{DATE_TO}.png\"\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.plot(df_weather_point[\"time_utc\"], df_weather_point[\"temperature_2m\"])\n",
    "    plt.title(f\"Open-Meteo — Temperature at fire point ({candidate_lat:.2f},{candidate_lon:.2f})\")\n",
    "    plt.xlabel(\"Time (UTC)\")\n",
    "    plt.ylabel(\"Temperature 2m (°C)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3696dc-dd54-4575-99c9-1c3ee2f89b82",
   "metadata": {},
   "source": [
    "## Cell 13 — Build Clean FIRMS DataFrame (df_fires)\n",
    "\n",
    "Purpose. Convert the raw FIRMS CSV (Cell 9) into a clean, analysis-ready table:\n",
    "\n",
    "normalize schema and types\n",
    "\n",
    "construct a UTC timestamp (acq_datetime)\n",
    "\n",
    "filter by Spain bbox and DATE_FROM → DATE_TO (UTC)\n",
    "\n",
    "drop duplicates and sort\n",
    "\n",
    "persist the result to data/processed/ and expose it as df_fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d4a8a-2645-47bb-8aa5-0b04e41e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Guards -------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "assert \"SPAIN_BBOX\" in globals(), \"Missing SPAIN_BBOX (run Global Configuration).\"\n",
    "assert \"DATE_FROM\" in globals() and \"DATE_TO\" in globals(), \"Missing DATE_FROM/DATE_TO.\"\n",
    "assert \"PROCESSED_DIR\" in globals(), \"Missing PROCESSED_DIR (run Global Configuration).\"\n",
    "\n",
    "# Prefer df_raw from Cell 9; otherwise try to read the saved raw CSV\n",
    "if \"df_raw\" not in globals() or not isinstance(df_raw, pd.DataFrame) or df_raw.empty:\n",
    "    from pathlib import Path\n",
    "    raw_fallback = (RAW_DIR / \"firms_last7d_es_raw.csv\")\n",
    "    if raw_fallback.exists():\n",
    "        df_raw = pd.read_csv(raw_fallback, dtype={\"acq_time\": \"string\"})\n",
    "        print(f\"Loaded raw fallback: {raw_fallback}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No FIRMS raw dataframe found. Run Cell 9 first.\")\n",
    "\n",
    "# --- Helpers ------------------------------------------------------------------\n",
    "def _parse_acq_datetime(acq_date, acq_time) -> pd.Series:\n",
    "    \"\"\"Combine acq_date (YYYY-MM-DD) and acq_time (HHMM) into a UTC timestamp.\"\"\"\n",
    "    t = pd.Series(acq_time, dtype=\"string\").str.zfill(4)\n",
    "    dt = pd.Series(acq_date, dtype=\"string\") + \" \" + t.str[:2] + \":\" + t.str[2:] + \":00\"\n",
    "    return pd.to_datetime(dt, utc=True, errors=\"coerce\")\n",
    "\n",
    "def _clean_firms(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # flexible renames (handles common variants)\n",
    "    rename_map = {\n",
    "        \"lat\": \"latitude\", \"long\": \"longitude\", \"lon\": \"longitude\",\n",
    "        \"brightness\": \"brightness\", \"bright_ti4\": \"bright_ti4\", \"bright_ti5\": \"bright_ti5\",\n",
    "        \"acq_date\": \"acq_date\", \"acq_time\": \"acq_time\",\n",
    "        \"confidence\": \"confidence\", \"daynight\": \"daynight\",\n",
    "        \"satellite\": \"satellite\", \"instrument\": \"instrument\",\n",
    "        \"scan\": \"scan\", \"track\": \"track\", \"version\": \"version\", \"frp\": \"frp\"\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # required columns\n",
    "    req = [\"latitude\", \"longitude\", \"acq_date\", \"acq_time\"]\n",
    "    missing = [c for c in req if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Raw FIRMS is missing required columns: {missing}\")\n",
    "\n",
    "    # timestamp\n",
    "    df[\"acq_datetime\"] = _parse_acq_datetime(df[\"acq_date\"], df[\"acq_time\"])\n",
    "\n",
    "    # numeric coercion\n",
    "    for c in [\"latitude\",\"longitude\",\"frp\",\"bright_ti4\",\"bright_ti5\",\"brightness\",\"scan\",\"track\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # confidence normalization (text → numeric if needed)\n",
    "    if \"confidence\" in df.columns:\n",
    "        if df[\"confidence\"].dtype == \"object\" or str(df[\"confidence\"].dtype).startswith(\"string\"):\n",
    "            map_text = {\"low\": 20, \"nominal\": 50, \"high\": 85}\n",
    "            df[\"confidence_text\"] = df[\"confidence\"].str.lower().map(lambda x: x if x in map_text else np.nan)\n",
    "            df[\"confidence_num\"]  = df[\"confidence\"].str.lower().map(map_text)\n",
    "        else:\n",
    "            df[\"confidence_num\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"confidence_num\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def _clip_bbox(df: pd.DataFrame, bbox) -> pd.DataFrame:\n",
    "    w, s, e, n = bbox\n",
    "    m = df[\"longitude\"].between(w, e) & df[\"latitude\"].between(s, n)\n",
    "    return df.loc[m].copy()\n",
    "\n",
    "def _clip_timerange(df: pd.DataFrame, start_utc: pd.Timestamp, end_utc_exclusive: pd.Timestamp) -> pd.DataFrame:\n",
    "    m = (df[\"acq_datetime\"] >= start_utc) & (df[\"acq_datetime\"] < end_utc_exclusive)\n",
    "    return df.loc[m].copy()\n",
    "\n",
    "# --- Clean + Filter -----------------------------------------------------------\n",
    "df_tmp = _clean_firms(df_raw)\n",
    "\n",
    "# time window as UTC [start, end)\n",
    "start_utc = pd.Timestamp(DATE_FROM).tz_localize(\"UTC\")\n",
    "end_utc_exclusive = pd.Timestamp(DATE_TO + pd.Timedelta(days=1)).tz_localize(\"UTC\")\n",
    "\n",
    "df_tmp = _clip_bbox(df_tmp, SPAIN_BBOX)\n",
    "df_tmp = _clip_timerange(df_tmp, start_utc, end_utc_exclusive)\n",
    "\n",
    "# de-duplicate & sort\n",
    "df_fires = (\n",
    "    df_tmp.drop_duplicates(subset=[\"acq_datetime\", \"latitude\", \"longitude\"])\n",
    "          .sort_values(\"acq_datetime\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- Persist & Report ---------------------------------------------------------\n",
    "out_path = PROCESSED_DIR / f\"firms_es_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_fires.to_csv(out_path, index=False)\n",
    "\n",
    "print(\" FIRMS cleaned dataframe ready → df_fires\")\n",
    "print(f\"Rows: {len(df_fires)}  |  Saved: {out_path}\")\n",
    "if len(df_fires):\n",
    "    print(f\"Time range (UTC): {df_fires['acq_datetime'].min()} → {df_fires['acq_datetime'].max()}\")\n",
    "    print(f\"BBox: {SPAIN_BBOX}\")\n",
    "\n",
    "# Preview\n",
    "display(df_fires.head(min(10, len(df_fires))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560ce3f-5014-460e-b8a5-23f67f816b16",
   "metadata": {},
   "source": [
    "## Cell 14 — Data Inventory (with last modified + CSV)\n",
    "\n",
    "This cell performs a **systematic inventory of the project’s data folders** to ensure full traceability of the pipeline. It inspects the contents of `data/raw/` and `data/processed/`, reports file sizes and modification timestamps, and consolidates the results into a structured DataFrame.\n",
    "\n",
    "**Main steps:**\n",
    "\n",
    "1. **Directory Resolution**  \n",
    "   - Supports multiple naming conventions (`RAW_DIR` / `DATA_RAW`, `PROCESSED_DIR` / `DATA_PROC`).  \n",
    "   - Ensures project directories are initialized before proceeding.  \n",
    "\n",
    "2. **Inventory Generation**  \n",
    "   - Iterates through all `.csv` files in the raw and processed folders.  \n",
    "   - Extracts **file name, size (KB), and last modified time**.  \n",
    "   - Appends results to a unified list for reporting.  \n",
    "\n",
    "3. **Persistence**  \n",
    "   - Saves the inventory itself as a CSV file under `data/processed/` → `data_inventory.csv`.  \n",
    "   - This creates a **meta-trace** of all generated artifacts, useful for reproducibility and audits.  \n",
    "\n",
    "4. **Visualization**  \n",
    "   - Prints a human-readable summary to the console.  \n",
    "   - Displays the first 20 rows of the inventory DataFrame inside the notebook.  \n",
    "\n",
    "This step ensures that the pipeline maintains a **transparent record of all inputs and outputs** generated so far, a requirement for scientific reproducibility and future integration with automated QA systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5585d-c05e-4d62-8f00-c3cdbb455337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Ensure raw FIRMS dataframe is available; if not, reload from the saved CSV (Cell 9 output)\n",
    "raw_path = RAW_DIR / \"firms_last7d_es_raw.csv\"\n",
    "if \"df_raw\" not in globals() or df_raw is None or df_raw.empty:\n",
    "    if raw_path.exists():\n",
    "        df_raw = pd.read_csv(raw_path, dtype={\"acq_time\": \"string\"})\n",
    "        print(f\"Loaded raw FIRMS from {raw_path}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"df_raw is missing and raw CSV not found. Run Cell 9 first.\")\n",
    "\n",
    "# 2) Normalize column names to lowercase\n",
    "df = df_raw.rename(columns=str.lower).copy()\n",
    "\n",
    "# 3) Basic schema validation: ensure required fields are present\n",
    "required = {\"latitude\", \"longitude\", \"acq_date\", \"acq_time\"}\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Raw FIRMS missing columns: {missing}\")\n",
    "\n",
    "# 4) Build UTC acquisition datetime from acq_date + acq_time\n",
    "t = pd.Series(df[\"acq_time\"], dtype=\"string\").str.zfill(4)\n",
    "dt = pd.Series(df[\"acq_date\"], dtype=\"string\") + \" \" + t.str[:2] + \":\" + t.str[2:] + \":00\"\n",
    "df[\"acq_datetime\"] = pd.to_datetime(dt, utc=True, errors=\"coerce\")\n",
    "\n",
    "# 5) Convert key numeric columns to proper dtypes\n",
    "for c in (\"latitude\", \"longitude\", \"frp\", \"bright_ti4\", \"bright_ti5\", \"scan\", \"track\"):\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 6) Apply spatial (Spain bounding box) and temporal filters\n",
    "W, S, E, N = SPAIN_BBOX\n",
    "m_geo = df[\"longitude\"].between(W, E) & df[\"latitude\"].between(S, N)\n",
    "m_time = (df[\"acq_datetime\"] >= pd.to_datetime(DATE_FROM).tz_localize(\"UTC\")) & \\\n",
    "         (df[\"acq_datetime\"] <= pd.to_datetime(DATE_TO).tz_localize(\"UTC\"))\n",
    "\n",
    "df_fires = df.loc[m_geo & m_time].copy()\n",
    "\n",
    "# 7) Drop duplicates and sort chronologically\n",
    "df_fires = (\n",
    "    df_fires.drop_duplicates(subset=[\"acq_datetime\", \"latitude\", \"longitude\"])\n",
    "            .sort_values(\"acq_datetime\")\n",
    "            .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\" df_fires built. Rows: {len(df_fires)}\")\n",
    "\n",
    "# 8) Save cleaned dataset to processed folder\n",
    "out_file = PROCESSED_DIR / f\"fires_firms_es_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "df_fires.to_csv(out_file, index=False)\n",
    "print(\"Saved:\", out_file)\n",
    "\n",
    "# 9) Quick preview of the cleaned dataset\n",
    "display(df_fires.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e038ec1-0102-41e2-b81c-adb906f2d09c",
   "metadata": {},
   "source": [
    "## Cell 15 — Fetch Open-Meteo for Fire Points (robust, cached)\n",
    "\n",
    "This cell enriches **FIRMS fire detections with localized weather conditions** by systematically querying the Open-Meteo ERA5 archive for each unique fire point. Compared to a naive batch approach, this implementation is designed for **resilience and reproducibility**.\n",
    "\n",
    "**Main steps:**\n",
    "\n",
    "1. **Preconditions**  \n",
    "   - Verifies that `df_fires` exists and is non-empty.  \n",
    "   - Ensures the pipeline only queries weather data when fire detections are available.  \n",
    "\n",
    "2. **Unique Fire Locations**  \n",
    "   - Extracts latitude/longitude pairs from FIRMS detections.  \n",
    "   - Rounds coordinates to 0.01° (~1.1 km) to avoid redundant requests while preserving spatial fidelity.  \n",
    "   - Drops duplicates, keeping only distinct fire points.  \n",
    "\n",
    "3. **Sampling Strategy & Limits**  \n",
    "   - Caps the number of processed points (`MAX_POINTS = 15` by default).  \n",
    "   - Prevents **API throttling** and keeps runtime manageable.  \n",
    "   - Prints the total vs. processed points for transparency.  \n",
    "\n",
    "4. **Resilient Weather Retrieval**  \n",
    "   - Implements **per-point caching** (`data/raw/openmeteo_cache/`), so repeated runs automatically reuse existing CSVs.  \n",
    "   - Uses `requests.Session()` with **retry logic (exponential backoff)** to handle transient network errors.  \n",
    "   - Applies a **delay between calls** to respect API rate limits.  \n",
    "   - Normalizes outputs (lat/lon rounding, hourly alignment) for later joins with fire data.  \n",
    "\n",
    "5. **Aggregation & Persistence**  \n",
    "   - Concatenates all weather frames into a single DataFrame.  \n",
    "   - Saves combined data under `data/raw/` with a date-stamped filename.  \n",
    "   - Provides console feedback and previews the first rows for verification.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afcbe08-4ccf-49b3-9593-8f425b46209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# --- Preconditions\n",
    "if \"df_fires\" not in globals() or df_fires is None or df_fires.empty:\n",
    "    raise RuntimeError(\"df_fires is empty. Build it first (Cells 9→12).\")\n",
    "\n",
    "# --- Unique fire points (rounded to reduce duplicates)\n",
    "pts = (\n",
    "    df_fires[[\"latitude\", \"longitude\"]]\n",
    "    .dropna()\n",
    "    .round(2)                     # 0.01° ~ 1.1 km\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "num_pts = len(pts)\n",
    "\n",
    "# --- Safety cap and rate limiting\n",
    "MAX_POINTS = 15                   # reduce a 15 para evitar timeouts\n",
    "DELAY_SEC  = 0.8                  # pausa entre peticiones (ajustable)\n",
    "RETRIES    = 3                    # reintentos por punto\n",
    "TIMEOUT    = 60                   # timeout por petición (s)\n",
    "\n",
    "if num_pts > MAX_POINTS:\n",
    "    print(f\"Found {num_pts} unique points; sampling first {MAX_POINTS} to limit API calls.\")\n",
    "    pts = pts.iloc[:MAX_POINTS].copy()\n",
    "    num_pts = MAX_POINTS\n",
    "\n",
    "# --- Cache dir per-point (resumable)\n",
    "CACHE_DIR = RAW_DIR / \"openmeteo_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def fetch_point_with_retries(lat_i: float, lon_i: float) -> pd.DataFrame:\n",
    "    \"\"\"Call fetch_open_meteo with retries/backoff and return a normalized DataFrame.\"\"\"\n",
    "    backoff = 1.5\n",
    "    err_last = None\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            df_w = fetch_open_meteo(lat_i, lon_i, DATE_FROM, DATE_TO, model=\"era5\")\n",
    "            # Normalizar columnas y claves de join\n",
    "            df_w = df_w.rename(columns={\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "            df_w[\"lat_round\"] = df_w[\"latitude\"].round(1)\n",
    "            df_w[\"lon_round\"] = df_w[\"longitude\"].round(1)\n",
    "            df_w[\"time_hour\"] = pd.to_datetime(df_w[\"time_utc\"]).dt.floor(\"h\")\n",
    "            return df_w\n",
    "        except Exception as e:\n",
    "            err_last = e\n",
    "            if attempt < RETRIES:\n",
    "                sleep(backoff)\n",
    "                backoff *= 2\n",
    "            else:\n",
    "                raise err_last\n",
    "\n",
    "weather_frames = []\n",
    "\n",
    "for i, row in pts.iterrows():\n",
    "    lat_i = float(row[\"latitude\"])\n",
    "    lon_i = float(row[\"longitude\"])\n",
    "\n",
    "    # archivo de caché por punto (usa coordenadas redondeadas para nombre estable)\n",
    "    key = f\"{lat_i:.2f}_{lon_i:.2f}_{DATE_FROM}_{DATE_TO}.csv\".replace(\" \", \"\")\n",
    "    cache_file = CACHE_DIR / key\n",
    "\n",
    "    print(f\"[{i+1}/{num_pts}] Weather lat={lat_i:.2f}, lon={lon_i:.2f} …\", end=\" \")\n",
    "    if cache_file.exists():\n",
    "        # usar caché\n",
    "        df_w = pd.read_csv(cache_file, parse_dates=[\"time_utc\", \"time_hour\"])\n",
    "        print(\"(cached)\")\n",
    "    else:\n",
    "        # llamar con reintentos y guardar caché\n",
    "        df_w = fetch_point_with_retries(lat_i, lon_i)\n",
    "        df_w.to_csv(cache_file, index=False)\n",
    "        print(\"ok\")\n",
    "        sleep(DELAY_SEC)  # rate limiting\n",
    "\n",
    "    weather_frames.append(df_w)\n",
    "\n",
    "# --- Combine & save batch\n",
    "if not weather_frames:\n",
    "    raise RuntimeError(\"No weather frames were fetched. Check earlier steps.\")\n",
    "\n",
    "weather_points = pd.concat(weather_frames, ignore_index=True)\n",
    "raw_weather_points_path = RAW_DIR / f\"openmeteo_firepoints_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "save_df(weather_points, raw_weather_points_path)\n",
    "\n",
    "print(f\" Weather fetched for {num_pts} fire point(s). Rows total: {len(weather_points)}\")\n",
    "display(weather_points.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1660c90-9f68-463b-9dce-67c81565ed64",
   "metadata": {},
   "source": [
    "## Cell 16 — Weather Join Validator (debug only)\n",
    "\n",
    "This intermediate cell provides a **sanity check** before performing the full spatiotemporal join between FIRMS and Open-Meteo (Cell 16).  \n",
    "\n",
    "**Purpose:**  \n",
    "- Ensure that both datasets (`fires` and `weather_points`) contain the necessary harmonized keys for merging.  \n",
    "- Validate that the temporal (`datetime_hour` / `time_hour`) and spatial (`lat_round`, `lon_round`) alignment columns are present.  \n",
    "- Print row counts, preview available keys, and highlight potential gaps in coverage.  \n",
    "\n",
    "**Why important:**  \n",
    "Skipping this step can result in silent join failures (empty merges or missing columns), which would propagate errors downstream.  \n",
    "By explicitly checking schemas and alignment, this validator cell reduces debugging time and improves pipeline robustness.  \n",
    "\n",
    "**Note:**  \n",
    "- This is a **diagnostic-only step**: it does not persist outputs.  \n",
    "- Safe to skip in production once the pipeline has stabilized.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4217ca-eb81-4c1c-86fc-adc015534698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Guards: ensure required dataframes exist -------------------------------\n",
    "if \"df_fires\" not in globals() and \"df_raw\" not in globals():\n",
    "    raise RuntimeError(\"No FIRMS dataframe found (df_fires or df_raw missing).\")\n",
    "if \"weather_points\" not in globals() or weather_points is None or weather_points.empty:\n",
    "    raise RuntimeError(\"weather_points is empty. Re-run Cell 15 (batched Open-Meteo).\")\n",
    "\n",
    "# Choose the fire dataframe: prefer df_fires if available\n",
    "fires_src = None\n",
    "if \"df_fires\" in globals() and isinstance(df_fires, pd.DataFrame) and not df_fires.empty:\n",
    "    fires_src = df_fires.copy()\n",
    "elif \"df_raw\" in globals() and isinstance(df_raw, pd.DataFrame) and not df_raw.empty:\n",
    "    fires_src = df_raw.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"Both df_fires and df_raw are empty — cannot continue.\")\n",
    "\n",
    "# Normalize schemas for comparison\n",
    "fires = fires_src.rename(columns=str.lower).copy()\n",
    "wx    = weather_points.rename(columns=str.lower).copy()\n",
    "\n",
    "# --- Check temporal alignment columns ---------------------------------------\n",
    "print(\"=== Temporal alignment check ===\")\n",
    "print(\"FIRMS datetime columns:\", [c for c in fires.columns if \"date\" in c or \"time\" in c])\n",
    "print(\"Weather datetime columns:\", [c for c in wx.columns if \"time\" in c])\n",
    "\n",
    "# --- Check spatial alignment columns ----------------------------------------\n",
    "print(\"\\n=== Spatial alignment check ===\")\n",
    "print(\"FIRMS lat/lon columns:\", [c for c in fires.columns if \"lat\" in c or \"lon\" in c])\n",
    "print(\"Weather lat/lon columns:\", [c for c in wx.columns if \"lat\" in c or \"lon\" in c])\n",
    "\n",
    "# --- Row counts --------------------------------------------------------------\n",
    "print(\"\\n=== Row counts ===\")\n",
    "print(\"FIRMS rows:\", len(fires))\n",
    "print(\"Weather rows:\", len(wx))\n",
    "\n",
    "# --- Quick preview of keys ---------------------------------------------------\n",
    "print(\"\\n=== Preview of join keys ===\")\n",
    "if \"acq_datetime\" in fires.columns:\n",
    "    fires[\"datetime_hour\"] = pd.to_datetime(fires[\"acq_datetime\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "elif {\"acq_date\", \"acq_time\"}.issubset(fires.columns):\n",
    "    t  = pd.Series(fires[\"acq_time\"], dtype=\"string\").str.zfill(4)\n",
    "    dt = pd.Series(fires[\"acq_date\"], dtype=\"string\") + \" \" + t.str[:2] + \":\" + t.str[2:] + \":00\"\n",
    "    fires[\"datetime_hour\"] = pd.to_datetime(dt, utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "\n",
    "fires[\"lat_round\"] = pd.to_numeric(fires.get(\"latitude\", pd.Series()), errors=\"coerce\").round(1)\n",
    "fires[\"lon_round\"] = pd.to_numeric(fires.get(\"longitude\", pd.Series()), errors=\"coerce\").round(1)\n",
    "\n",
    "if \"time_hour\" not in wx.columns and \"time_utc\" in wx.columns:\n",
    "    wx[\"time_hour\"] = pd.to_datetime(wx[\"time_utc\"]).dt.floor(\"h\")\n",
    "if \"lat_round\" not in wx.columns and \"latitude\" in wx.columns:\n",
    "    wx[\"lat_round\"] = pd.to_numeric(wx[\"latitude\"], errors=\"coerce\").round(1)\n",
    "if \"lon_round\" not in wx.columns and \"longitude\" in wx.columns:\n",
    "    wx[\"lon_round\"] = pd.to_numeric(wx[\"longitude\"], errors=\"coerce\").round(1)\n",
    "\n",
    "display(fires[[\"datetime_hour\",\"lat_round\",\"lon_round\"]].head(5))\n",
    "display(wx[[\"time_hour\",\"lat_round\",\"lon_round\"]].head(5))\n",
    "\n",
    "print(\"\\n Validation complete — proceed to Cell 17 for the actual join.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b9f94-eb02-4428-b4ba-8dbf3b64d435",
   "metadata": {},
   "source": [
    "## Cell 17 — Join Coverage Diagnostics (place after Cell 16; before Cell 17)\n",
    "Objective:\n",
    "Quantify current merge coverage between FIRMS (df_fires) and Open-Meteo (weather_points) at two spatial roundings (±0.1°, ±0.2°). This is read-only: it does NOT modify existing artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627bb20-9b93-43e8-aec6-a5f721a31e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_join_keys(\n",
    "    fires_in: pd.DataFrame,\n",
    "    wx_in: pd.DataFrame,\n",
    "    round_deg: float\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Return (fires, wx) with normalized hourly time and spatial rounding for the join keys.\"\"\"\n",
    "    # Normalize schemas\n",
    "    fires = fires_in.rename(columns=str.lower).copy()\n",
    "    wx = wx_in.rename(columns=str.lower).copy()\n",
    "\n",
    "    # --- Temporal alignment (UTC → hourly) ---\n",
    "    # FIRMS\n",
    "    if \"acq_datetime\" not in fires.columns:\n",
    "        raise RuntimeError(\"df_fires lacks 'acq_datetime'. Run the cleaning cell first.\")\n",
    "    fires[\"acq_datetime\"] = pd.to_datetime(fires[\"acq_datetime\"], utc=True, errors=\"coerce\")\n",
    "    fires[\"datetime_hour\"] = fires[\"acq_datetime\"].dt.floor(\"h\")\n",
    "\n",
    "    # Weather\n",
    "    if \"time_hour\" in wx.columns:\n",
    "        wx[\"datetime_hour\"] = pd.to_datetime(wx[\"time_hour\"], utc=True, errors=\"coerce\")\n",
    "    elif \"time_utc\" in wx.columns:\n",
    "        wx[\"datetime_hour\"] = pd.to_datetime(wx[\"time_utc\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "    else:\n",
    "        raise RuntimeError(\"weather_points lacks 'time_hour' or 'time_utc' columns.\")\n",
    "\n",
    "    # --- Spatial rounding (to specified grid) ---\n",
    "    def _round_to_base(vals, base: float) -> np.ndarray:\n",
    "        arr = np.array(pd.to_numeric(vals, errors=\"coerce\"), dtype=float)\n",
    "        return (np.round(arr / base) * base).astype(float)\n",
    "\n",
    "    for col_src, col_dst in ((\"latitude\", \"lat_round\"), (\"longitude\", \"lon_round\")):\n",
    "        if col_src not in fires.columns:\n",
    "            raise RuntimeError(f\"df_fires lacks '{col_src}'.\")\n",
    "        fires[col_dst] = _round_to_base(fires[col_src], round_deg)\n",
    "\n",
    "        if col_src not in wx.columns:\n",
    "            raise RuntimeError(f\"weather_points lacks '{col_src}'.\")\n",
    "        wx[col_dst] = _round_to_base(wx[col_src], round_deg)\n",
    "\n",
    "    keys = [\"datetime_hour\", \"lat_round\", \"lon_round\"]\n",
    "    return fires, wx, keys\n",
    "\n",
    "def join_coverage(round_deg: float = 0.1) -> Dict[str, float]:\n",
    "    \"\"\"Compute coverage metrics for a given rounding tolerance.\"\"\"\n",
    "    assert \"df_fires\" in globals() and isinstance(df_fires, pd.DataFrame) and len(df_fires), \\\n",
    "        \"df_fires is missing or empty.\"\n",
    "    assert \"weather_points\" in globals() and isinstance(weather_points, pd.DataFrame) and len(weather_points), \\\n",
    "        \"weather_points is missing or empty.\"\n",
    "\n",
    "    fires, wx, keys = _ensure_join_keys(df_fires, weather_points, round_deg)\n",
    "\n",
    "    # Unique key sets\n",
    "    k_fires = fires[keys].dropna().drop_duplicates()\n",
    "    k_wx = wx[keys].dropna().drop_duplicates()\n",
    "\n",
    "    # Overlap of join keys\n",
    "    k_inner = k_fires.merge(k_wx, on=keys, how=\"inner\")\n",
    "    coverage_keys = len(k_inner) / max(len(k_fires), 1)\n",
    "\n",
    "    # Quick left-merge sample to probe temperature availability after join\n",
    "    keep_cols_wx = [c for c in wx.columns if c in (\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"windspeed_10m\", \"precipitation\"\n",
    "    )]\n",
    "    probe = fires.merge(wx[keys + keep_cols_wx], on=keys, how=\"left\", suffixes=(\"\", \"_wx\"))\n",
    "\n",
    "    # Temperature columns may appear under different names later; here we only test presence\n",
    "    temp_like = [c for c in probe.columns if c.startswith(\"temperature_2m\")]\n",
    "    if temp_like:\n",
    "        temp_any = pd.concat([pd.to_numeric(probe[c], errors=\"coerce\") for c in temp_like], axis=1).notna().any(axis=1)\n",
    "        temp_rows = int(temp_any.sum())\n",
    "    else:\n",
    "        temp_rows = 0\n",
    "\n",
    "    return {\n",
    "        \"round_deg\": round_deg,\n",
    "        \"fires_rows\": int(len(fires)),\n",
    "        \"wx_rows\": int(len(wx)),\n",
    "        \"fires_key_unique\": int(len(k_fires)),\n",
    "        \"wx_key_unique\": int(len(k_wx)),\n",
    "        \"key_overlap\": int(len(k_inner)),\n",
    "        \"key_coverage_ratio\": float(round(coverage_keys, 4)),\n",
    "        \"temperature_rows_after_left_merge\": temp_rows,\n",
    "        \"temperature_ratio_vs_fires\": float(round(temp_rows / max(len(fires), 1), 4)),\n",
    "    }\n",
    "\n",
    "# Run diagnostics at ±0.1° and ±0.2°\n",
    "diag_01 = join_coverage(0.1)\n",
    "diag_02 = join_coverage(0.2)\n",
    "\n",
    "print(\"=== Join Coverage Diagnostics ===\")\n",
    "for d in (diag_01, diag_02):\n",
    "    print(f\"\\nRounding: ±{d['round_deg']}°\")\n",
    "    print(f\"- FIRMS rows: {d['fires_rows']} | unique join keys: {d['fires_key_unique']}\")\n",
    "    print(f\"- Weather rows: {d['wx_rows']} | unique join keys: {d['wx_key_unique']}\")\n",
    "    print(f\"- Key overlap: {d['key_overlap']}  (coverage={d['key_coverage_ratio']*100:.1f}%)\")\n",
    "    print(f\"- Temperature presence after left-merge: {d['temperature_rows_after_left_merge']} \"\n",
    "          f\"({d['temperature_ratio_vs_fires']*100:.1f}% of FIRMS rows)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30196b7f-654f-4762-bed6-4bb559db6a12",
   "metadata": {},
   "source": [
    "## Cell 17 — Join FIRMS and Open-Meteo Data\n",
    "\n",
    "This cell performs the first **spatiotemporal integration** between satellite fire detections (NASA FIRMS) and local weather conditions (Open-Meteo).\n",
    "\n",
    "**Operations**\n",
    "1. **Schema harmonization**\n",
    "   - Convert FIRMS timestamps to UTC and normalize to **hourly** resolution (`datetime_hour`).\n",
    "   - Apply **soft spatial rounding** (0.1°) on lat/lon in both datasets to align with the meteorological grid.\n",
    "\n",
    "2. **Join**\n",
    "   - Merge on the composite keys: **(`datetime_hour`, `lat_round`, `lon_round`)**.\n",
    "   - Produce a joined table with fire attributes and co-located hourly weather.\n",
    "\n",
    "3. **Persistence**\n",
    "   - Save the integrated dataset to `data/processed/` with a date-scoped filename.\n",
    "\n",
    "4. **Diagnostics & Visualization**\n",
    "   - Generate **Daily Fire Detections** figure from FIRMS.\n",
    "   - Compute and plot **Daily Mean Temperature** at joined fire locations (if coverage is sufficient).\n",
    "   - Store figures under `reports/` for traceability.\n",
    "\n",
    "**Notes**\n",
    "- This step assumes weather retrieval for fire points has been completed (see **Cell 14**).  \n",
    "- Join quality depends on temporal alignment (hour rounding) and spatial tolerance (0.1°). These parameters can be tuned in subsequent iterations to maximize coverage without over-matching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64e082-8f31-4c6f-b7f8-37ce8e3920ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Preconditions ------------------------------------------------------\n",
    "assert \"df_fires\" in globals() and len(df_fires), \"df_fires missing.\"\n",
    "assert \"weather_points\" in globals() and len(weather_points), \"weather_points missing.\"\n",
    "assert \"PROCESSED_DIR\" in globals() and \"REPORTS_DIR\" in globals(), \"Run Global Configuration first.\"\n",
    "assert \"DATE_FROM\" in globals() and \"DATE_TO\" in globals(), \"Missing DATE_FROM/DATE_TO.\"\n",
    "\n",
    "# --- Parameters ---------------------------------------------------------\n",
    "TIME_TOL_H = 2        # temporal tolerance in hours (±2h) to boost coverage\n",
    "MAX_RADIUS_KM = 40.0  # spatial tolerance (km)\n",
    "EARTH_R_KM  = 6371.0088\n",
    "\n",
    "# --- Normalize inputs ---------------------------------------------------\n",
    "fires = df_fires.rename(columns=str.lower).copy()\n",
    "wx    = weather_points.rename(columns=str.lower).copy()\n",
    "\n",
    "fires[\"acq_datetime\"]  = pd.to_datetime(fires[\"acq_datetime\"], utc=True, errors=\"coerce\")\n",
    "fires[\"datetime_hour\"] = fires[\"acq_datetime\"].dt.floor(\"h\")\n",
    "fires = fires.dropna(subset=[\"latitude\", \"longitude\", \"datetime_hour\"]).reset_index(drop=True)\n",
    "\n",
    "if \"datetime_hour\" in wx.columns:\n",
    "    wx[\"datetime_hour\"] = pd.to_datetime(wx[\"datetime_hour\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "elif \"time_hour\" in wx.columns:\n",
    "    wx[\"datetime_hour\"] = pd.to_datetime(wx[\"time_hour\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "elif \"time_utc\" in wx.columns:\n",
    "    wx[\"datetime_hour\"] = pd.to_datetime(wx[\"time_utc\"], utc=True, errors=\"coerce\").dt.floor(\"h\")\n",
    "else:\n",
    "    raise RuntimeError(\"weather_points lacks a datetime column (time_hour/time_utc).\")\n",
    "\n",
    "# Keep only features that exist in wx\n",
    "wx_feats_all = [\n",
    "    \"temperature_2m\",\n",
    "    \"relative_humidity_2m\",\n",
    "    \"windspeed_10m\",\n",
    "    \"precipitation\",\n",
    "    \"dew_point_2m\",\n",
    "    \"windgusts_10m\",\n",
    "    \"winddirection_10m\",\n",
    "    \"surface_pressure\",\n",
    "]\n",
    "wx_feats = [c for c in wx_feats_all if c in wx.columns]\n",
    "wx = wx[[\"datetime_hour\",\"latitude\",\"longitude\"] + wx_feats].dropna(subset=[\"latitude\",\"longitude\",\"datetime_hour\"])\n",
    "\n",
    "# --- Expand temporal window (±2h) --------------------------------------\n",
    "wx_expanded = []\n",
    "for dt_shift in range(-TIME_TOL_H, TIME_TOL_H + 1):\n",
    "    w = wx.copy()\n",
    "    w[\"match_hour\"] = w[\"datetime_hour\"] + pd.Timedelta(hours=dt_shift)\n",
    "    w[\"dt_offset_h\"] = dt_shift\n",
    "    wx_expanded.append(w)\n",
    "wx_expanded = pd.concat(wx_expanded, ignore_index=True)\n",
    "\n",
    "fires_ = fires.copy()\n",
    "fires_[\"match_hour\"] = fires_[\"datetime_hour\"]\n",
    "\n",
    "# --- Candidate join by hour, then pick nearest in space -----------------\n",
    "cand = fires_[[\"acq_datetime\",\"datetime_hour\",\"latitude\",\"longitude\",\"match_hour\"]].merge(\n",
    "    wx_expanded,\n",
    "    on=\"match_hour\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_fire\",\"_wx\")\n",
    ")\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2) -> np.ndarray:\n",
    "    lat1 = np.radians(pd.to_numeric(lat1, errors=\"coerce\").astype(float))\n",
    "    lon1 = np.radians(pd.to_numeric(lon1, errors=\"coerce\").astype(float))\n",
    "    lat2 = np.radians(pd.to_numeric(lat2, errors=\"coerce\").astype(float))\n",
    "    lon2 = np.radians(pd.to_numeric(lon2, errors=\"coerce\").astype(float))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arcsin(np.minimum(1.0, np.sqrt(a)))\n",
    "    return EARTH_R_KM * c\n",
    "\n",
    "cand[\"dist_km\"] = haversine_km(\n",
    "    cand[\"latitude\"],  cand[\"longitude\"],\n",
    "    cand[\"latitude_wx\"], cand[\"longitude_wx\"]\n",
    ")\n",
    "\n",
    "cand = cand[cand[\"dist_km\"] <= MAX_RADIUS_KM].copy()\n",
    "\n",
    "# Resolve to the single best weather sample per fire row (min distance, then min |dt|)\n",
    "cand[\"abs_dt_h\"] = cand[\"dt_offset_h\"].abs()\n",
    "# Attach a stable fire_row id\n",
    "fires_key = fires_[[\"acq_datetime\",\"datetime_hour\",\"latitude\",\"longitude\"]].reset_index().rename(columns={\"index\":\"fire_row\"})\n",
    "cand = cand.merge(\n",
    "    fires_key,\n",
    "    on=[\"acq_datetime\",\"datetime_hour\",\"latitude\",\"longitude\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "cand = cand.sort_values([\"fire_row\",\"dist_km\",\"abs_dt_h\"]).drop_duplicates(subset=[\"fire_row\"], keep=\"first\")\n",
    "\n",
    "# --- Assemble final joined frame ---------------------------------------\n",
    "keep_cols = [\n",
    "    \"fire_row\",\"acq_datetime\",\"datetime_hour\",\"latitude\",\"longitude\",\n",
    "    \"datetime_hour_wx\",\"dt_offset_h\",\"latitude_wx\",\"longitude_wx\",\"dist_km\"\n",
    "] + wx_feats\n",
    "df_join = cand.rename(columns={\"datetime_hour_x\":\"datetime_hour\"})  # just in case\n",
    "df_join = df_join[keep_cols]\n",
    "\n",
    "# --- Coverage metrics ---------------------------------------------------\n",
    "def _nz(s): return int(pd.to_numeric(s, errors=\"coerce\").notna().sum())\n",
    "cov = {c: {\"non_null\": _nz(df_join[c])} for c in wx_feats}\n",
    "for c in cov:\n",
    "    cov[c][\"ratio_vs_fires\"] = float(round(cov[c][\"non_null\"] / max(len(fires_),1), 4))\n",
    "\n",
    "print(f\"FIRMS rows: {len(fires_)} | WX rows: {len(wx)} | Candidates kept: {len(df_join)}\")\n",
    "print(f\"Radius ≤ {MAX_RADIUS_KM:.0f} km, time window ±{TIME_TOL_H}h\")\n",
    "print(\"\\n=== Coverage after NN-join ===\")\n",
    "for k,v in cov.items():\n",
    "    print(f\"- {k:>20s}: {v['non_null']:6d} rows  ({v['ratio_vs_fires']*100:5.1f}% of FIRMS rows)\")\n",
    "\n",
    "# --- Persist ------------------------------------------------------------\n",
    "out_csv = PROCESSED_DIR / f\"fires_weather_es_{DATE_FROM}_{DATE_TO}_nn_{int(MAX_RADIUS_KM)}km_pm{TIME_TOL_H}h.csv\"\n",
    "df_join.to_csv(out_csv, index=False)\n",
    "print(\"\\nSaved NN-joined CSV →\", out_csv)\n",
    "\n",
    "# --- Safe plotting helpers ---------------------------------------------\n",
    "def _save_line_safe(df_xy: pd.DataFrame, xcol: str, ycol: str, title: str, ylabel: str, fname: str):\n",
    "    # Emit plot only if there is valid data\n",
    "    if ycol not in df_xy.columns or pd.to_numeric(df_xy[ycol], errors=\"coerce\").notna().sum() == 0:\n",
    "        print(f\"Skipped plot (no data): {title}\")\n",
    "        return\n",
    "    plt.figure(figsize=(8.2, 4.4))\n",
    "    plt.plot(df_xy[xcol], df_xy[ycol])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date (UTC)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    outp = REPORTS_DIR / fname\n",
    "    plt.savefig(outp, dpi=160)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", outp)\n",
    "\n",
    "# --- Figures (only when data exists) -----------------------------------\n",
    "# 0) Daily detections (always exists)\n",
    "daily_det = (\n",
    "    fires_.assign(day=fires_[\"acq_datetime\"].dt.floor(\"D\"))\n",
    "    .groupby(\"day\")[\"acq_datetime\"].count()\n",
    "    .reset_index(name=\"detections\")\n",
    ")\n",
    "_save_line_safe(\n",
    "    daily_det, \"day\", \"detections\",\n",
    "    \"Daily Fire Detections (FIRMS)\",\n",
    "    \"Detections\",\n",
    "    f\"nn_daily_detections_{DATE_FROM}_{DATE_TO}.png\"\n",
    ")\n",
    "\n",
    "# 1) Daily meteo means on matched rows (skip if empty)\n",
    "if \"datetime_hour\" in df_join.columns:\n",
    "    base = df_join.assign(day=pd.to_datetime(df_join[\"datetime_hour\"], utc=True).dt.floor(\"D\"))\n",
    "    if \"temperature_2m\" in df_join.columns:\n",
    "        tmp = base.groupby(\"day\")[\"temperature_2m\"].mean().reset_index(name=\"temp_mean\")\n",
    "        _save_line_safe(tmp, \"day\", \"temp_mean\",\n",
    "                        \"Daily Mean Temperature (NN-Joined)\",\n",
    "                        \"Temp 2m (°C)\",\n",
    "                        f\"nn_daily_temp_{DATE_FROM}_{DATE_TO}.png\")\n",
    "    if \"relative_humidity_2m\" in df_join.columns:\n",
    "        tmp = base.groupby(\"day\")[\"relative_humidity_2m\"].mean().reset_index(name=\"rh_mean\")\n",
    "        _save_line_safe(tmp, \"day\", \"rh_mean\",\n",
    "                        \"Daily Mean Relative Humidity (NN-Joined)\",\n",
    "                        \"RH 2m (%)\",\n",
    "                        f\"nn_daily_rh_{DATE_FROM}_{DATE_TO}.png\")\n",
    "    if \"windspeed_10m\" in df_join.columns:\n",
    "        tmp = base.groupby(\"day\")[\"windspeed_10m\"].mean().reset_index(name=\"wind_mean\")\n",
    "        _save_line_safe(tmp, \"day\", \"wind_mean\",\n",
    "                        \"Daily Mean Wind Speed (NN-Joined)\",\n",
    "                        \"Wind 10m (m/s)\",\n",
    "                        f\"nn_daily_wind_{DATE_FROM}_{DATE_TO}.png\")\n",
    "    if \"precipitation\" in df_join.columns:\n",
    "        tmp = base.groupby(\"day\")[\"precipitation\"].sum().reset_index(name=\"precip_sum\")\n",
    "        _save_line_safe(tmp, \"day\", \"precip_sum\",\n",
    "                        \"Daily Precipitation Sum (NN-Joined)\",\n",
    "                        \"Precipitation (mm)\",\n",
    "                        f\"nn_daily_precip_{DATE_FROM}_{DATE_TO}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730b843-c573-48b7-ac5a-d3744d13c609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6d2d8-3228-40e4-a760-5a191240a46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0509e-c3fc-4c3c-9d37-edc3c79cad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save a DataFrame to CSV and print the path.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def preview(df: pd.DataFrame, n: int = MAX_ROWS_PREVIEW) -> None:\n",
    "    \"\"\"Quick preview of a DataFrame.\"\"\"\n",
    "    display(df.head(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5641fd-a9d8-4a3a-ae4b-38328c950907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WildfiresAI)",
   "language": "python",
   "name": "wildfiresai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
