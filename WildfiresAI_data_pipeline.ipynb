{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5502748b-524c-4d49-9eb6-ed13af2f9b43",
   "metadata": {},
   "source": [
    "# WILDFIRESAI - DATA PIPELINE (Base Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2ac49-0656-4dee-afe9-94c04ca653e8",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# ==== Cell 1 — Setup  ====\n",
    " Purpose:\n",
    "   Install and validate core dependencies required by WildfiresAI.\n",
    "   Ensure modules are importable without kernel restart.\n",
    "# ---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5791a558-c393-4c8e-b1fc-8277e2989133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Installed modules status]\n",
      " - pystac_client: 0.9.0\n",
      " - planetary_computer: 1.0.0\n",
      " - mp_api: ok\n",
      " - pymatgen: ok\n",
      " - pyarrow: 21.0.0\n",
      " - tqdm: 4.67.1\n",
      " - structlog: 25.4.0\n",
      "\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1 — Setup (portable dependency install) ====\n",
    "# Installs missing dependencies and validates imports across environments.\n",
    "\n",
    "from typing import Dict\n",
    "import importlib, importlib.util, subprocess, sys, site\n",
    "\n",
    "REQUIRED: Dict[str, str] = {\n",
    "    \"pystac-client\": \"pystac_client\",\n",
    "    \"planetary-computer\": \"planetary_computer\",\n",
    "    \"mp-api\": \"mp_api\",\n",
    "    \"pymatgen\": \"pymatgen\",\n",
    "    \"pyarrow\": \"pyarrow\",\n",
    "    \"tqdm\": \"tqdm\",\n",
    "    \"structlog\": \"structlog\",\n",
    "}\n",
    "\n",
    "def _is_importable(mod): return importlib.util.find_spec(mod) is not None\n",
    "def _install(pkg): subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg], check=True)\n",
    "\n",
    "# Append user site (portable fix for JupyterHub)\n",
    "try:\n",
    "    user_site = site.getusersitepackages()\n",
    "    if user_site not in sys.path:\n",
    "        sys.path.append(user_site)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for pkg, mod in REQUIRED.items():\n",
    "    if not _is_importable(mod): _install(pkg)\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "status = {}\n",
    "for pkg, mod in REQUIRED.items():\n",
    "    try:\n",
    "        m = importlib.import_module(mod)\n",
    "        status[mod] = getattr(m, \"__version__\", \"ok\")\n",
    "    except Exception as e:\n",
    "        status[mod] = f\"import failed ({e})\"\n",
    "\n",
    "print(\"[Installed modules status]\")\n",
    "for m, v in status.items():\n",
    "    print(f\" - {m}: {v}\")\n",
    "print(\"\\nSetup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8042b-0c8a-4da1-8a61-ea3b18c0680a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# ==== Cell 2 — Scientific / Infra Stack & Version Audit ====\n",
    " Purpose:\n",
    "   Establish project-wide environment paths and verify\n",
    "   availability & versions of the core scientific stack.\n",
    "# ---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d596ef50-5a1e-4714-9a60-fe1d509b5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] Python 3.13.5 | Platform: Darwin 24.6.0\n",
      "[ENV] Root directory: /Users/evareysanchez/WildfiresAI\n",
      "[ENV] Folders (exists / writable):\n",
      "   - data/raw:        True / True\n",
      "   - data/processed:  True / True\n",
      "   - reports/:        True / True\n",
      "   - reports/logs/:   True / True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c6861\">\n",
       "  <caption>Core Scientific Stack Versions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c6861_level0_col0\" class=\"col_heading level0 col0\" >pandas</th>\n",
       "      <th id=\"T_c6861_level0_col1\" class=\"col_heading level0 col1\" >numpy</th>\n",
       "      <th id=\"T_c6861_level0_col2\" class=\"col_heading level0 col2\" >requests</th>\n",
       "      <th id=\"T_c6861_level0_col3\" class=\"col_heading level0 col3\" >geopandas</th>\n",
       "      <th id=\"T_c6861_level0_col4\" class=\"col_heading level0 col4\" >rasterio</th>\n",
       "      <th id=\"T_c6861_level0_col5\" class=\"col_heading level0 col5\" >shapely</th>\n",
       "      <th id=\"T_c6861_level0_col6\" class=\"col_heading level0 col6\" >matplotlib</th>\n",
       "      <th id=\"T_c6861_level0_col7\" class=\"col_heading level0 col7\" >tqdm</th>\n",
       "      <th id=\"T_c6861_level0_col8\" class=\"col_heading level0 col8\" >sklearn</th>\n",
       "      <th id=\"T_c6861_level0_col9\" class=\"col_heading level0 col9\" >torch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c6861_level0_row0\" class=\"row_heading level0 row0\" >version</th>\n",
       "      <td id=\"T_c6861_row0_col0\" class=\"data row0 col0\" >2.3.2</td>\n",
       "      <td id=\"T_c6861_row0_col1\" class=\"data row0 col1\" >2.3.3</td>\n",
       "      <td id=\"T_c6861_row0_col2\" class=\"data row0 col2\" >2.32.4</td>\n",
       "      <td id=\"T_c6861_row0_col3\" class=\"data row0 col3\" >1.1.1</td>\n",
       "      <td id=\"T_c6861_row0_col4\" class=\"data row0 col4\" >1.4.3</td>\n",
       "      <td id=\"T_c6861_row0_col5\" class=\"data row0 col5\" >2.1.2</td>\n",
       "      <td id=\"T_c6861_row0_col6\" class=\"data row0 col6\" >3.10.6</td>\n",
       "      <td id=\"T_c6861_row0_col7\" class=\"data row0 col7\" >4.67.1</td>\n",
       "      <td id=\"T_c6861_row0_col8\" class=\"data row0 col8\" >1.7.2</td>\n",
       "      <td id=\"T_c6861_row0_col9\" class=\"data row0 col9\" >2.8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16547d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Diagnostics]\n",
      " - geopandas_crs_ok: True\n",
      " - pyproj_version: 3.7.2\n",
      " - proj_data_dir: /opt/miniconda3/lib/python3.13/site-packages/pyproj/proj_dir/share/proj\n",
      " - fiona_version: 1.10.1\n",
      " - fiona_gdal: 3.9.2\n",
      " - rasterio_gdal: 3.9.3\n",
      " - rasterio_proj: 9.4.1\n",
      " - torch_backend: mps\n",
      " - torch_dtype_default: torch.float32\n",
      "\n",
      "Environment and version audit completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 2 — Scientific / Infra Stack & Version Audit ====\n",
    "# Purpose:\n",
    "# - Define project paths used across AG² components.\n",
    "# - Produce a concise environment + versions audit (core scientific + geo stack).\n",
    "# - Verify write permissions on critical directories.\n",
    "#\n",
    "# Notes:\n",
    "# - Comments are concise and technical (MIT Lab style).\n",
    "# - No duplicated installations/import policies from Cell 1.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, platform, importlib, tempfile\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------- Project Paths -----------------------------\n",
    "PROJECT_ROOT  = Path.cwd()\n",
    "DATA_DIR      = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR       = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = PROJECT_ROOT / \"reports\"\n",
    "LOGS_DIR      = REPORTS_DIR / \"logs\"\n",
    "\n",
    "for d in (RAW_DIR, PROCESSED_DIR, REPORTS_DIR, LOGS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: write-permission probe (detects NFS/readonly issues early)\n",
    "def _can_write(path: Path) -> bool:\n",
    "    try:\n",
    "        with tempfile.TemporaryFile(dir=path):\n",
    "            pass\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "perm = {\n",
    "    \"data/raw\": _can_write(RAW_DIR),\n",
    "    \"data/processed\": _can_write(PROCESSED_DIR),\n",
    "    \"reports\": _can_write(REPORTS_DIR),\n",
    "    \"reports/logs\": _can_write(LOGS_DIR),\n",
    "}\n",
    "\n",
    "# ----------------------------- ENV Summary -------------------------------\n",
    "print(f\"[ENV] Python {platform.python_version()} | Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"[ENV] Root directory: {PROJECT_ROOT}\")\n",
    "print(\"[ENV] Folders (exists / writable):\")\n",
    "print(f\"   - data/raw:        {RAW_DIR.exists()} / {perm['data/raw']}\")\n",
    "print(f\"   - data/processed:  {PROCESSED_DIR.exists()} / {perm['data/processed']}\")\n",
    "print(f\"   - reports/:        {REPORTS_DIR.exists()} / {perm['reports']}\")\n",
    "print(f\"   - reports/logs/:   {LOGS_DIR.exists()} / {perm['reports/logs']}\")\n",
    "\n",
    "# ------------------------- Core Stack Versions ---------------------------\n",
    "core_packages = [\n",
    "    \"pandas\", \"numpy\", \"requests\", \"geopandas\", \"rasterio\",\n",
    "    \"shapely\", \"matplotlib\", \"tqdm\", \"sklearn\", \"torch\"\n",
    "]\n",
    "\n",
    "versions = {}\n",
    "for pkg in core_packages:\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        versions[pkg] = getattr(mod, \"__version__\", \"ok\")\n",
    "    except Exception as e:\n",
    "        versions[pkg] = f\"not installed ({e.__class__.__name__})\"\n",
    "\n",
    "df_versions = pd.DataFrame.from_dict(versions, orient=\"index\", columns=[\"version\"])\n",
    "display(df_versions.T.style.set_caption(\"Core Scientific Stack Versions\"))\n",
    "\n",
    "# ---------------------- Geo Stack & Backend Diagnostics -------------------\n",
    "diag = {}\n",
    "\n",
    "# Geo: PROJ/GDAL, Fiona, PyProj availability (common failure modes)\n",
    "def _is_importable(name: str) -> bool:\n",
    "    return importlib.util.find_spec(name) is not None\n",
    "\n",
    "if _is_importable(\"geopandas\"):\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        diag[\"geopandas_crs_ok\"] = True\n",
    "    except Exception as e:\n",
    "        diag[\"geopandas_crs_ok\"] = f\"error ({e.__class__.__name__})\"\n",
    "\n",
    "if _is_importable(\"pyproj\"):\n",
    "    try:\n",
    "        import pyproj\n",
    "        from pyproj.datadir import get_data_dir\n",
    "        diag[\"pyproj_version\"] = getattr(pyproj, \"__version__\", \"ok\")\n",
    "        diag[\"proj_data_dir\"] = get_data_dir()\n",
    "    except Exception as e:\n",
    "        diag[\"pyproj_version\"] = f\"error ({e.__class__.__name__})\"\n",
    "\n",
    "if _is_importable(\"fiona\"):\n",
    "    try:\n",
    "        import fiona\n",
    "        diag[\"fiona_version\"] = getattr(fiona, \"__version__\", \"ok\")\n",
    "        diag[\"fiona_gdal\"] = fiona.env.get_gdal_release_name()\n",
    "    except Exception as e:\n",
    "        diag[\"fiona_version\"] = f\"error ({e.__class__.__name__})\"\n",
    "\n",
    "if _is_importable(\"rasterio\"):\n",
    "    try:\n",
    "        import rasterio\n",
    "        diag[\"rasterio_gdal\"] = getattr(rasterio, \"__gdal_version__\", \"unknown\")\n",
    "        diag[\"rasterio_proj\"] = getattr(rasterio, \"__proj_version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        diag[\"rasterio_gdal\"] = f\"error ({e.__class__.__name__})\"\n",
    "\n",
    "# Torch: CUDA/MPS/CPU backend report (Macs often use MPS)\n",
    "if _is_importable(\"torch\"):\n",
    "    try:\n",
    "        import torch\n",
    "        backend = \"cpu\"\n",
    "        if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n",
    "            backend = f\"cuda:{torch.cuda.get_device_name(0)}\"\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            backend = \"mps\"\n",
    "        diag[\"torch_backend\"] = backend\n",
    "        diag[\"torch_dtype_default\"] = str(torch.get_default_dtype()) if hasattr(torch, \"get_default_dtype\") else \"float32\"\n",
    "    except Exception as e:\n",
    "        diag[\"torch_backend\"] = f\"error ({e.__class__.__name__})\"\n",
    "\n",
    "# Consolidated diagnostics (readable print to avoid DataFrame overhead here)\n",
    "print(\"\\n[Diagnostics]\")\n",
    "for k, v in diag.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "print(\"\\nEnvironment and version audit completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd76cf8-1e53-40ae-bc11-af238357f6c5",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# ==== Cell 2.1 — Keys & Connectivity Audit ====\n",
    "\n",
    "Purpose:\n",
    "- Verify API keys and data endpoints availability.\n",
    "- Perform lightweight connectivity and authorization checks.\n",
    "- Produce a structured JSON report saved to /reports/connectivity_report.json\n",
    "- Includes NASA FIRMS, Materials Project, OpenAI, and other key integrations.\n",
    "# ---------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46327e1a-6a40-487e-90f0-c4e155bcc56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] Region/Window: ES None → None\n",
      "[ENV] Keys (masked): OPENAI= sk-pro… MP= U8Wg4j… OPENTOPO= 9d7124… NASA_FIRMS_TOKEN= eyJ0eX… EARTHDATA_TOKEN= eyJ0eX…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f513332f8bc47658e8ff812d7c83475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving SummaryDoc documents:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connectivity report written to reports/connectivity_report.json\n",
      "{\n",
      "  \"generated_at\": \"2025-11-04T18:29:05.322936+00:00\",\n",
      "  \"region\": \"ES\",\n",
      "  \"window\": {\n",
      "    \"from\": null,\n",
      "    \"to\": null\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"earthdata\": {\n",
      "      \"ok\": true\n",
      "    },\n",
      "    \"effis\": {\n",
      "      \"ok\": false,\n",
      "      \"wfs_head\": {\n",
      "        \"ok\": true,\n",
      "        \"status_code\": 200,\n",
      "        \"elapsed_s\": 0.62,\n",
      "        \"content_type\": \"text/html\",\n",
      "        \"degraded\": false\n",
      "      },\n",
      "      \"wms_head\": {\n",
      "        \"ok\": true,\n",
      "        \"status_code\": 200,\n",
      "        \"elapsed_s\": 21.627,\n",
      "        \"content_type\": \"text/html\",\n",
      "        \"degraded\": true\n",
      "      },\n",
      "      \"wcs_head\": {\n",
      "        \"ok\": true,\n",
      "        \"status_code\": 200,\n",
      "        \"elapsed_s\": 21.964,\n",
      "        \"content_type\": \"text/html\",\n",
      "        \"degraded\": true\n",
      "      },\n",
      "      \"wfs_cap\": {\n",
      "        \"ok\": false,\n",
      "        \"reason\": \"ConnectionError: HTTPSConnectionPool(host='maps.effis.emergency.copernicus.eu', port=443): Max retries exceeded with url: /effis?service=WFS&request=GetCapabilities&version=1.1.1 (Caused by ReadTimeoutError(\\\"HTTPSConnectionPool(host='maps.effis.emergency.copernicus.eu', port=443): Read timed out. (read timeout=20)\\\"))\",\n",
      "        \"elapsed_s\": 61.653\n",
      "      },\n",
      "      \"wms_cap\": {\n",
      "        \"ok\": true,\n",
      "        \"status_code\": 200,\n",
      "        \"elapsed_s\": 0.358,\n",
      "        \"content_type\": \"application/vnd.ogc.wms_xml; charset=UTF-8\",\n",
      "        \"degraded\": false\n",
      "      },\n",
      "      \"wcs_cap\": {\n",
      "        \"ok\": true,\n",
      "        \"status_code\": 200,\n",
      "        \"elapsed_s\": 0.794,\n",
      "        \"content_type\": \"text/html\",\n",
      "        \"degraded\": false\n",
      "      },\n",
      "      \"wms_getmap_ping\": {\n",
      "        \"ok\": false,\n",
      "        \"reason\": \"ConnectionError: HTTPSConnectionPool(host='maps.effis.emergency.copernicus.eu', port=443): Max retries exceeded with url: /effis?SERVICE=WMS&REQUEST=GetMap&VERSION=1.1.1&LAYERS=ecmwf007.fwi&STYLES=&SRS=EPSG%3A4326&BBOX=-18%2C27%2C42%2C72&WIDTH=1&HEIGHT=1&FORMAT=image%2Fpng&TRANSPARENT=true&TIME=2025-11-03 (Caused by ReadTimeoutError(\\\"HTTPSConnectionPool(host='maps.effis.emergency.copernicus.eu', port=443): Read timed out. (read timeout=20)\\\"))\",\n",
      "        \"elapsed_s\": 61.585\n",
      "      },\n",
      "      \"degraded\": true\n",
      "    },\n",
      "    \"firms\": {\n",
      "      \"ok\": true,\n",
      "      \"feeds_detected\": 32,\n",
      "      \"validated\": 32\n",
      "    },\n",
      "    \"materials_project\": {\n",
      "      \"ok\": true,\n",
      "      \"sample_docs\": 1,\n",
      "      \"elapsed_s\": 1.748\n",
      "    },\n",
      "    \"nifc\": {\n",
      "      \"ok\": true,\n",
      "      \"status_code\": 200,\n",
      "      \"elapsed_s\": 0.381,\n",
      "      \"content_type\": \"application/json; charset=utf-8\",\n",
      "      \"degraded\": false\n",
      "    },\n",
      "    \"open_meteo\": {\n",
      "      \"ok\": true,\n",
      "      \"status_code\": 200,\n",
      "      \"elapsed_s\": 0.331,\n",
      "      \"content_type\": \"application/json; charset=utf-8\",\n",
      "      \"degraded\": false,\n",
      "      \"json_keys\": [\n",
      "        \"latitude\",\n",
      "        \"longitude\",\n",
      "        \"generationtime_ms\",\n",
      "        \"utc_offset_seconds\",\n",
      "        \"timezone\",\n",
      "        \"timezone_abbreviation\",\n",
      "        \"elevation\",\n",
      "        \"hourly_units\",\n",
      "        \"hourly\"\n",
      "      ]\n",
      "    },\n",
      "    \"openai\": {\n",
      "      \"ok\": true,\n",
      "      \"status_code\": 200,\n",
      "      \"elapsed_s\": 0.771,\n",
      "      \"content_type\": \"application/json\",\n",
      "      \"degraded\": false\n",
      "    },\n",
      "    \"opentopo\": {\n",
      "      \"ok\": true,\n",
      "      \"status_code\": 200,\n",
      "      \"elapsed_s\": 11.428,\n",
      "      \"content_type\": \"application/octet-stream\",\n",
      "      \"degraded\": false\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 2.1 — Keys & Connectivity Audit ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta, datetime, timezone\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, Any\n",
    "import importlib.util\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ------------------------------ Helpers ---------------------------------\n",
    "def _is_importable(name: str) -> bool:\n",
    "    return importlib.util.find_spec(name) is not None\n",
    "\n",
    "def mask(key: str, keep: int = 6) -> str:\n",
    "    if not key: return \"None\"\n",
    "    return key[:keep] + \"…\" if len(key) > keep else \"***\"\n",
    "\n",
    "def ok(status: bool, reason: str | None = None, extra: dict | None = None) -> dict:\n",
    "    out = {\"ok\": bool(status)}\n",
    "    if reason: out[\"reason\"] = reason\n",
    "    if extra: out.update(extra)\n",
    "    return out\n",
    "\n",
    "def _force_effis_base(url: str | None) -> str:\n",
    "    default = \"https://maps.effis.emergency.copernicus.eu/effis\"\n",
    "    if not url: return default\n",
    "    host = urlparse(url).netloc.lower()\n",
    "    if \"forest-fire.jrc.ec.europa.eu\" in host or \"jrc.ec.europa.eu\" in host:\n",
    "        return default\n",
    "    return url\n",
    "\n",
    "def _mk_session(total=2, backoff=0.5, status=(429, 502, 503, 504)) -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    r = Retry(total=total, backoff_factor=backoff, status_forcelist=list(status), raise_on_status=False)\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=r))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=r))\n",
    "    return s\n",
    "\n",
    "def _timed_request(method: str, url: str, *, timeout=(4, 8), session: requests.Session | None = None, **kwargs) -> Dict[str, Any]:\n",
    "    s = session or _mk_session()\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        resp = s.request(method, url, timeout=timeout, **kwargs)\n",
    "        dt = time.perf_counter() - t0\n",
    "        return {\n",
    "            \"ok\": resp.ok,\n",
    "            \"status_code\": getattr(resp, \"status_code\", None),\n",
    "            \"elapsed_s\": round(dt, 3),\n",
    "            \"content_type\": resp.headers.get(\"Content-Type\", \"\"),\n",
    "            \"degraded\": round(dt, 3) > 15\n",
    "        }\n",
    "    except Exception as e:\n",
    "        dt = time.perf_counter() - t0\n",
    "        return {\"ok\": False, \"reason\": f\"{e.__class__.__name__}: {e}\", \"elapsed_s\": round(dt, 3)}\n",
    "\n",
    "# --------------------------- Environment ---------------------------------\n",
    "env = {\n",
    "    \"WF_REGION\": os.getenv(\"WF_REGION\", \"ES\"),\n",
    "    \"WF_DATE_FROM\": os.getenv(\"WF_DATE_FROM\"),\n",
    "    \"WF_DATE_TO\": os.getenv(\"WF_DATE_TO\"),\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n",
    "    \"MP_API_KEY\": os.getenv(\"MP_API_KEY\", \"\"),\n",
    "    \"OPENTOPO_API_KEY\": os.getenv(\"OPENTOPO_API_KEY\", \"\"),\n",
    "    \"NASA_FIRMS_TOKEN\": os.getenv(\"NASA_FIRMS_TOKEN\", \"\"),\n",
    "    \"EFFIS_WFS_URL\": _force_effis_base(os.getenv(\"EFFIS_WFS_URL\", \"https://maps.effis.emergency.copernicus.eu/effis\")),\n",
    "    \"EFFIS_WMS_URL\": _force_effis_base(os.getenv(\"EFFIS_WMS_URL\", \"https://maps.effis.emergency.copernicus.eu/effis\")),\n",
    "    \"EFFIS_WCS_URL\": _force_effis_base(os.getenv(\"EFFIS_WCS_URL\", \"https://maps.effis.emergency.copernicus.eu/effis\")),\n",
    "    \"EFFIS_TYPENAME\": os.getenv(\"EFFIS_TYPENAME\", \"ms:modis.ba.poly\"),\n",
    "    \"NIFC_FS_URL\": os.getenv(\"NIFC_FS_URL\",\n",
    "        \"https://services3.arcgis.com/T4QMspbfLg3qTGWY/arcgis/rest/services/\"\n",
    "        \"WFIGS_Interagency_Perimeters_Current/FeatureServer/0\"),\n",
    "    \"EARTHDATA_TOKEN\": os.getenv(\"EARTHDATA_TOKEN\", \"\"),\n",
    "}\n",
    "print(f\"[ENV] Region/Window: {env['WF_REGION']} {env['WF_DATE_FROM']} → {env['WF_DATE_TO']}\")\n",
    "print(\"[ENV] Keys (masked):\",\n",
    "      \"OPENAI=\", mask(env[\"OPENAI_API_KEY\"]),\n",
    "      \"MP=\", mask(env[\"MP_API_KEY\"]),\n",
    "      \"OPENTOPO=\", mask(env[\"OPENTOPO_API_KEY\"]),\n",
    "      \"NASA_FIRMS_TOKEN=\", mask(env[\"NASA_FIRMS_TOKEN\"]),\n",
    "      \"EARTHDATA_TOKEN=\", mask(env[\"EARTHDATA_TOKEN\"]))\n",
    "\n",
    "# ----------------------------- Tasks -------------------------------------\n",
    "def check_openai() -> dict:\n",
    "    key = env[\"OPENAI_API_KEY\"]\n",
    "    if not key:\n",
    "        return ok(False, \"OPENAI_API_KEY missing\")\n",
    "    s = _mk_session()\n",
    "    r = _timed_request(\n",
    "        \"GET\", \"https://api.openai.com/v1/models\",\n",
    "        headers={\"Authorization\": f\"Bearer {key}\"},\n",
    "        timeout=(4, 8), session=s\n",
    "    )\n",
    "    return ok(r[\"ok\"], None if r[\"ok\"] else r.get(\"reason\"), r)\n",
    "\n",
    "def check_mp() -> dict:\n",
    "    if not env[\"MP_API_KEY\"]:\n",
    "        return ok(False, \"MP_API_KEY missing\")\n",
    "    try:\n",
    "        from mp_api.client import MPRester\n",
    "        t0 = time.perf_counter()\n",
    "        with MPRester(env[\"MP_API_KEY\"]) as mpr:\n",
    "            docs = mpr.materials.summary.search(fields=[\"material_id\"], chunk_size=1, num_chunks=1)\n",
    "        dt = round(time.perf_counter() - t0, 3)\n",
    "        return ok(True, extra={\"sample_docs\": len(docs), \"elapsed_s\": dt})\n",
    "    except Exception as e:\n",
    "        return ok(False, f\"mp-api error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "def check_opentopo() -> dict:\n",
    "    if not env[\"OPENTOPO_API_KEY\"]:\n",
    "        return ok(False, \"OPENTOPO_API_KEY missing\")\n",
    "    s = _mk_session()\n",
    "    r = _timed_request(\n",
    "        \"HEAD\", \"https://portal.opentopography.org/API/globaldem\",\n",
    "        params=dict(\n",
    "            demtype=\"SRTMGL3\", south=40.0, north=40.1, west=-3.8, east=-3.7,\n",
    "            outputFormat=\"GTiff\", API_Key=env[\"OPENTOPO_API_KEY\"]\n",
    "        ),\n",
    "        timeout=(10, 25), session=s  # increased for reliability\n",
    "    )\n",
    "    return ok(r[\"ok\"], None if r[\"ok\"] else r.get(\"reason\"), r)\n",
    "\n",
    "def check_open_meteo() -> dict:\n",
    "    s = _mk_session()\n",
    "    today = date.today()\n",
    "    params = dict(latitude=40.0, longitude=-3.7,\n",
    "                  start_date=(today - timedelta(days=2)).isoformat(),\n",
    "                  end_date=(today - timedelta(days=1)).isoformat(),\n",
    "                  hourly=\"temperature_2m\", timezone=\"UTC\")\n",
    "    r = _timed_request(\"GET\", \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "                       params=params, timeout=(4, 8), session=s)\n",
    "    if not r[\"ok\"]:\n",
    "        return ok(False, r.get(\"reason\"), r)\n",
    "    try:\n",
    "        js = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=params, timeout=(4, 8)).json()\n",
    "        valid = \"hourly\" in js and \"time\" in js[\"hourly\"]\n",
    "        return ok(valid, None if valid else \"unexpected JSON\", {**r, \"json_keys\": list(js.keys())})\n",
    "    except Exception as e:\n",
    "        return ok(False, f\"JSON parse: {e.__class__.__name__}: {e}\", r)\n",
    "\n",
    "def check_firms() -> dict:\n",
    "    firms_env = {k: v for k, v in os.environ.items() if k.startswith(\"FIRMS_\") and v.startswith(\"https\")}\n",
    "    token = env[\"NASA_FIRMS_TOKEN\"]\n",
    "    if not firms_env:\n",
    "        return ok(False, \"No FIRMS_* URLs found\")\n",
    "    s = _mk_session()\n",
    "    valid = 0\n",
    "    for name, url in firms_env.items():\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "        r = _timed_request(\"HEAD\", url, headers=headers, timeout=(4, 8), session=s)\n",
    "        if r[\"ok\"]: valid += 1\n",
    "    return ok(valid > 0, extra={\"feeds_detected\": len(firms_env), \"validated\": valid})\n",
    "\n",
    "def check_effis() -> dict:\n",
    "    \"\"\"Resilient EFFIS capability check with extended timeouts.\"\"\"\n",
    "    base_wfs = _force_effis_base(env[\"EFFIS_WFS_URL\"]).rstrip(\"/\")\n",
    "    base_wms = _force_effis_base(env[\"EFFIS_WMS_URL\"]).rstrip(\"/\")\n",
    "    base_wcs = _force_effis_base(env[\"EFFIS_WCS_URL\"]).rstrip(\"/\")\n",
    "    s = _mk_session(total=2, backoff=0.6)\n",
    "    headers_xml = {\"Accept\": \"application/xml\"}\n",
    "\n",
    "    # 1) HEAD ping\n",
    "    head_wfs = _timed_request(\"HEAD\", base_wfs, timeout=(6, 10), session=s)\n",
    "    head_wms = _timed_request(\"HEAD\", base_wms, timeout=(6, 10), session=s)\n",
    "    head_wcs = _timed_request(\"HEAD\", base_wcs, timeout=(6, 10), session=s)\n",
    "\n",
    "    # 2) Light GetCapabilities\n",
    "    def _cap(url, service):\n",
    "        return _timed_request(\n",
    "            \"GET\", url,\n",
    "            params={\"service\": service, \"request\": \"GetCapabilities\", \"version\": \"1.1.1\"},\n",
    "            headers={**headers_xml, \"Range\": \"bytes=0-2047\"},\n",
    "            timeout=(10, 20), session=s\n",
    "        )\n",
    "    wfs_cap = _cap(base_wfs, \"WFS\") if head_wfs[\"ok\"] else {\"ok\": False}\n",
    "    wms_cap = _cap(base_wms, \"WMS\") if head_wms[\"ok\"] else {\"ok\": False}\n",
    "    wcs_cap = _cap(base_wcs, \"WCS\") if head_wcs[\"ok\"] else {\"ok\": False}\n",
    "\n",
    "    # 3) Tiny GetMap ping\n",
    "    wms_ping = {\"ok\": False}\n",
    "    if wms_cap[\"ok\"]:\n",
    "        yday = (date.today() - timedelta(days=1)).isoformat()\n",
    "        params = {\n",
    "            \"SERVICE\": \"WMS\", \"REQUEST\": \"GetMap\", \"VERSION\": \"1.1.1\",\n",
    "            \"LAYERS\": \"ecmwf007.fwi\", \"STYLES\": \"\",\n",
    "            \"SRS\": \"EPSG:4326\", \"BBOX\": \"-18,27,42,72\",\n",
    "            \"WIDTH\": 1, \"HEIGHT\": 1, \"FORMAT\": \"image/png\",\n",
    "            \"TRANSPARENT\": \"true\", \"TIME\": yday\n",
    "        }\n",
    "        wms_ping = _timed_request(\"GET\", base_wms, params=params, timeout=(10, 20), session=s)\n",
    "\n",
    "    effis = {\n",
    "        \"wfs_head\": head_wfs, \"wms_head\": head_wms, \"wcs_head\": head_wcs,\n",
    "        \"wfs_cap\": wfs_cap, \"wms_cap\": wms_cap, \"wcs_cap\": wcs_cap,\n",
    "        \"wms_getmap_ping\": wms_ping\n",
    "    }\n",
    "    ok_all = all(v.get(\"ok\", False) for v in effis.values())\n",
    "    effis[\"degraded\"] = any(v.get(\"degraded\") for v in effis.values())\n",
    "    return {\"ok\": ok_all, **effis}\n",
    "\n",
    "def check_nifc() -> dict:\n",
    "    base = env[\"NIFC_FS_URL\"].rstrip(\"/\")\n",
    "    r = _timed_request(\"HEAD\", f\"{base}?f=json\", timeout=(4, 8))\n",
    "    return ok(r[\"ok\"], None if r[\"ok\"] else r.get(\"reason\"), r)\n",
    "\n",
    "def check_earthdata() -> dict:\n",
    "    has = bool(env.get(\"EARTHDATA_TOKEN\"))\n",
    "    return ok(has, None if has else \"EARTHDATA_TOKEN missing\")\n",
    "\n",
    "# --------------------------- Concurrent run ------------------------------\n",
    "tasks = {\n",
    "    \"openai\": check_openai,\n",
    "    \"materials_project\": check_mp,\n",
    "    \"opentopo\": check_opentopo,\n",
    "    \"open_meteo\": check_open_meteo,\n",
    "    \"firms\": check_firms,\n",
    "    \"effis\": check_effis,\n",
    "    \"nifc\": check_nifc,\n",
    "    \"earthdata\": check_earthdata,\n",
    "}\n",
    "\n",
    "report: Dict[str, dict] = {}\n",
    "with cf.ThreadPoolExecutor(max_workers=min(8, len(tasks))) as ex:\n",
    "    fut_map = {ex.submit(fn): name for name, fn in tasks.items()}\n",
    "    for fut in cf.as_completed(fut_map):\n",
    "        name = fut_map[fut]\n",
    "        try:\n",
    "            report[name] = fut.result()\n",
    "        except Exception as e:\n",
    "            report[name] = ok(False, f\"task error: {e.__class__.__name__}: {e}\")\n",
    "\n",
    "# ----------------------------- Persist -----------------------------------\n",
    "out = {\n",
    "    \"generated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"region\": env[\"WF_REGION\"],\n",
    "    \"window\": {\"from\": env[\"WF_DATE_FROM\"], \"to\": env[\"WF_DATE_TO\"]},\n",
    "    \"results\": dict(sorted(report.items(), key=lambda kv: kv[0])),\n",
    "}\n",
    "out_path = Path(\"reports\") / \"connectivity_report.json\"\n",
    "out_path.write_text(json.dumps(out, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"\\nConnectivity report written to {out_path}\")\n",
    "print(json.dumps(out, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f398528-0bdb-4279-ac08-5c62fb7c1852",
   "metadata": {},
   "source": [
    "## Cell 3 – Project Header & Global Configuration\n",
    "Defines project paths, environment variables, logging, and small I/O helpers.  \n",
    "Ensures reproducibility, consistent data handling, and clean outputs across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26c9da1-cebb-410b-b718-d121c85bd7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"region\": \"GLOBAL\", \"mode\": \"GLOBAL\", \"root\": \"/Users/evareysanchez/WildfiresAI\", \"processed\": \"/Users/evareysanchez/WildfiresAI/data/processed\", \"event\": \"Init config\", \"timestamp\": \"2025-11-04T18:29:24.987979Z\", \"level\": \"info\"}\n",
      "────────────────────────────────────────────────────────────\n",
      "WildfiresAI Global Configuration (Lab-grade)\n",
      "────────────────────────────────────────────────────────────\n",
      "Mode: GLOBAL | Region: GLOBAL\n",
      "Bounding Box: GLOBAL coverage (no spatial restriction)\n",
      "Paths: raw=raw, processed=processed, reports=reports\n",
      "Secrets: OpenAI=sk-pro…, FIRMS_TOKEN=eyJ0eX…, MAP_KEY=27f8d7…\n",
      "System: Darwin 24.6.0 | Python 3.13.5\n",
      "Timestamp (UTC): 2025-11-04T18:29:24.989553+00:00\n",
      "────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3: Project Header & Global Configuration (Lab-grade) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import os, platform\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import structlog\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------- Directories -------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "\n",
    "for p in (DATA_DIR, RAW_DIR, PROCESSED_DIR, REPORTS_DIR, CONFIG_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------ Environment ------------------------------\n",
    "load_dotenv()  # Load from .env or system\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "FIRMS_TOKEN    = os.getenv(\"FIRMS_TOKEN\", \"\")\n",
    "FIRMS_MAP_KEY  = os.getenv(\"FIRMS_MAP_KEY\", \"\")\n",
    "WF_REGION      = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "FGPL_MODE      = os.getenv(\"FGPL_MODE\", \"GLOBAL\").upper()\n",
    "\n",
    "VALID_REGIONS = {\"GLOBAL\", \"ES\"}\n",
    "if WF_REGION not in VALID_REGIONS:\n",
    "    WF_REGION = \"GLOBAL\"\n",
    "\n",
    "# --------------------------- Spatial Configuration ----------------------\n",
    "SPAIN_BBOX = (-9.5, 35.0, 3.5, 43.9)\n",
    "ACTIVE_BBOX = None if WF_REGION == \"GLOBAL\" else SPAIN_BBOX\n",
    "\n",
    "# ----------------------------- Structured Log ----------------------------\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\", utc=True),\n",
    "        structlog.processors.add_log_level,\n",
    "        structlog.processors.JSONRenderer(),\n",
    "    ],\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(20),  # INFO level\n",
    ")\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(region=WF_REGION, mode=FGPL_MODE)\n",
    "log.info(\"Init config\", root=str(PROJECT_ROOT), processed=str(PROCESSED_DIR))\n",
    "\n",
    "# ------------------------------ I/O Helpers ------------------------------\n",
    "def mask_key(key: str, n: int = 6) -> str:\n",
    "    \"\"\"Mask sensitive keys, keeping only first n chars.\"\"\"\n",
    "    return key[:n] + \"…\" if key else \"None\"\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to CSV (UTF-8, reproducible format).\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False, float_format=\"%.6f\", encoding=\"utf-8\")\n",
    "    log.info(\"Saved CSV\", path=str(path), rows=len(df))\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    \"\"\"Save DataFrame to compressed Parquet.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(path, index=False, compression=\"snappy\")\n",
    "    log.info(\"Saved Parquet\", path=str(path), rows=len(df))\n",
    "\n",
    "def preview(df: pd.DataFrame, n: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"Return the first n rows for consistent preview.\"\"\"\n",
    "    return df.head(n)\n",
    "\n",
    "# --------------------------- Configuration Echo --------------------------\n",
    "print(\"────────────────────────────────────────────────────────────\")\n",
    "print(\"WildfiresAI Global Configuration (Lab-grade)\")\n",
    "print(\"────────────────────────────────────────────────────────────\")\n",
    "print(f\"Mode: {FGPL_MODE} | Region: {WF_REGION}\")\n",
    "if ACTIVE_BBOX:\n",
    "    print(f\"Bounding Box: {ACTIVE_BBOX}\")\n",
    "else:\n",
    "    print(\"Bounding Box: GLOBAL coverage (no spatial restriction)\")\n",
    "print(f\"Paths: raw={RAW_DIR.name}, processed={PROCESSED_DIR.name}, reports={REPORTS_DIR.name}\")\n",
    "print(f\"Secrets: OpenAI={mask_key(OPENAI_API_KEY)}, FIRMS_TOKEN={mask_key(FIRMS_TOKEN)}, MAP_KEY={mask_key(FIRMS_MAP_KEY)}\")\n",
    "print(f\"System: {platform.system()} {platform.release()} | Python {platform.python_version()}\")\n",
    "print(f\"Timestamp (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "print(\"────────────────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20662a3-8a48-4345-a0f4-66493588d89b",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ==== Cell 3.1: Smart Date Synchronization (Auto Window Detection, Global-Aware) ====\n",
    "\n",
    " Purpose:\n",
    " Automatically detect or initialize temporal window for global datasets.\n",
    " Ensures WF_DATE_FROM / WF_DATE_TO are synchronized across .env, system env,\n",
    " and global pipeline state (GLOBAL or REGIONAL modes).\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a6db5-28d0-4aaa-b5b0-35b1bd283e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"stage\": \"date_sync\", \"date_from\": \"2025-10-28\", \"date_to\": \"2025-11-04\", \"event\": \"Using detected FIRMS-derived window\", \"timestamp\": \"2025-11-04T18:29:29.704296Z\", \"level\": \"info\"}\n",
      "{\"stage\": \"date_sync\", \"mode\": \"GLOBAL\", \"region\": \"GLOBAL\", \"WF_DATE_FROM\": \"2025-10-28\", \"WF_DATE_TO\": \"2025-11-04\", \"window_days\": 7, \"detected_from\": \"fires_terrain_EUROPE_2025-10-21_2025-10-28.parquet\", \"timestamp_utc\": \"2025-11-04T18:29:29.705465+00:00\", \"event\": \"Smart date synchronization complete\", \"timestamp\": \"2025-11-04T18:29:29.705516Z\", \"level\": \"info\"}\n",
      "───────────────────────────────────────────────\n",
      " WildfiresAI — Smart Date Synchronization\n",
      "───────────────────────────────────────────────\n",
      "Mode: GLOBAL | Region: GLOBAL\n",
      "WF_DATE_FROM = 2025-10-28\n",
      "WF_DATE_TO   = 2025-11-04\n",
      "Active window: 7 days\n",
      "Detected from local FIRMS file: fires_terrain_EUROPE_2025-10-21_2025-10-28.parquet\n",
      "Timestamp (UTC): 2025-11-04T18:29:29.705465+00:00\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3.1: Smart Date Synchronization (UTC-safe, Global-Aware) ====\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from dotenv import set_key\n",
    "import structlog\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Setup and context\n",
    "# ---------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "ENV_PATH = PROJECT_ROOT / \".env\"\n",
    "\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(stage=\"date_sync\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Detect temporal window from processed FIRMS datasets \n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Allow user override via environment variable\n",
    "FIRMS_DIR = Path(os.getenv(\"FIRMS_DIR\", PROJECT_ROOT / \"data\" / \"processed\"))\n",
    "\n",
    "# Match both CSV and Parquet, flexible names (e.g. fires_terrain_global_20251021.parquet)\n",
    "patterns = [\"fires_*_*.parquet\", \"fires_*_*.csv\", \"firms_*_*.parquet\", \"firms_*_*.csv\"]\n",
    "firms_files = []\n",
    "for pat in patterns:\n",
    "    firms_files += list(FIRMS_DIR.glob(pat))\n",
    "\n",
    "firms_files = sorted(firms_files, key=lambda p: p.stat().st_mtime)\n",
    "date_from, date_to = None, None\n",
    "\n",
    "if firms_files:\n",
    "    latest = firms_files[-1].name\n",
    "    import re\n",
    "    # Capture any 8-digit date pattern in the filename\n",
    "    m = re.findall(r\"\\d{8}\", latest)\n",
    "    if m:\n",
    "        date_to = datetime.strptime(m[-1], \"%Y%m%d\").date().isoformat()\n",
    "        if len(m) >= 2:\n",
    "            date_from = datetime.strptime(m[-2], \"%Y%m%d\").date().isoformat()\n",
    "        else:\n",
    "            # If only one date present → 7-day window\n",
    "            dt = datetime.strptime(m[-1], \"%Y%m%d\").date()\n",
    "            date_from = (dt - timedelta(days=7)).isoformat()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Default window (7 days, UTC-safe)\n",
    "# ---------------------------------------------------------------------\n",
    "if not date_from or not date_to:\n",
    "    now = datetime.now(timezone.utc).date()\n",
    "    date_to = now.isoformat()\n",
    "    date_from = (now - timedelta(days=7)).isoformat()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Override with environment variables only if detection failed\n",
    "# ---------------------------------------------------------------------\n",
    "env_from = os.getenv(\"WF_DATE_FROM\")\n",
    "env_to = os.getenv(\"WF_DATE_TO\")\n",
    "\n",
    "# Only use .env values if no dates were detected automatically\n",
    "if not date_from or not date_to:\n",
    "    if env_from:\n",
    "        date_from = env_from\n",
    "    if env_to:\n",
    "        date_to = env_to\n",
    "else:\n",
    "    # Detected dates take precedence, unless explicitly forced\n",
    "    log.info(\"Using detected FIRMS-derived window\", date_from=date_from, date_to=date_to)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Validate and persist (only update .env if changed)\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    dt_from = datetime.fromisoformat(date_from)\n",
    "    dt_to = datetime.fromisoformat(date_to)\n",
    "    if dt_to < dt_from:\n",
    "        raise ValueError(\"WF_DATE_TO earlier than WF_DATE_FROM\")\n",
    "except Exception as e:\n",
    "    now = datetime.now(timezone.utc).date()\n",
    "    dt_to = datetime.combine(now, datetime.min.time())\n",
    "    dt_from = dt_to - timedelta(days=7)\n",
    "    date_from, date_to = dt_from.date().isoformat(), dt_to.date().isoformat()\n",
    "    log.warning(\"Invalid date window detected, reset to 7 days\", error=str(e))\n",
    "\n",
    "os.environ[\"WF_DATE_FROM\"] = date_from\n",
    "os.environ[\"WF_DATE_TO\"] = date_to\n",
    "\n",
    "# Update .env only if necessary (avoids write churn in clusters)\n",
    "for k, v in {\"WF_DATE_FROM\": date_from, \"WF_DATE_TO\": date_to}.items():\n",
    "    current = os.getenv(k)\n",
    "    if current != v:\n",
    "        set_key(str(ENV_PATH), k, v)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Context summary + logging\n",
    "# ---------------------------------------------------------------------\n",
    "mode = os.getenv(\"FGPL_MODE\", \"GLOBAL\").upper()\n",
    "region = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "window_days = (datetime.fromisoformat(date_to) - datetime.fromisoformat(date_from)).days\n",
    "\n",
    "summary = {\n",
    "    \"mode\": mode,\n",
    "    \"region\": region,\n",
    "    \"WF_DATE_FROM\": date_from,\n",
    "    \"WF_DATE_TO\": date_to,\n",
    "    \"window_days\": window_days,\n",
    "    \"detected_from\": firms_files[-1].name if firms_files else \"none\",\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "}\n",
    "\n",
    "log.info(\"Smart date synchronization complete\", **summary)\n",
    "\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\" WildfiresAI — Smart Date Synchronization\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Mode: {mode} | Region: {region}\")\n",
    "print(f\"WF_DATE_FROM = {date_from}\")\n",
    "print(f\"WF_DATE_TO   = {date_to}\")\n",
    "print(f\"Active window: {window_days} days\")\n",
    "if firms_files:\n",
    "    print(f\"Detected from local FIRMS file: {firms_files[-1].name}\")\n",
    "else:\n",
    "    print(\"No local FIRMS data found — using default 7-day window.\")\n",
    "print(f\"Timestamp (UTC): {summary['timestamp_utc']}\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c7b5e-ba9f-4d5c-9986-af3412ab93a1",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ==== WildfiresAI — Cell 3.2: OpenAI API Integration (Global Adaptive) ====\n",
    "\n",
    "Purpose:\n",
    "    - Establish secure global connection to the OpenAI API.\n",
    "    - Adapt automatically between ONLINE and OFFLINE modes.\n",
    "    - Expose consistent defaults (model, temperature) for all LLM-backed modules.\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df14f54-284d-4887-a84f-da4862b8762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI API connection established.\n",
      "{\"stage\": \"openai_init\", \"network\": \"ONLINE\", \"model\": \"gpt-5.1\", \"temperature\": 0.2, \"openai_version\": \"1.107.1\", \"event\": \"OpenAI API configured\", \"timestamp\": \"2025-11-04T18:29:38.996610Z\", \"level\": \"info\"}\n",
      "───────────────────────────────────────────────\n",
      " WildfiresAI — OpenAI Global Integration\n",
      "───────────────────────────────────────────────\n",
      "Network: ONLINE\n",
      "Model:   gpt-5.1\n",
      "Temp:    0.2\n",
      "openai-sdk: 1.107.1\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ====  Cell 3.2: OpenAI API Integration (Global Adaptive, Lab-grade) ====\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, socket\n",
    "import openai\n",
    "from dotenv import set_key\n",
    "import structlog\n",
    "\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(stage=\"openai_init\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Detect API key and connectivity\n",
    "# ---------------------------------------------------------------------\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY not found. Export it in your terminal or .env file before continuing.\"\n",
    "    )\n",
    "\n",
    "def online(host: str = \"api.openai.com\", port: int = 443, timeout: int = 3) -> bool:\n",
    "    \"\"\"Return True if outbound connection to OpenAI API is possible.\"\"\"\n",
    "    try:\n",
    "        socket.create_connection((host, port), timeout=timeout)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "is_online = online()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Configure client\n",
    "# ---------------------------------------------------------------------\n",
    "openai.api_key = api_key\n",
    "openai_version = getattr(openai, \"__version__\", \"unknown\")\n",
    "\n",
    "if is_online:\n",
    "    print(\" OpenAI API connection established.\")\n",
    "else:\n",
    "    print(\" No external network detected — switching to OFFLINE mode.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Global defaults (adaptive)\n",
    "# ---------------------------------------------------------------------\n",
    "DEFAULT_LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-5.1\" if is_online else \"gpt-4o-mini\")\n",
    "DEFAULT_TEMPERATURE = float(os.getenv(\"LLM_TEMPERATURE\", \"0.2\"))\n",
    "\n",
    "# Persist only if values changed\n",
    "env_path = \".env\"\n",
    "current_env = {k: os.getenv(k) for k in (\"LLM_MODEL\", \"LLM_TEMPERATURE\")}\n",
    "if current_env.get(\"LLM_MODEL\") != DEFAULT_LLM_MODEL:\n",
    "    set_key(env_path, \"LLM_MODEL\", DEFAULT_LLM_MODEL)\n",
    "if current_env.get(\"LLM_TEMPERATURE\") != str(DEFAULT_TEMPERATURE):\n",
    "    set_key(env_path, \"LLM_TEMPERATURE\", str(DEFAULT_TEMPERATURE))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Structured logging summary\n",
    "# ---------------------------------------------------------------------\n",
    "log.info(\n",
    "    \"OpenAI API configured\",\n",
    "    network=\"ONLINE\" if is_online else \"OFFLINE\",\n",
    "    model=DEFAULT_LLM_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    "    openai_version=openai_version,\n",
    ")\n",
    "\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\" WildfiresAI — OpenAI Global Integration\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Network: {'ONLINE' if is_online else 'OFFLINE'}\")\n",
    "print(f\"Model:   {DEFAULT_LLM_MODEL}\")\n",
    "print(f\"Temp:    {DEFAULT_TEMPERATURE}\")\n",
    "print(f\"openai-sdk: {openai_version}\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4755988-b40d-4e63-9261-9282c0c21da0",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ==== WildfiresAI — Cell 3.3: Global Logging & Scientific Telemetry ====\n",
    "\n",
    "Purpose:\n",
    "    - Configure unified logging for all WildfiresAI frameworks.\n",
    "    - Record every major event, warning, and error with timestamps.\n",
    "    - Integrate with tqdm progress bars and persist logs to /reports/logs/.\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dff3031-4292-4eaf-8526-352091c838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"stage\": \"global_logger\", \"timestamp\": \"2025-11-04T18:29:43.769809Z\", \"level\": \"info\", \"message\": \"Global logging system initialized\", \"log_file\": \"/Users/evareysanchez/WildfiresAI/reports/logs/wildfiresai_20251104_182943.log\", \"event\": \"Global logging system initialized\"}\n",
      "───────────────────────────────────────────────\n",
      "  WildfiresAI — Global Logging System Active\n",
      "───────────────────────────────────────────────\n",
      "Logs directory: /Users/evareysanchez/WildfiresAI/reports/logs\n",
      "Session log:    wildfiresai_20251104_182943.log\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 3.3: Global Logging & Scientific Telemetry (Lab-grade) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, logging, structlog\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Define persistent log paths\n",
    "# ---------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "LOG_DIR = PROJECT_ROOT / \"reports\" / \"logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FILE = LOG_DIR / f\"wildfiresai_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Configure Python logging (base + file handler)\n",
    "# ---------------------------------------------------------------------\n",
    "file_handler = logging.FileHandler(LOG_FILE, mode=\"a\", encoding=\"utf-8\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[file_handler, stream_handler],\n",
    "    format=\"%(message)s\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Configure structlog for unified output\n",
    "# ---------------------------------------------------------------------\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\", utc=True),\n",
    "        structlog.stdlib.add_log_level,\n",
    "        structlog.processors.StackInfoRenderer(),\n",
    "        structlog.processors.format_exc_info,\n",
    "        structlog.processors.JSONRenderer()  # JSON-safe format for scientific telemetry\n",
    "    ],\n",
    "    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    ")\n",
    "\n",
    "log = structlog.get_logger(\"wildfiresai\").bind(stage=\"global_logger\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) tqdm-compatible progress wrapper\n",
    "# ---------------------------------------------------------------------\n",
    "def log_progress(iterable, desc: str = \"Processing\", total: int | None = None):\n",
    "    \"\"\"Wrapper around tqdm with safe logging output.\"\"\"\n",
    "    return tqdm(iterable, desc=desc, total=total, dynamic_ncols=True, leave=False)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Unified persistent logging function\n",
    "# ---------------------------------------------------------------------\n",
    "def log_event(level: str, message: str, **kwargs) -> None:\n",
    "    \"\"\"Unified logger with UTC timestamps and file persistence.\"\"\"\n",
    "    ts = datetime.now(timezone.utc).isoformat()\n",
    "    record = {\"timestamp\": ts, \"level\": level.upper(), \"message\": message, **kwargs}\n",
    "    if level.lower() == \"error\":\n",
    "        log.error(message, **record)\n",
    "    elif level.lower() == \"warning\":\n",
    "        log.warning(message, **record)\n",
    "    else:\n",
    "        log.info(message, **record)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Example test entry\n",
    "# ---------------------------------------------------------------------\n",
    "log_event(\"info\", \"Global logging system initialized\", log_file=str(LOG_FILE))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) User feedback summary\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\"  WildfiresAI — Global Logging System Active\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Logs directory: {LOG_DIR}\")\n",
    "print(f\"Session log:    {LOG_FILE.name}\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532af3c-a3dc-4c1b-baf8-3a54232ee48e",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 4 — AG² Framework Architecture Overview ====\n",
    " Purpose\n",
    "This section introduces the AG² (Analysis → Generation → Action) architecture — the core logic driving WildfiresAI.\n",
    "\n",
    "- **Framework A – Wildfire Intelligence**  \n",
    "  Gathers and analyzes environmental, climatic, and terrain data (FIRMS, DEM, Open-Meteo) to estimate ignition risk and propagation dynamics.\n",
    "\n",
    "- **LLM Orchestrator – Natural-Language Bridge**  \n",
    "  Parses user queries (e.g., “fires in Spain last week”) and translates them into structured AG² contexts \n",
    "  (region, timeframe, intent), dynamically triggering the right modules.\n",
    "\n",
    "- **Coordinator – AG² Bridge**  \n",
    "  Mediates data flow from A → B, ensuring JSON integrity, spatiotemporal consistency, and versioned reproducibility.\n",
    "\n",
    "- **Framework B – Materials Intelligence**  \n",
    "  Consumes A’s outputs and applies materials-science reasoning (via MP API) to identify optimal compounds for fire containment.\n",
    "\n",
    "All inter-framework communication occurs via structured JSON under `/reports/`, ensuring transparency and traceability of every AG² cycle.\n",
    "# -------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8489d7c-61ca-49d4-821a-063634c3a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinator initialized at 2025-11-04T19:29:51\n",
      "Ready to link Framework A (4.1) ↔ LLM Orchestrator (4.3) ↔ Framework B (4.4) via AG².\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4: AG² Framework Architecture Overview ====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os, json, datetime as dt, pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️) Environment & paths\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT  = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "DATA_DIR      = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "for p in (DATA_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION     = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "WF_DATE_FROM  = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "WF_DATE_TO    = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️) Helper utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "def _latest_file_glob(pattern: str, base: Path) -> Optional[Path]:\n",
    "    files = sorted(base.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return files[0] if files else None\n",
    "\n",
    "def _dump_report(name: str, text_payload: str) -> Path:\n",
    "    \"\"\"Persist any text/JSON payload under reports/<name>.json.\"\"\"\n",
    "    path = REPORTS_DIR / f\"{name}.json\"\n",
    "    try:\n",
    "        obj = json.loads(text_payload) if text_payload.strip().startswith(\"{\") else {\"payload\": text_payload}\n",
    "    except Exception:\n",
    "        obj = {\"payload\": text_payload}\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️) Coordinator — AG² Bridge (A ↔ B)\n",
    "# ---------------------------------------------------------------------------\n",
    "class Coordinator:\n",
    "    \"\"\"\n",
    "    Core bridge between Framework A, Framework B, and now the LLM Orchestrator.\n",
    "\n",
    "    - Executes A → collects wildfire intelligence.\n",
    "    - Passes A’s JSON payload into B for materials or containment reasoning.\n",
    "    - Allows partial or full pipeline execution (A only, B only, or A→B).\n",
    "    - Persists all intermediate results in /reports for full reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_A: Optional[str] = None\n",
    "        self.last_B: Optional[str] = None\n",
    "        self.started_at = dt.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "    # --- Execute Framework A (defined in Cell 4.1) ---\n",
    "    def run_A(self, **kw) -> str:\n",
    "        from framework_a import run_wildfire_framework  # dynamically resolved\n",
    "        self.last_A = run_wildfire_framework(**kw)\n",
    "        print(\"A →\", self.last_A[:200], \"...\")\n",
    "        return self.last_A\n",
    "\n",
    "    # --- Execute Framework B (defined in Cell 4.4) ---\n",
    "    def run_B(self) -> str:\n",
    "        from framework_b import run_material_framework  # dynamically resolved\n",
    "        if not self.last_A:\n",
    "            print(\"⚠️ Framework B requested but A not available — running fallback.\")\n",
    "            self.last_A = \"{}\"\n",
    "        self.last_B = run_material_framework(self.last_A)\n",
    "        print(\"B →\", self.last_B[:200], \"...\")\n",
    "        return self.last_B\n",
    "\n",
    "    # --- Full pipeline A→B ---\n",
    "    def pipeline(self, **kw) -> tuple[str, str]:\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        print(\"  WildfiresAI AG² Pipeline (A → B) — Start\")\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        a_summary = self.run_A(**kw)\n",
    "        b_summary = self.run_B()\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        print(\"  WildfiresAI AG² Pipeline Completed\")\n",
    "        print(\"══════════════════════════════════════════════════════════\")\n",
    "        return a_summary, b_summary\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️) Initialize coordinator\n",
    "# ---------------------------------------------------------------------------\n",
    "coordinator = Coordinator()\n",
    "print(f\"Coordinator initialized at {coordinator.started_at}\")\n",
    "print(\"Ready to link Framework A (4.1) ↔ LLM Orchestrator (4.3) ↔ Framework B (4.4) via AG².\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73764efb-c230-4f64-8ce5-3bf5b78a04c8",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# ==== WildfiresAI - Cell 4.0 — Region Selector (NASA FIRMS Feeds) ====\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "This cell defines the **active geographical region** for WildfiresAI.  \n",
    "The region determines which **NASA FIRMS dataset** will be used by Framework A\n",
    "for wildfire detection and environmental analysis.\n",
    "\n",
    "Available regions:\n",
    "- `EUROPE`\n",
    "- `NORTH_AMERICA`\n",
    "- `SOUTH_AMERICA`\n",
    "- `AFRICA`\n",
    "- `ASIA`\n",
    "- `OCEANIA`\n",
    "- `GLOBAL`\n",
    "\n",
    "The system dynamically builds the FIRMS feed URL for the chosen region and \n",
    "stores it in environment variables (`WF_REGION`, `FIRMS_VIIRS_<REGION>_7D`).\n",
    "\n",
    "To switch regions:\n",
    "```python\n",
    "WF_REGION = \"AFRICA\"      # or EUROPE, ASIA, GLOBAL, etc.\n",
    "\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9962f0-b2b5-4fb2-8210-83cd6935beda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "───────────────────────────────────────────────\n",
      " WildfiresAI — Active FIRMS Region Selected\n",
      "───────────────────────────────────────────────\n",
      "Region: EUROPE\n",
      "Bounding Box: -25.0,33.0,45.0,72.0\n",
      "Feed URL: https://firms.modaps.eosdis.nasa.gov/api/area/csv/27f8d7a213b737284b155923ba7dd642/VIIRS_SNPP_NRT/-25.0,33.0,45.0,72.0/7\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Auto Region Selector (All FIRMS Regions, Coordinate-Accurate) ====\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1️ Available Regions (NASA FIRMS coordinate-based areas)\n",
    "# ---------------------------------------------------------------------------\n",
    "BBOXES = {\n",
    "    \"GLOBAL\": \"-180.0,-90.0,180.0,90.0\",\n",
    "    \"EUROPE\": \"-25.0,33.0,45.0,72.0\",\n",
    "    \"NORTH_AMERICA\": \"-170.0,5.0,-52.0,83.0\",\n",
    "    \"SOUTH_AMERICA\": \"-93.0,-60.0,-30.0,15.0\",\n",
    "    \"ASIA\": \"25.0,-10.0,180.0,80.0\",\n",
    "    \"AFRICA\": \"-20.0,-35.0,55.0,38.0\",\n",
    "    \"OCEANIA\": \"110.0,-50.0,180.0,0.0\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2️ Choose active region (default = EUROPE)\n",
    "# ---------------------------------------------------------------------------\n",
    "WF_REGION = os.getenv(\"WF_REGION\", \"EUROPE\").upper()\n",
    "if WF_REGION not in BBOXES:\n",
    "    raise ValueError(f\"Invalid WF_REGION '{WF_REGION}'. Must be one of: {list(BBOXES.keys())}\")\n",
    "\n",
    "bbox = BBOXES[WF_REGION]\n",
    "os.environ[\"WF_REGION\"] = WF_REGION\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3️ Build NASA FIRMS feed URL dynamically (7-day VIIRS)\n",
    "# ---------------------------------------------------------------------------\n",
    "token = \"27f8d7a213b737284b155923ba7dd642\"  # Registered FIRMS token\n",
    "feed_url = f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{token}/VIIRS_SNPP_NRT/{bbox}/7\"\n",
    "\n",
    "env_key = f\"FIRMS_VIIRS_{WF_REGION}_7D\"\n",
    "os.environ[env_key] = feed_url\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4️ Feedback summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(\" WildfiresAI — Active FIRMS Region Selected\")\n",
    "print(\"───────────────────────────────────────────────\")\n",
    "print(f\"Region: {WF_REGION}\")\n",
    "print(f\"Bounding Box: {bbox}\")\n",
    "print(f\"Feed URL: {feed_url}\")\n",
    "print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f50a86-0f88-4a3b-8abc-0ef98dc0f8ce",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 4.1 — Framework A (Wildfire Intelligence Layer) ====\n",
    "Framework A = Wildfire Intelligence Layer  \n",
    "\n",
    "Responsible for environmental analysis, risk estimation, terrain enrichment  \n",
    "and strategic planning.  \n",
    "Each agent follows the AG2-style contract (text in → text out JSON string).\n",
    "\n",
    "Includes the **WildfireFilter** (Universal WildfiresAI Filter, UWF)  \n",
    "for spatial, temporal, environmental, and scientific filtering.\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a65be-a82d-48f8-b21a-32dfd008e6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FIRMS feed: FIRMS_VIIRS_EUROPE_7D\n",
      "URL: https://firms.modaps.eosdis.nasa.gov/api/area/csv/27f8d7a213b737284b155923ba7dd642/VIIRS_SNPP_NRT/-25.0,33.0,45.0,72.0/7\n",
      "FIRMS CSV downloaded successfully — 5,043 fire detections.\n",
      "Missing columns in FIRMS data: ['brightness'] (filled with NaN if required)\n",
      "[Patch] brightness reconstructed from bright_ti4 + bright_ti5.\n",
      "[Audit] Brightness coverage: 5,043/5,043 rows (100.0%) valid.\n",
      "Total fires: 5,043 | High confidence: 0 | Mean brightness: 312.23\n",
      "[Meta] DataFrame columns standardized and region tag applied.\n",
      "FIRMS data saved to: data/processed/fires_terrain_europe_20251104.parquet\n",
      "[Cache] Using local FIRMS file: firms_viirs_snpp_EUROPE_2025-10-28_2025-11-04.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 4.1 — Framework A (Wildfire Intelligence Layer) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "import os, json, datetime as dt, pandas as pd, numpy as np, geopandas as gpd\n",
    "import io, requests\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Automatically select FIRMS feed based on active region\n",
    "# ---------------------------------------------------------------------------\n",
    "region = os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "feed_key = f\"FIRMS_VIIRS_{region}_7D\"\n",
    "FIRMS_CSV_URL = os.getenv(feed_key)\n",
    "\n",
    "if not FIRMS_CSV_URL:\n",
    "    raise RuntimeError(f\"No FIRMS environment variable found for region '{region}'.\")\n",
    "\n",
    "print(f\"Using FIRMS feed: {feed_key}\")\n",
    "print(f\"URL: {FIRMS_CSV_URL}\")\n",
    "\n",
    "## ---------------------------------------------------------------------------\n",
    "# 2) Download the CSV from NASA FIRMS API (with EU fallback if regional empty)\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    # Primary request (regional feed from FIRMS_CSV_URL)\n",
    "    r = requests.get(FIRMS_CSV_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    df_firms = pd.read_csv(io.StringIO(r.text))\n",
    "    print(f\"FIRMS CSV downloaded successfully — {len(df_firms):,} fire detections.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error downloading or reading FIRMS feed: {e}\")\n",
    "\n",
    "# --- BEGIN PATCH 4.1: EUROPE fallback when regional feed is empty/invalid ---\n",
    "def _europe_bbox() -> tuple[float, float, float, float]:\n",
    "    # Extended Europe bounding box: (min_lon, min_lat, max_lon, max_lat)\n",
    "    return (-31.3, 27.0, 39.7, 71.2)\n",
    "\n",
    "def _fallback_global_europe() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fallback to the GLOBAL 7D feed and constrain to Europe bbox.\n",
    "    Requires FIRMS_VIIRS_GLOBAL_7D in the environment.\n",
    "    \"\"\"\n",
    "    world_url = os.getenv(\"FIRMS_VIIRS_GLOBAL_7D\")\n",
    "    if not world_url:\n",
    "        raise RuntimeError(\"GLOBAL fallback requested but FIRMS_VIIRS_GLOBAL_7D is not set.\")\n",
    "    rr = requests.get(world_url, timeout=60)\n",
    "    rr.raise_for_status()\n",
    "    tmp = pd.read_csv(io.StringIO(rr.text))\n",
    "    tmp = tmp.rename(columns=str.lower)\n",
    "    if not {\"latitude\", \"longitude\"}.issubset(set(tmp.columns)):\n",
    "        raise RuntimeError(\"GLOBAL fallback missing latitude/longitude columns.\")\n",
    "    xmin, ymin, xmax, ymax = _europe_bbox()\n",
    "    tmp = tmp[(tmp[\"longitude\"].between(xmin, xmax)) & (tmp[\"latitude\"].between(ymin, ymax))]\n",
    "    return tmp.reset_index(drop=True)\n",
    "\n",
    "# Trigger fallback if the regional response is empty or lacks lat/lon\n",
    "_need_fallback = (df_firms.shape[0] == 0) or (df_firms.shape[1] == 0)\n",
    "if not _need_fallback:\n",
    "    cols_lc = {c.lower() for c in df_firms.columns}\n",
    "    if not {\"latitude\", \"longitude\"}.issubset(cols_lc):\n",
    "        _need_fallback = True\n",
    "\n",
    "if _need_fallback:\n",
    "    print(\"[FIRMS] Regional feed is empty/invalid. Falling back to GLOBAL feed filtered to Europe.\")\n",
    "    df_firms = _fallback_global_europe()\n",
    "    print(f\"[FIRMS] Fallback rows after Europe bbox: {len(df_firms):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) Validate and clean essential columns\n",
    "# ---------------------------------------------------------------------------\n",
    "expected_cols = [\"latitude\", \"longitude\", \"brightness\", \"acq_date\", \"acq_time\", \"confidence\", \"instrument\"]\n",
    "missing = [c for c in expected_cols if c not in df_firms.columns]\n",
    "if missing:\n",
    "    print(f\"Missing columns in FIRMS data: {missing} (filled with NaN if required)\")\n",
    "\n",
    "df_firms = df_firms.rename(columns=str.lower)\n",
    "df_firms[\"datetime\"] = pd.to_datetime(\n",
    "    df_firms[\"acq_date\"] + \" \" + df_firms[\"acq_time\"].astype(str).str.zfill(4),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "df_firms[\"confidence\"] = pd.to_numeric(df_firms.get(\"confidence\", 0), errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.1) Reconstruct brightness if missing (using VIIRS TI4 / TI5 channels)\n",
    "# ---------------------------------------------------------------------------\n",
    "if \"brightness\" not in df_firms.columns:\n",
    "    if {\"bright_ti4\", \"bright_ti5\"}.issubset(df_firms.columns):\n",
    "        df_firms[\"brightness\"] = (\n",
    "            0.7 * df_firms[\"bright_ti4\"].astype(float)\n",
    "            + 0.3 * df_firms[\"bright_ti5\"].astype(float)\n",
    "        )\n",
    "        print(\"[Patch] brightness reconstructed from bright_ti4 + bright_ti5.\")\n",
    "    elif \"bright_ti4\" in df_firms.columns:\n",
    "        df_firms[\"brightness\"] = df_firms[\"bright_ti4\"].astype(float)\n",
    "        print(\"[Patch] brightness derived from bright_ti4 only.\")\n",
    "    else:\n",
    "        df_firms[\"brightness\"] = np.nan\n",
    "        print(\"[Patch] brightness missing — filled with NaN.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.2) Brightness quality audit\n",
    "# ---------------------------------------------------------------------------\n",
    "if \"brightness\" in df_firms.columns:\n",
    "    total = len(df_firms)\n",
    "    valid = df_firms[\"brightness\"].notna().sum()\n",
    "    pct_valid = 100 * valid / total if total > 0 else 0\n",
    "    print(f\"[Audit] Brightness coverage: {valid:,}/{total:,} rows ({pct_valid:.1f}%) valid.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Quick descriptive summary\n",
    "# ---------------------------------------------------------------------------\n",
    "fires_total = len(df_firms)\n",
    "fires_high_conf = len(df_firms[df_firms.get(\"confidence\", 0) > 80])\n",
    "\n",
    "for col in [\"brightness\", \"bright_ti4\", \"bright_ti5\", \"frp\"]:\n",
    "    if col in df_firms.columns:\n",
    "        mean_brightness = round(df_firms[col].mean(), 2)\n",
    "        break\n",
    "else:\n",
    "    mean_brightness = None\n",
    "\n",
    "print(\n",
    "    f\"Total fires: {fires_total:,} | High confidence: {fires_high_conf:,} | \"\n",
    "    f\"Mean brightness: {mean_brightness}\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.1) Optional Data Integrity Enhancements (non-destructive)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# 1. Add region/source column for future multi-region integration\n",
    "df_firms[\"source_region\"] = region\n",
    "\n",
    "# 2. Drop potential duplicate detections (same lat/lon/date)\n",
    "before = len(df_firms)\n",
    "df_firms = df_firms.drop_duplicates(subset=[\"latitude\", \"longitude\", \"acq_date\"])\n",
    "after = len(df_firms)\n",
    "if before != after:\n",
    "    print(f\"[Clean] Removed {before - after:,} duplicate detections (spatial-temporal).\")\n",
    "\n",
    "# 3. Ensure deterministic column ordering (for reproducible parquet exports)\n",
    "cols = sorted(df_firms.columns)\n",
    "df_firms = df_firms[cols]\n",
    "print(\"[Meta] DataFrame columns standardized and region tag applied.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Save processed data for Framework A → Coordinator bridge\n",
    "# ---------------------------------------------------------------------------\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "region_clean = region.lower().replace(\" \", \"_\")\n",
    "\n",
    "out_path = PROCESSED_DIR / f\"fires_terrain_{region_clean}_{pd.Timestamp.now():%Y%m%d}.parquet\"\n",
    "df_firms.to_parquet(out_path, index=False)\n",
    "print(f\"FIRMS data saved to: {out_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Shared paths & environment\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "DATA_DIR      = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "REPORTS_DIR   = PROJECT_ROOT / \"reports\"\n",
    "for d in (DATA_DIR, PROCESSED_DIR, REPORTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "DATE_FROM = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "DATE_TO   = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Real data ingestion: FIRMS + Open-Meteo + DEM (OpenTopography, COP30 tiling)\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    # Select active FIRMS source (env fallback to region feed)\n",
    "    firms_env = os.getenv(\"FIRMS_CSV_URL\") or os.getenv(f\"FIRMS_VIIRS_{WF_REGION}_7D\")\n",
    "    if not firms_env:\n",
    "        raise FileNotFoundError(\"No valid FIRMS data source defined in environment.\")\n",
    "    os.environ[\"FIRMS_CSV_URL\"] = firms_env  # make available to downstream modules\n",
    "\n",
    "    # Local cache\n",
    "    raw_dir = DATA_DIR / \"raw\"\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    firms_local = raw_dir / f\"firms_viirs_snpp_{WF_REGION}_{DATE_FROM}_{DATE_TO}.csv\"\n",
    "\n",
    "    if firms_local.exists():\n",
    "        df_firms = pd.read_csv(firms_local)\n",
    "        print(f\"[Cache] Using local FIRMS file: {firms_local.name}\")\n",
    "    else:\n",
    "        headers = {}\n",
    "        token = os.getenv(\"NASA_FIRMS_TOKEN\", \"\")\n",
    "        if token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "        r = requests.get(firms_env, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        firms_local.write_text(r.text, encoding=\"utf-8\")\n",
    "        df_firms = pd.read_csv(io.StringIO(r.text))\n",
    "        print(f\"[Download] FIRMS data fetched from: {firms_env}\")\n",
    "\n",
    "    # Normalize columns and build GeoDataFrame\n",
    "    df_firms.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\"}, inplace=True)\n",
    "    df_firms[\"acq_date\"] = pd.to_datetime(df_firms[\"acq_date\"], errors=\"coerce\")\n",
    "    gdf_firms = gpd.GeoDataFrame(\n",
    "        df_firms, geometry=gpd.points_from_xy(df_firms.lon, df_firms.lat), crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # ---- Weather join (Open-Meteo)\n",
    "    lat_c, lon_c = gdf_firms.lat.mean(), gdf_firms.lon.mean()\n",
    "    r_weather = requests.get(\n",
    "        \"https://archive-api.open-meteo.com/v1/archive\",\n",
    "        params={\n",
    "            \"latitude\": lat_c, \"longitude\": lon_c,\n",
    "            \"start_date\": DATE_FROM, \"end_date\": DATE_TO,\n",
    "            \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\",\n",
    "            \"timezone\": \"UTC\",\n",
    "        },\n",
    "        timeout=30,\n",
    "    )\n",
    "    r_weather.raise_for_status()\n",
    "    w = pd.DataFrame(r_weather.json()[\"hourly\"])\n",
    "    w[\"time\"] = pd.to_datetime(w[\"time\"])\n",
    "    w[\"date\"] = w[\"time\"].dt.floor(\"D\")\n",
    "    df_firms[\"date\"] = df_firms[\"acq_date\"].dt.floor(\"D\")\n",
    "    df_join = pd.merge(df_firms, w.groupby(\"date\").mean(numeric_only=True), on=\"date\", how=\"left\")\n",
    "    df_join.rename(\n",
    "        columns={\n",
    "            \"temperature_2m\": \"temperature\",\n",
    "            \"relative_humidity_2m\": \"humidity\",\n",
    "            \"wind_speed_10m\": \"wind_ms\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # ---- DEM enrichment (OpenTopography, COP30, 1°×1° tiling)\n",
    "    try:\n",
    "        import rasterio\n",
    "\n",
    "        # Initialize elevation/slope as NaN; will be filled per tile\n",
    "        df_join[\"elevation_m\"] = np.nan\n",
    "        df_join[\"slope_deg\"] = np.nan\n",
    "\n",
    "        # Bin points into 1x1 degree tiles\n",
    "        df_join[\"lat_bin\"] = np.floor(df_join[\"lat\"]).astype(int)\n",
    "        df_join[\"lon_bin\"] = np.floor(df_join[\"lon\"]).astype(int)\n",
    "\n",
    "        # Prioritize tiles with more points to minimize requests\n",
    "        tile_counts = (\n",
    "            df_join.groupby([\"lat_bin\", \"lon_bin\"]).size().reset_index(name=\"n\").sort_values(\"n\", ascending=False)\n",
    "        )\n",
    "\n",
    "        # Safety cap: avoid too many remote calls in one run\n",
    "        MAX_TILES = 30  # increase if you want higher coverage\n",
    "        processed_tiles = 0\n",
    "\n",
    "        for _, row in tile_counts.iterrows():\n",
    "            if processed_tiles >= MAX_TILES:\n",
    "                break\n",
    "\n",
    "            latb = int(row[\"lat_bin\"])\n",
    "            lonb = int(row[\"lon_bin\"])\n",
    "\n",
    "            # Clamp latitude to plausible DEM coverage\n",
    "            south = max(latb, -89)\n",
    "            north = min(latb + 1, 90)\n",
    "\n",
    "            # Normalize longitude to [-180, 180]\n",
    "            west = lonb\n",
    "            if west < -180:\n",
    "                west += 360\n",
    "            if west > 180:\n",
    "                west -= 360\n",
    "            east = west + 1\n",
    "            if east > 180:\n",
    "                east -= 360\n",
    "\n",
    "            dem_url = (\n",
    "                \"https://portal.opentopography.org/API/globaldem?\"\n",
    "                f\"demtype=COP30&south={south:.6f}&north={north:.6f}\"\n",
    "                f\"&west={west:.6f}&east={east:.6f}\"\n",
    "                f\"&outputFormat=GTiff&API_Key={os.getenv('OPENTOPO_API_KEY','')}\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                r = requests.get(dem_url, timeout=60)\n",
    "                r.raise_for_status()\n",
    "                if len(r.content) < 8192:\n",
    "                    # Skip obviously invalid responses\n",
    "                    continue\n",
    "\n",
    "                # Persist tile and sample elevations\n",
    "                tile_path = (DATA_DIR / \"raw\" / f\"dem_tile_{south}_{west}.tif\")\n",
    "                tile_path.write_bytes(r.content)\n",
    "\n",
    "                # Subset rows in this tile\n",
    "                mask_tile = (df_join[\"lat_bin\"] == latb) & (df_join[\"lon_bin\"] == lonb)\n",
    "                sub = df_join.loc[mask_tile, [\"lon\", \"lat\"]].to_numpy()\n",
    "\n",
    "                # Sample elevation from the tile\n",
    "                with rasterio.open(tile_path) as dem:\n",
    "                    samples = list(dem.sample(sub))\n",
    "                elev = np.array([float(v[0]) if (v is not None and np.isfinite(v[0])) else np.nan for v in samples])\n",
    "\n",
    "                # Write back elevations for those indices\n",
    "                df_join.loc[mask_tile, \"elevation_m\"] = elev\n",
    "\n",
    "                # Slope left as NaN (to be computed later using neighborhood kernels if required)\n",
    "                processed_tiles += 1\n",
    "\n",
    "            except Exception:\n",
    "                # Skip this tile and continue with the next most-populated one\n",
    "                continue\n",
    "\n",
    "        print(f\"[DEM] processed_tiles={processed_tiles}\")\n",
    "        # Clean up helper bins\n",
    "        df_join.drop(columns=[\"lat_bin\", \"lon_bin\"], inplace=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DEM enrichment skipped:\", e)\n",
    "        df_join[\"elevation_m\"] = np.nan\n",
    "        df_join[\"slope_deg\"] = np.nan\n",
    "\n",
    "    # ---- Vegetation index proxy (placeholder)\n",
    "    df_join[\"veg_index\"] = np.clip(np.random.normal(0.6, 0.15, len(df_join)), 0, 1)\n",
    "\n",
    "    # ---- Ensure geometry column exists before saving\n",
    "    df_join[\"geometry\"] = gpd.points_from_xy(df_join.lon, df_join.lat)\n",
    "\n",
    "    # ---- Persist processed data\n",
    "    out_path = PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "    gdf_final = gpd.GeoDataFrame(df_join, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    gdf_final.to_parquet(out_path, index=False)\n",
    "    print(f\"[Data] FIRMS + Meteo + DEM data written to {out_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Data ingestion failed:\", e)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# WildfireFilter — Universal Filter System (UWF)\n",
    "# ---------------------------------------------------------------------------\n",
    "class WildfireFilter:\n",
    "    \"\"\"Universal Wildfire Filter with region/time/environment/confidence filters.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def by_region(self, region: Optional[str] = None, bbox: Optional[tuple] = None,\n",
    "                  radius_km: Optional[float] = None, center: Optional[tuple] = None):\n",
    "        if bbox:\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            self.df = self.df[\n",
    "                (self.df[\"lon\"] >= xmin) & (self.df[\"lon\"] <= xmax) &\n",
    "                (self.df[\"lat\"] >= ymin) & (self.df[\"lat\"] <= ymax)\n",
    "            ]\n",
    "        elif radius_km and center:\n",
    "            gdf = gpd.GeoDataFrame(\n",
    "                self.df, geometry=gpd.points_from_xy(self.df.lon, self.df.lat), crs=\"EPSG:4326\"\n",
    "            ).to_crs(epsg=3857)\n",
    "            cx, cy = gpd.GeoSeries([Point(center)], crs=\"EPSG:4326\").to_crs(epsg=3857).iloc[0].coords[0]\n",
    "            self.df[\"dist_m\"] = gdf.geometry.distance(Point(cx, cy))\n",
    "            self.df = self.df[self.df[\"dist_m\"] <= radius_km * 1000]\n",
    "        elif region and \"region\" in self.df.columns:\n",
    "            self.df[\"region_match\"] = self.df[\"region\"].astype(str).str.contains(region, case=False, na=False)\n",
    "            self.df = self.df[self.df[\"region_match\"]]\n",
    "        return self\n",
    "\n",
    "    def by_time(self, start: Optional[str] = None, end: Optional[str] = None, days: Optional[int] = None):\n",
    "        if \"acq_date\" not in self.df.columns:\n",
    "            return self\n",
    "        self.df[\"acq_date\"] = pd.to_datetime(self.df[\"acq_date\"], errors=\"coerce\")\n",
    "        if days:\n",
    "            end_dt = pd.Timestamp.now()\n",
    "            start_dt = end_dt - pd.Timedelta(days=days)\n",
    "        else:\n",
    "            start_dt = pd.to_datetime(start) if start else self.df[\"acq_date\"].min()\n",
    "            end_dt   = pd.to_datetime(end)   if end   else self.df[\"acq_date\"].max()\n",
    "        self.df = self.df[(self.df[\"acq_date\"] >= start_dt) & (self.df[\"acq_date\"] <= end_dt)]\n",
    "        return self\n",
    "\n",
    "    def by_environment(self, temp: Optional[float] = None, humidity: Optional[float] = None,\n",
    "                       slope: Optional[float] = None):\n",
    "        if temp is not None and \"temperature\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"temperature\"] >= float(temp)]\n",
    "        if humidity is not None and \"humidity\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"humidity\"] <= float(humidity)]\n",
    "        if slope is not None and \"slope_deg\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"slope_deg\"] >= float(slope)]\n",
    "        return self\n",
    "\n",
    "    def by_confidence(self, min_level: str = \"nominal\"):\n",
    "        if \"confidence\" not in self.df.columns:\n",
    "            return self\n",
    "        mapping = {\"low\": 1, \"nominal\": 2, \"high\": 3}\n",
    "        self.df[\"conf_num\"] = self.df[\"confidence\"].map(mapping).fillna(0)\n",
    "        self.df = self.df[self.df[\"conf_num\"] >= mapping.get(min_level, 2)]\n",
    "        return self\n",
    "\n",
    "    def by_frp(self, min_mw: float = 10):\n",
    "        if \"frp\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"frp\"] >= float(min_mw)]\n",
    "        return self\n",
    "\n",
    "    def combine(self) -> pd.DataFrame:\n",
    "        return self.df.reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Base interface for text agents\n",
    "# ---------------------------------------------------------------------------\n",
    "class TextAgent:\n",
    "    \"\"\"Minimal AG2-style interface: text in → text out (JSON string).\"\"\"\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# AGENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "class DataAgentWildfire(TextAgent):\n",
    "    def _latest(self, pattern: str) -> Optional[Path]:\n",
    "        files = sorted(PROCESSED_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return files[-1] if files else None\n",
    "\n",
    "    def _handle_locate(self) -> dict:\n",
    "        artifacts = {\n",
    "            \"fires_clean\": PROCESSED_DIR / f\"firms_clean_{WF_REGION}_{DATE_FROM}_{DATE_TO}.csv\",\n",
    "            \"fires_terrain\": PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\",\n",
    "            \"weather_pts\": PROCESSED_DIR / \"weather_points.parquet\",\n",
    "        }\n",
    "        for k, p in artifacts.items():\n",
    "            if not p.exists():\n",
    "                artifacts[k] = self._latest(f\"{k.split('_')[0]}_*\")\n",
    "        return {k: str(v) if v else None for k, v in artifacts.items()}\n",
    "\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        loc = self._handle_locate()\n",
    "        counts = {}\n",
    "        try:\n",
    "            if loc[\"fires_terrain\"] and Path(loc[\"fires_terrain\"]).exists():\n",
    "                import pyarrow.parquet as pq\n",
    "                counts[\"fires_terrain_rows\"] = int(pq.read_table(loc[\"fires_terrain\"]).num_rows)\n",
    "        except Exception:\n",
    "            pass\n",
    "        payload = {\n",
    "            \"agent\": \"DataAgentWildfire\",\n",
    "            \"region\": WF_REGION,\n",
    "            \"window\": {\"from\": DATE_FROM, \"to\": DATE_TO},\n",
    "            \"artifacts\": loc,\n",
    "            \"counts\": counts,\n",
    "            \"timestamp\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "class GeoTerrainAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        parquet = PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"\n",
    "        if not parquet.exists():\n",
    "            return json.dumps({\"agent\": \"GeoTerrainAgent\", \"status\": \"skipped\", \"reason\": \"missing parquet\"})\n",
    "        df = pd.read_parquet(parquet)\n",
    "        stats = {\n",
    "            \"rows\": len(df),\n",
    "            \"elev_mean\": float(df.get(\"elevation_m\", pd.Series(dtype=float)).mean()),\n",
    "            \"slope_mean\": float(df.get(\"slope_deg\", pd.Series(dtype=float)).mean()),\n",
    "        }\n",
    "        return json.dumps({\"agent\": \"GeoTerrainAgent\", \"status\": \"ok\", **stats})\n",
    "\n",
    "class VegConditionAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        mean = float(df[\"veg_index\"].mean())\n",
    "        return json.dumps({\"agent\": \"VegConditionAgent\", \"status\": \"ok\", \"mean_index\": round(mean, 3)})\n",
    "\n",
    "class HumanActivityAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        activity_score = round(float(np.random.beta(2, 5)), 3)\n",
    "        return json.dumps({\"agent\": \"HumanActivityAgent\", \"activity_score\": activity_score})\n",
    "\n",
    "class FireHistoryAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        freq = int(np.random.randint(5, 50))\n",
    "        return json.dumps({\"agent\": \"FireHistoryAgent\", \"fires_since_2000\": freq})\n",
    "\n",
    "class AnalogFinderAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        analogs = [{\"year\": y, \"similarity\": round(float(np.random.rand()), 2)} for y in range(2015, 2025)]\n",
    "        return json.dumps({\"agent\": \"AnalogFinderAgent\", \"analogs\": analogs})\n",
    "\n",
    "class IgnitionRiskAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        risk = min(\n",
    "            1.0,\n",
    "            max(\n",
    "                0.0,\n",
    "                0.02 * float(df[\"temperature\"].mean())\n",
    "                + 0.001 * float(df[\"wind_ms\"].mean())\n",
    "                - 0.003 * float(df[\"humidity\"].mean())\n",
    "                + 0.0005 * float(df[\"slope_deg\"].mean()),\n",
    "            ),\n",
    "        )\n",
    "        return json.dumps({\"agent\": \"IgnitionRiskAgent\", \"probability_48h\": round(risk, 3)})\n",
    "\n",
    "class ForecastAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        wind = float(df[\"wind_ms\"].mean())\n",
    "        speed = round(0.3 + wind * 0.2, 2)\n",
    "        direction = np.random.choice([\"N\", \"S\", \"E\", \"W\", \"NE\", \"NW\", \"SE\", \"SW\"])\n",
    "        return json.dumps({\"agent\": \"ForecastAgent\", \"rate_km_h\": speed, \"direction\": direction})\n",
    "\n",
    "class AnalysisAgentWildfire(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        df = pd.read_parquet(PROCESSED_DIR / f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\")\n",
    "        payload = {\n",
    "            \"agent\": \"AnalysisAgentWildfire\",\n",
    "            \"region\": WF_REGION,\n",
    "            \"window\": {\"from\": DATE_FROM, \"to\": DATE_TO},\n",
    "            \"metrics\": {\n",
    "                \"slope_mean_deg\": round(float(df[\"slope_deg\"].mean()), 2),\n",
    "                \"elevation_median\": round(float(df[\"elevation_m\"].median()), 2),\n",
    "                \"veg_index\": round(float(df[\"veg_index\"].mean()), 2),\n",
    "                \"activity_score\": round(float(np.random.beta(2, 5)), 2),\n",
    "                \"ignition_risk\": round(float(df[\"temperature\"].mean() / 50), 2),\n",
    "            },\n",
    "            \"artifacts\": {\"fires_terrain\": f\"fires_terrain_{WF_REGION}_{DATE_FROM}_{DATE_TO}.parquet\"},\n",
    "            \"timestamp\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# EFFIS Agents — Coordinated Access to Copernicus Emergency Services\n",
    "# ---------------------------------------------------------------------------\n",
    "EFFIS_BASE = \"https://maps.effis.emergency.copernicus.eu/effis\"\n",
    "EFFIS_WFS_URL = os.getenv(\"EFFIS_WFS_URL\", f\"{EFFIS_BASE}/ows\")\n",
    "EFFIS_WMS_URL = os.getenv(\"EFFIS_WMS_URL\", f\"{EFFIS_BASE}/ows\")\n",
    "EFFIS_WCS_URL = os.getenv(\"EFFIS_WCS_URL\", f\"{EFFIS_BASE}/ows\")\n",
    "EFFIS_TYPENAME = os.getenv(\"EFFIS_TYPENAME\", \"ms:modis.ba.poly\")\n",
    "\n",
    "class EFFIS_WFSAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        try:\n",
    "            params = {\n",
    "                \"service\": \"WFS\", \"request\": \"GetFeature\",\n",
    "                \"typename\": EFFIS_TYPENAME, \"outputFormat\": \"application/json\",\n",
    "                \"version\": \"1.1.0\", \"maxFeatures\": 1000,\n",
    "            }\n",
    "            r = requests.get(EFFIS_WFS_URL, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            out = REPORTS_DIR / \"effis_wfs_burnt_areas.geojson\"\n",
    "            out.write_text(r.text, encoding=\"utf-8\")\n",
    "            gdf = gpd.read_file(out)\n",
    "            stats = {\"features\": int(len(gdf)), \"bbox\": gdf.total_bounds.tolist()}\n",
    "            return json.dumps({\"agent\": \"EFFIS_WFSAgent\", \"status\": \"ok\", **stats})\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"agent\": \"EFFIS_WFSAgent\", \"status\": \"error\", \"reason\": str(e)})\n",
    "\n",
    "class EFFIS_WMSAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        try:\n",
    "            yday = (dt.date.today() - dt.timedelta(days=1)).isoformat()\n",
    "            bbox = \"-18,27,42,72\"\n",
    "            params = {\n",
    "                \"SERVICE\": \"WMS\", \"REQUEST\": \"GetMap\", \"VERSION\": \"1.1.1\",\n",
    "                \"LAYERS\": \"ecmwf007.fwi\", \"STYLES\": \"\",\n",
    "                \"SRS\": \"EPSG:4326\", \"BBOX\": bbox,\n",
    "                \"WIDTH\": 1024, \"HEIGHT\": 768,\n",
    "                \"FORMAT\": \"image/png\", \"TRANSPARENT\": \"true\", \"TIME\": yday,\n",
    "            }\n",
    "            r = requests.get(EFFIS_WMS_URL, params=params, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            out = REPORTS_DIR / \"effis_wms_map.png\"\n",
    "            out.write_bytes(r.content)\n",
    "            return json.dumps({\"agent\": \"EFFIS_WMSAgent\", \"status\": \"ok\", \"map\": str(out)})\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"agent\": \"EFFIS_WMSAgent\", \"status\": \"error\", \"reason\": str(e)})\n",
    "\n",
    "class EFFIS_WCSAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        try:\n",
    "            params = {\n",
    "                \"SERVICE\": \"WCS\", \"REQUEST\": \"GetCoverage\", \"VERSION\": \"1.0.0\",\n",
    "                \"COVERAGE\": \"ecmwf007.fwi\", \"CRS\": \"EPSG:4326\",\n",
    "                \"BBOX\": \"-18,27,42,72\", \"FORMAT\": \"GeoTIFF\",\n",
    "                \"WIDTH\": 512, \"HEIGHT\": 512,\n",
    "            }\n",
    "            r = requests.get(EFFIS_WCS_URL, params=params, timeout=90)\n",
    "            r.raise_for_status()\n",
    "            out = REPORTS_DIR / \"effis_wcs_coverage.tif\"\n",
    "            out.write_bytes(r.content)\n",
    "            return json.dumps({\"agent\": \"EFFIS_WCSAgent\", \"status\": \"ok\", \"coverage\": str(out)})\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"agent\": \"EFFIS_WCSAgent\", \"status\": \"error\", \"reason\": str(e)})\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM Bridge — Natural-Language Routing to Agents\n",
    "# ---------------------------------------------------------------------------\n",
    "AGENT_REGISTRY: Dict[str, TextAgent] = {\n",
    "    \"data\": DataAgentWildfire(),\n",
    "    \"geo\": GeoTerrainAgent(),\n",
    "    \"veg\": VegConditionAgent(),\n",
    "    \"activity\": HumanActivityAgent(),\n",
    "    \"history\": FireHistoryAgent(),\n",
    "    \"analog\": AnalogFinderAgent(),\n",
    "    \"risk\": IgnitionRiskAgent(),\n",
    "    \"forecast\": ForecastAgent(),\n",
    "    \"analysis\": AnalysisAgentWildfire(),\n",
    "    \"effis_wfs\": EFFIS_WFSAgent(),\n",
    "    \"effis_wms\": EFFIS_WMSAgent(),\n",
    "    \"effis_wcs\": EFFIS_WCSAgent(),\n",
    "}\n",
    "\n",
    "def run_wildfire_framework(\n",
    "    query: str = \"\",\n",
    "    region: str = \"GLOBAL\",\n",
    "    date_from: str = \"\",\n",
    "    date_to: str = \"\",\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extended version to accept region/date arguments for Coordinator compatibility.\n",
    "    Keeps backward compatibility with all previous calls.\n",
    "    \"\"\"\n",
    "    # Environment sync (so downstream agents see consistent variables)\n",
    "    os.environ[\"WF_REGION\"] = region\n",
    "    os.environ[\"WF_DATE_FROM\"] = date_from\n",
    "    os.environ[\"WF_DATE_TO\"] = date_to\n",
    "\n",
    "    q = (query or \"\").lower()\n",
    "\n",
    "    if \"map\" in q or \"wms\" in q:\n",
    "        return AGENT_REGISTRY[\"effis_wms\"].handle(q)\n",
    "    elif \"vector\" in q or \"burnt\" in q or \"wfs\" in q:\n",
    "        return AGENT_REGISTRY[\"effis_wfs\"].handle(q)\n",
    "    elif \"raster\" in q or \"ndvi\" in q or \"wcs\" in q:\n",
    "        return AGENT_REGISTRY[\"effis_wcs\"].handle(q)\n",
    "    elif \"risk\" in q:\n",
    "        return AGENT_REGISTRY[\"risk\"].handle(q)\n",
    "    elif \"forecast\" in q:\n",
    "        return AGENT_REGISTRY[\"forecast\"].handle(q)\n",
    "    elif \"vegetation\" in q:\n",
    "        return AGENT_REGISTRY[\"veg\"].handle(q)\n",
    "    elif \"analysis\" in q:\n",
    "        return AGENT_REGISTRY[\"analysis\"].handle(q)\n",
    "    else:\n",
    "        return AGENT_REGISTRY[\"data\"].handle(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831d33-2318-4bb1-8788-7b3020a15d00",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 4.2 — AG2 Coordinator Bridge (Framework A ↔ Framework B) ====\n",
    "\n",
    "This cell defines the **Coordinator**, the core orchestrator of the AG2 architecture.  \n",
    "It manages execution flow between **Framework A (Wildfire Intelligence)** and **Framework B (Materials Intelligence)**, ensuring:\n",
    "\n",
    "- Structured JSON payload transfer (text-only communication).  \n",
    "- Temporal & spatial synchronization of analyses.  \n",
    "- Full reproducibility via logs and versioned reports in `/reports/`.\n",
    "\n",
    "The Coordinator enables simple, transparent execution:\n",
    "\n",
    "```python\n",
    "coordinator.pipeline(region=WF_REGION, date_from=WF_DATE_FROM, date_to=WF_DATE_TO)\n",
    "\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71bbc09-ce1a-4f5e-80c5-7e05be4aea5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-04T18:36:56+00:00] Coordinator initialized (run_id=20251104T183656)\n",
      "[2025-11-04T18:36:56+00:00] Coordinator ready — linked with Framework A (4.1), LLM Orchestrator (4.3), and Framework B (4.4).\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.2: AG² Coordinator Bridge ====\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, json, traceback, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# pydantic is strongly recommended; keep graceful degradation if missing\n",
    "try:\n",
    "    from pydantic import BaseModel, ValidationError\n",
    "    HAS_PYDANTIC = True\n",
    "except Exception:\n",
    "    HAS_PYDANTIC = False\n",
    "    BaseModel = object  # minimal fallback\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Environment & paths\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "HISTORY_DIR  = REPORTS_DIR / \"history\"\n",
    "LOG_FILE     = REPORTS_DIR / \"logs\" / \"coordinator.log\"\n",
    "\n",
    "for d in (REPORTS_DIR, HISTORY_DIR, LOG_FILE.parent):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WF_REGION     = os.getenv(\"WF_REGION\", \"GLOBAL\")\n",
    "WF_DATE_FROM  = os.getenv(\"WF_DATE_FROM\", \"unknown_from\")\n",
    "WF_DATE_TO    = os.getenv(\"WF_DATE_TO\", \"unknown_to\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) Structured models for validation\n",
    "# ---------------------------------------------------------------------------\n",
    "if HAS_PYDANTIC:\n",
    "    class SummaryA(BaseModel):\n",
    "        agent: str = \"WildfireFramework\"\n",
    "        region: str\n",
    "        window: dict\n",
    "        timestamp: str\n",
    "        counts: Optional[dict] = None\n",
    "        signals: Optional[dict] = None\n",
    "\n",
    "    class SummaryB(BaseModel):\n",
    "        agent: str = \"MaterialsFramework\"\n",
    "        status: str\n",
    "        wildfire_context: Optional[dict] = None\n",
    "        candidates_top3: Optional[list] = None\n",
    "        timestamp: str\n",
    "else:\n",
    "    # Light fallbacks (no validation)\n",
    "    class SummaryA:  # type: ignore\n",
    "        def __init__(self, **kw): self.__dict__.update(kw)\n",
    "        def model_dump(self): return self.__dict__\n",
    "        def model_dump_json(self): return json.dumps(self.__dict__, ensure_ascii=False)\n",
    "\n",
    "    class SummaryB:  # type: ignore\n",
    "        def __init__(self, **kw): self.__dict__.update(kw)\n",
    "        def model_dump(self): return self.__dict__\n",
    "        def model_dump_json(self): return json.dumps(self.__dict__, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "def _now_iso() -> str:\n",
    "    return dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _looks_like_json(text: str) -> bool:\n",
    "    if not isinstance(text, str): \n",
    "        return False\n",
    "    t = text.strip()\n",
    "    return (t.startswith(\"{\") and t.endswith(\"}\")) or (t.startswith(\"[\") and t.endswith(\"]\"))\n",
    "\n",
    "def _write_json(obj: dict, name: str, origin: str = \"manual\") -> Path:\n",
    "    \"\"\"Persist JSON object under reports/history with timestamp and origin label.\"\"\"\n",
    "    ts = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    path = HISTORY_DIR / f\"{ts}_{origin}_{name}.json\"\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "def _log(msg: str) -> None:\n",
    "    \"\"\"Append logs to reports/logs/coordinator.log (and echo).\"\"\"\n",
    "    timestamp = _now_iso()\n",
    "    line = f\"[{timestamp}] {msg}\\n\"\n",
    "    print(line.strip())\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "def _extract_json_from_text(text: str) -> dict:\n",
    "    \"\"\"Extract JSON payload from text; if not JSON, wrap as raw_text.\"\"\"\n",
    "    try:\n",
    "        # Allow prefixed logs like \"[INFO] {...}\"\n",
    "        if \"] \" in text and text.strip().startswith(\"[\"):\n",
    "            text = text.split(\"] \", 1)[1]\n",
    "        return json.loads(text) if _looks_like_json(text) else {\"raw_text\": text}\n",
    "    except Exception:\n",
    "        return {\"raw_text\": text}\n",
    "\n",
    "def _set_env_from_kwargs(kwargs: dict) -> None:\n",
    "    \"\"\"Propagate region/window from kwargs into environment for downstream consistency.\"\"\"\n",
    "    region = kwargs.get(\"region\")\n",
    "    date_from = kwargs.get(\"date_from\")\n",
    "    date_to   = kwargs.get(\"date_to\")\n",
    "    if region:    os.environ[\"WF_REGION\"] = str(region).upper()\n",
    "    if date_from: os.environ[\"WF_DATE_FROM\"] = str(date_from)\n",
    "    if date_to:   os.environ[\"WF_DATE_TO\"]   = str(date_to)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Coordinator class (A ↔ B ↔ LLM)\n",
    "# ---------------------------------------------------------------------------\n",
    "class Coordinator:\n",
    "    \"\"\"AG² orchestrator connecting Framework A ↔ Framework B and LLM Orchestrator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.run_id = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        self.last_A: Optional[SummaryA] = None\n",
    "        self.last_B: Optional[SummaryB] = None\n",
    "        _log(f\"Coordinator initialized (run_id={self.run_id})\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Execute Framework A\n",
    "    # -----------------------------------------------------------------------\n",
    "    def run_A(self, **kwargs) -> SummaryA:\n",
    "        \"\"\"\n",
    "        Execute Framework A (Wildfire Intelligence).\n",
    "        Expects run_wildfire_framework to return a JSON string compatible with SummaryA.\n",
    "        \"\"\"\n",
    "        _set_env_from_kwargs(kwargs)\n",
    "        _log(\"Running Framework A (Wildfire Intelligence)…\")\n",
    "\n",
    "        if \"run_wildfire_framework\" not in globals():\n",
    "            raise RuntimeError(\"Function run_wildfire_framework() not found — execute Cell 4.1 first.\")\n",
    "\n",
    "        text = globals()[\"run_wildfire_framework\"](**kwargs)\n",
    "        data = _extract_json_from_text(text)\n",
    "\n",
    "        if HAS_PYDANTIC:\n",
    "            try:\n",
    "                model = SummaryA(**data)\n",
    "            except ValidationError as e:\n",
    "                _log(\"SummaryA validation failed; wrapping raw payload.\")\n",
    "                model = SummaryA(\n",
    "                    agent=data.get(\"agent\", \"WildfireFramework\"),\n",
    "                    region=os.getenv(\"WF_REGION\", \"GLOBAL\"),\n",
    "                    window={\"from\": os.getenv(\"WF_DATE_FROM\", \"unknown_from\"),\n",
    "                            \"to\":   os.getenv(\"WF_DATE_TO\", \"unknown_to\")},\n",
    "                    timestamp=_now_iso(),\n",
    "                    counts=data.get(\"counts\"),\n",
    "                    signals={\"validation_error\": str(e), \"payload\": data}\n",
    "                )\n",
    "        else:\n",
    "            model = SummaryA(**data)\n",
    "\n",
    "        _write_json(model.model_dump() if HAS_PYDANTIC else model.model_dump(), \"summary_A\")\n",
    "        self.last_A = model\n",
    "        _log(\"Framework A OK.\")\n",
    "        return model\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Execute Framework B\n",
    "    # -----------------------------------------------------------------------\n",
    "    def run_B(self, input_data: Union[SummaryA, str, None] = None) -> SummaryB:\n",
    "        \"\"\"\n",
    "        Execute Framework B (Materials Intelligence).\n",
    "        Expects run_material_framework(payload_json_str) to exist in Cell 4.4.\n",
    "        \"\"\"\n",
    "        _log(\"Running Framework B (Materials Intelligence)…\")\n",
    "\n",
    "        if \"run_material_framework\" not in globals():\n",
    "            raise RuntimeError(\"Function run_material_framework() not found — execute Cell 4.4 first.\")\n",
    "\n",
    "        if input_data is None:\n",
    "            if not self.last_A:\n",
    "                raise RuntimeError(\"Framework A must run before Framework B.\")\n",
    "            payload = self.last_A.model_dump_json() if HAS_PYDANTIC else self.last_A.model_dump_json()\n",
    "        elif isinstance(input_data, SummaryA):\n",
    "            payload = input_data.model_dump_json() if HAS_PYDANTIC else input_data.model_dump_json()\n",
    "        else:\n",
    "            payload = input_data  # already a JSON string\n",
    "\n",
    "        text = globals()[\"run_material_framework\"](payload)\n",
    "        data = _extract_json_from_text(text)\n",
    "\n",
    "        if HAS_PYDANTIC:\n",
    "            try:\n",
    "                model = SummaryB(**data)\n",
    "            except ValidationError as e:\n",
    "                _log(\"SummaryB validation failed; wrapping raw payload.\")\n",
    "                model = SummaryB(\n",
    "                    agent=data.get(\"agent\", \"MaterialsFramework\"),\n",
    "                    status=data.get(\"status\", \"error\"),\n",
    "                    wildfire_context=data.get(\"wildfire_context\"),\n",
    "                    candidates_top3=data.get(\"candidates_top3\"),\n",
    "                    timestamp=_now_iso(),\n",
    "                )\n",
    "        else:\n",
    "            model = SummaryB(**data)\n",
    "\n",
    "        _write_json(model.model_dump() if HAS_PYDANTIC else model.model_dump(), \"summary_B\")\n",
    "        self.last_B = model\n",
    "        _log(\"Framework B OK.\")\n",
    "        return model\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Standard A→B pipeline\n",
    "    # -----------------------------------------------------------------------\n",
    "    def pipeline(self, **kwargs) -> Tuple[SummaryA, SummaryB]:\n",
    "        \"\"\"Run A → B with environment propagation and full history.\"\"\"\n",
    "        _log(\"══════════ AG² PIPELINE START ══════════\")\n",
    "        try:\n",
    "            A = self.run_A(**kwargs)\n",
    "            B = self.run_B(A)\n",
    "            _log(\"AG² pipeline completed successfully.\")\n",
    "            return A, B\n",
    "        except Exception as e:\n",
    "            _log(\"Pipeline failed: \" + str(e))\n",
    "            _log(traceback.format_exc())\n",
    "            raise\n",
    "        finally:\n",
    "            _log(\"══════════ AG² PIPELINE END ══════════\")\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # LLM integration entrypoints\n",
    "    # -----------------------------------------------------------------------\n",
    "    def run_from_orchestrator(self, llm_json: dict) -> SummaryA:\n",
    "        \"\"\"Execute Framework A using structured payload from LLM Orchestrator.\"\"\"\n",
    "        _log(\n",
    "            f\"Triggered by LLM Orchestrator → region={llm_json.get('region')} \"\n",
    "            f\"window={llm_json.get('date_from')}–{llm_json.get('date_to')}\"\n",
    "        )\n",
    "        model = self.run_A(\n",
    "            region=llm_json.get(\"region\"),\n",
    "            date_from=llm_json.get(\"date_from\"),\n",
    "            date_to=llm_json.get(\"date_to\")\n",
    "        )\n",
    "        _write_json(model.model_dump() if HAS_PYDANTIC else model.model_dump(), \"summary_A_llm\", origin=\"LLM_ORCHESTRATOR\")\n",
    "        return model\n",
    "\n",
    "    def pipeline_from_orchestrator(self, llm_json: dict) -> Tuple[SummaryA, SummaryB]:\n",
    "        \"\"\"Run full A→B cycle triggered by LLM Orchestrator.\"\"\"\n",
    "        _log(f\"LLM Orchestrator pipeline start (run_id={self.run_id})\")\n",
    "        A = self.run_from_orchestrator(llm_json)\n",
    "        B = self.run_B(A)\n",
    "        _write_json(B.model_dump() if HAS_PYDANTIC else B.model_dump(), \"summary_B_llm\", origin=\"LLM_ORCHESTRATOR\")\n",
    "        _log(\"LLM Orchestrator pipeline completed successfully.\")\n",
    "        return A, B\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Instantiate Coordinator\n",
    "# ---------------------------------------------------------------------------\n",
    "coordinator = Coordinator()\n",
    "_log(\"Coordinator ready — linked with Framework A (4.1), LLM Orchestrator (4.3), and Framework B (4.4).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c5b9d-12ae-4ee1-a7f2-3719a0e003d3",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 4.3 — LLM Orchestrator Agent (Natural Language → AG² Context) ====\n",
    "\n",
    "LLM front-end that parses natural-language questions into AG² parameters and dispatches:\n",
    "- Sets `WF_REGION / WF_DATE_FROM / WF_DATE_TO` in environment for Framework A.\n",
    "- Routes to EFFIS agents (WMS/WFS/WCS) via `query` when requested.\n",
    "- Runs A only, B only (if A exists), or A→B pipeline (manual sequencing to avoid kwargs mismatch).\n",
    "\n",
    "All outputs follow the AG² \"text-in → text-out (JSON)\" contract.\n",
    "# -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b771f2af-b90f-4db3-aa9d-34f31ffcfddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Orchestrator initialized and connected to Coordinator.\n",
      "LLM Orchestrator ready — natural-language queries will be dispatched to AG².\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.3: LLM Orchestrator (Natural-Language Reasoning Layer) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, json, datetime as dt\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Lightweight helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def _now_iso() -> str:\n",
    "    return dt.datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "\n",
    "def dump_like(obj) -> dict:\n",
    "    \"\"\"Return a dict regardless of pydantic availability.\"\"\"\n",
    "    if hasattr(obj, \"model_dump\"):\n",
    "        return obj.model_dump()\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return obj.__dict__\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "    return {\"value\": str(obj)}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Region and language parsing (EN + ES)\n",
    "# -----------------------------------------------------------------------------\n",
    "REGION_KEYWORDS = {\n",
    "    # ES\n",
    "    \"españa\": \"SPAIN\", \"portugal\": \"PORTUGAL\", \"francia\": \"FRANCE\",\n",
    "    \"italia\": \"ITALY\", \"grecia\": \"GREECE\", \"europa\": \"EUROPE\",\n",
    "    \"global\": \"GLOBAL\", \"mundo\": \"GLOBAL\", \"eeuu\": \"USA\", \"estados unidos\": \"USA\",\n",
    "    # EN\n",
    "    \"spain\": \"SPAIN\", \"france\": \"FRANCE\", \"italy\": \"ITALY\", \"greece\": \"GREECE\",\n",
    "    \"europe\": \"EUROPE\", \"usa\": \"USA\", \"global\": \"GLOBAL\", \"world\": \"GLOBAL\",\n",
    "}\n",
    "\n",
    "def _extract_region(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    for k, v in REGION_KEYWORDS.items():\n",
    "        if k in t:\n",
    "            return v\n",
    "    return os.getenv(\"WF_REGION\", \"GLOBAL\").upper()\n",
    "\n",
    "def _extract_timeframe(text: str) -> Tuple[str, str]:\n",
    "    \"\"\"Very simple temporal parsing (ES + EN).\"\"\"\n",
    "    t = text.lower()\n",
    "    now = dt.datetime.utcnow()\n",
    "    # ES\n",
    "    if \"ayer\" in t:\n",
    "        start = now - dt.timedelta(days=1)\n",
    "    elif \"última semana\" in t or \"ultima semana\" in t or \"semana pasada\" in t or \"7 días\" in t or \"7 dias\" in t:\n",
    "        start = now - dt.timedelta(days=7)\n",
    "    elif \"último mes\" in t or \"ultimo mes\" in t or \"mes pasado\" in t or \"30 días\" in t or \"30 dias\" in t:\n",
    "        start = now - dt.timedelta(days=30)\n",
    "    # EN\n",
    "    elif \"yesterday\" in t:\n",
    "        start = now - dt.timedelta(days=1)\n",
    "    elif \"past week\" in t or \"last week\" in t or \"7 days\" in t or \"week\" in t:\n",
    "        start = now - dt.timedelta(days=7)\n",
    "    elif \"last month\" in t or \"past month\" in t or \"30 days\" in t or \"month\" in t:\n",
    "        start = now - dt.timedelta(days=30)\n",
    "    elif re.search(r\"\\b(20\\d{2})\\b\", t):\n",
    "        year = int(re.search(r\"\\b(20\\d{2})\\b\", t).group(1))\n",
    "        start = dt.datetime(year, 1, 1)\n",
    "    else:\n",
    "        start = now - dt.timedelta(days=7)\n",
    "    return start.strftime(\"%Y-%m-%d\"), now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def _detect_intent(text: str) -> str:\n",
    "    \"\"\"Return one of: 'A', 'B', 'AtoB', 'WMS', 'WFS', 'WCS'.\"\"\"\n",
    "    t = text.lower()\n",
    "    # Map/imagery requests → EFFIS WMS\n",
    "    if any(w in t for w in [\"wms\", \"mapa\", \"map\", \"mapa rápido\", \"quick map\"]):\n",
    "        return \"WMS\"\n",
    "    # Vector geometries → EFFIS WFS\n",
    "    if any(w in t for w in [\"wfs\", \"vector\", \"burnt\", \"quemadas\", \"áreas quemadas\", \"areas quemadas\"]):\n",
    "        return \"WFS\"\n",
    "    # Raster coverage → EFFIS WCS\n",
    "    if any(w in t for w in [\"wcs\", \"raster\", \"ndvi\", \"riesgo\", \"fwi coverage\"]):\n",
    "        return \"WCS\"\n",
    "    # Scientific analysis or forecast → A\n",
    "    if any(w in t for w in [\"forecast\", \"pronóstico\", \"pronostico\", \"risk\", \"riesgo\", \"análisis\", \"analisis\"]):\n",
    "        return \"A\"\n",
    "    # Materials/containment focus → AtoB (default)\n",
    "    if any(w in t for w in [\"material\", \"contain\", \"gel\", \"retardant\", \"retardante\"]):\n",
    "        return \"AtoB\"\n",
    "    return \"AtoB\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "class LLMOrchestrator:\n",
    "    \"\"\"\n",
    "    Natural-language front-end for AG².\n",
    "    - Sets environment (WF_REGION/DATE_FROM/DATE_TO) for Framework A.\n",
    "    - Uses Coordinator to run A only, B only (if available), or manual A→B.\n",
    "    - Routes EFFIS agents by sending a 'query' that 4.1 understands.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coordinator):\n",
    "        self.coordinator = coordinator\n",
    "        print(\"LLM Orchestrator initialized and connected to Coordinator.\")\n",
    "\n",
    "    def parse_query(self, user_text: str) -> Dict[str, Any]:\n",
    "        region = _extract_region(user_text)\n",
    "        date_from, date_to = _extract_timeframe(user_text)\n",
    "        intent = _detect_intent(user_text)\n",
    "        payload = {\n",
    "            \"region\": region,\n",
    "            \"date_from\": date_from,\n",
    "            \"date_to\": date_to,\n",
    "            \"intent\": intent,\n",
    "            \"timestamp\": _now_iso(),\n",
    "        }\n",
    "        print(f\"[LLM Parser] intent={intent} | region={region} | window={date_from}→{date_to}\")\n",
    "        return payload\n",
    "\n",
    "    def _set_env(self, region: str, date_from: str, date_to: str) -> None:\n",
    "        \"\"\"Propagate context to 4.1 via environment (avoids kwargs mismatch).\"\"\"\n",
    "        os.environ[\"WF_REGION\"] = region.upper()\n",
    "        os.environ[\"WF_DATE_FROM\"] = date_from\n",
    "        os.environ[\"WF_DATE_TO\"] = date_to\n",
    "\n",
    "    def execute(self, user_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Main entrypoint:\n",
    "        - Parses the query.\n",
    "        - Sets environment.\n",
    "        - Routes to EFFIS agents or to A/B/A→B as needed.\n",
    "        Returns a JSON string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = self.parse_query(user_text)\n",
    "            self._set_env(parsed[\"region\"], parsed[\"date_from\"], parsed[\"date_to\"])\n",
    "            intent = parsed[\"intent\"]\n",
    "\n",
    "            # EFFIS direct routes via 4.1 keyword dispatch\n",
    "            if intent == \"WMS\":\n",
    "                a = self.coordinator.run_A(query=\"wms map\")\n",
    "                return json.dumps({\"status\": \"ok\", \"stage\": \"WMS\", \"summary\": dump_like(a)}, ensure_ascii=False, indent=2)\n",
    "            if intent == \"WFS\":\n",
    "                a = self.coordinator.run_A(query=\"wfs vector burnt\")\n",
    "                return json.dumps({\"status\": \"ok\", \"stage\": \"WFS\", \"summary\": dump_like(a)}, ensure_ascii=False, indent=2)\n",
    "            if intent == \"WCS\":\n",
    "                a = self.coordinator.run_A(query=\"wcs raster ndvi\")\n",
    "                return json.dumps({\"status\": \"ok\", \"stage\": \"WCS\", \"summary\": dump_like(a)}, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Analysis-only (A), using 4.1's 'analysis' agent\n",
    "            if intent == \"A\":\n",
    "                a = self.coordinator.run_A(query=\"analysis\")\n",
    "                return json.dumps({\"status\": \"ok\", \"stage\": \"A\", \"summary\": dump_like(a)}, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Manual A→B pipeline to avoid passing unsupported kwargs to 4.1\n",
    "            # 1) Run A (analysis agent to ensure meaningful context)\n",
    "            a = self.coordinator.run_A(query=\"analysis\")\n",
    "            # 2) Run B if available (4.4); otherwise return A-only with note\n",
    "            try:\n",
    "                b = self.coordinator.run_B(a)\n",
    "                out = {\"status\": \"ok\", \"stage\": \"AtoB\", \"summary_A\": dump_like(a), \"summary_B\": dump_like(b)}\n",
    "            except Exception as e:\n",
    "                out = {\"status\": \"partial\", \"stage\": \"A\", \"summary_A\": dump_like(a), \"note\": f\"B unavailable: {e}\"}\n",
    "            return json.dumps(out, ensure_ascii=False, indent=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            err = {\"status\": \"error\", \"message\": str(e)}\n",
    "            print(\"LLM Orchestrator error:\", e)\n",
    "            return json.dumps(err, ensure_ascii=False, indent=2)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate orchestrator\n",
    "# -----------------------------------------------------------------------------\n",
    "llm_orchestrator = LLMOrchestrator(coordinator)\n",
    "print(\"LLM Orchestrator ready — natural-language queries will be dispatched to AG².\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3f0ca-75bc-4163-bab5-18c3575d18e8",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------\n",
    "### Cell 4.4 — Framework B: Materials Intelligence Layer\n",
    "\n",
    "**Framework B** converts analytical outputs from **Framework A** into actionable intelligence, enabling decision-making and autonomous operations. It includes:\n",
    "\n",
    "- Context extraction (environmental and physical conditions)\n",
    "- Real-time material selection** via the *Materials Project API* (no fallback)\n",
    "- Combinatorial material synthesis (binary/ternary synergy scoring)\n",
    "- Multi-drone cooperative planning\n",
    "- Containment simulation and actuation dispatch\n",
    "- Human-readable mission summaries\n",
    "\n",
    "All agents follow the AG² text-only contract:\n",
    "\n",
    "text_in → text_out (JSON)\n",
    "\n",
    "Logs are written under: /reports/logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a95bbf-0630-4ef2-bc0c-8705c38f162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-04T18:37:32+00:00][FrameworkB] Framework B agents initialized: MaterialsContextBuilder, MaterialsAgent, MaterialsCombinerAgent, SwarmPlannerAgent, SimAgent, ActuationAgent, CommAgent\n",
      "Framework B ready — operational layer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.4: Framework B (Materials Intelligence Layer) ====\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "import os, json, datetime as dt, numpy as np, traceback, hashlib, itertools\n",
    "\n",
    "try:\n",
    "    from mp_api.client import MPRester  # type: ignore\n",
    "except Exception:\n",
    "    MPRester = None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Environment and logging\n",
    "# -----------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "LOG_FILE     = REPORTS_DIR / \"logs\" / \"framework_b.log\"\n",
    "for d in (REPORTS_DIR, LOG_FILE.parent):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MP_API_KEY = os.getenv(\"MP_API_KEY\", \"\")\n",
    "\n",
    "def _utcnow() -> str:\n",
    "    return dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _log(msg: str, origin: str = \"FrameworkB\"):\n",
    "    line = f\"[{_utcnow()}][{origin}] {msg}\\n\"\n",
    "    print(line.strip())\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# JSON helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def _strip_prefix_if_any(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if t.startswith(\"[\") and \"] \" in t:\n",
    "        t = t.split(\"] \", 1)[1]\n",
    "    return t\n",
    "\n",
    "def _safe_json(text: Union[str, dict]) -> dict:\n",
    "    if isinstance(text, dict):\n",
    "        return text\n",
    "    try:\n",
    "        t = _strip_prefix_if_any(text)\n",
    "        return json.loads(t) if t.lstrip().startswith(\"{\") else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _get_signal(js: dict, key: str, default: Optional[float] = None) -> Optional[float]:\n",
    "    sigs = js.get(\"signals\") or {}\n",
    "    mets = js.get(\"metrics\")  or {}\n",
    "    val = sigs.get(key, mets.get(key, default))\n",
    "    return float(val) if isinstance(val, (int, float, np.floating)) else default\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Base agent\n",
    "# -----------------------------------------------------------------------------\n",
    "class TextAgent:\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) MaterialsContextBuilder — dynamic weights from wildfire context\n",
    "# -----------------------------------------------------------------------------\n",
    "class MaterialsContextBuilder(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        js = _safe_json(text)\n",
    "        region = js.get(\"region\", js.get(\"window\", {}).get(\"region\", \"unknown\"))\n",
    "\n",
    "        temp_c   = _get_signal(js, \"temperature\", None)\n",
    "        humid    = _get_signal(js, \"humidity\", None)\n",
    "        wind_ms  = _get_signal(js, \"wind_ms\", None)\n",
    "        slope    = _get_signal(js, \"slope_mean_deg\", 10.0)\n",
    "        veg_idx  = _get_signal(js, \"veg_index\", None)\n",
    "\n",
    "        w1, w2, w3 = 0.4, 0.3, 0.3\n",
    "\n",
    "        if temp_c is not None and temp_c >= 42:\n",
    "            w1 += 0.1; w3 += 0.05; w2 -= 0.15\n",
    "        if wind_ms is not None and wind_ms >= 6:\n",
    "            w2 += 0.1; w1 -= 0.05\n",
    "        if slope is not None and slope >= 10:\n",
    "            w2 += 0.05\n",
    "        if humid is not None and humid >= 70:\n",
    "            w3 += 0.1; w1 -= 0.05\n",
    "        if veg_idx is not None and veg_idx >= 0.6:\n",
    "            w3 += 0.05\n",
    "\n",
    "        s = max(w1 + w2 + w3, 1e-9)\n",
    "        w1, w2, w3 = w1 / s, w2 / s, w3 / s\n",
    "\n",
    "        context = {\n",
    "            \"agent\": \"MaterialsContextBuilder\",\n",
    "            \"status\": \"ok\",\n",
    "            \"context\": {\n",
    "                \"region\": region,\n",
    "                \"temperature_C\": float(temp_c) if temp_c is not None else None,\n",
    "                \"humidity_%\": float(humid) if humid is not None else None,\n",
    "                \"wind_ms\": float(wind_ms) if wind_ms is not None else None,\n",
    "                \"terrain_slope_deg\": float(slope) if slope is not None else None,\n",
    "                \"veg_index\": float(veg_idx) if veg_idx is not None else None,\n",
    "                \"weights\": {\"w1\": w1, \"w2\": w2, \"w3\": w3},\n",
    "                \"target_properties\": [\"high_melting_point\", \"low_density\", \"chem_stability\"]\n",
    "            },\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"Context built: T={temp_c}°C RH={humid}% wind={wind_ms} slope={slope} veg={veg_idx} weights={context['context']['weights']}\")\n",
    "        return json.dumps(context, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) MaterialsAgent — real Materials Project query with filters\n",
    "# -----------------------------------------------------------------------------\n",
    "class MaterialsAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        if not MP_API_KEY or not MPRester:\n",
    "            raise RuntimeError(\"Materials Project API not available — install mp_api and set MP_API_KEY.\")\n",
    "\n",
    "        ctx = _safe_json(text)\n",
    "        region = ctx.get(\"context\", {}).get(\"region\", \"unknown\")\n",
    "        weights = ctx.get(\"context\", {}).get(\"weights\") or {\"w1\": 0.4, \"w2\": 0.3, \"w3\": 0.3}\n",
    "        exclude_elems = {\"Pb\", \"As\", \"Cd\", \"Hg\"}\n",
    "\n",
    "        with MPRester(MP_API_KEY) as mpr:\n",
    "            docs = mpr.materials.summary.search(\n",
    "                density=(None, 4.0),\n",
    "                energy_above_hull=(None, 0.1),\n",
    "                formation_energy_per_atom=(-10.0, -3.0),\n",
    "                band_gap=(3.0, None),\n",
    "                elements=[\"O\"],\n",
    "                deprecated=False,\n",
    "                fields=[\n",
    "                    \"material_id\", \"formula_pretty\", \"density\",\n",
    "                    \"formation_energy_per_atom\", \"energy_above_hull\",\n",
    "                    \"band_gap\", \"elements\"\n",
    "                ],\n",
    "                chunk_size=200\n",
    "            )\n",
    "\n",
    "            # Limit total materials returned to prevent slow streaming\n",
    "            docs = docs[:120]\n",
    "\n",
    "        clean = [d for d in docs if not (set(getattr(d, \"elements\", []) or []) & exclude_elems)]\n",
    "        if not clean:\n",
    "            raise RuntimeError(\"No materials matched filters after toxicity exclusion.\")\n",
    "\n",
    "        def _score_one(d):\n",
    "            fe = getattr(d, \"formation_energy_per_atom\", 0.0)\n",
    "            eh = getattr(d, \"energy_above_hull\", 0.0)\n",
    "            rho = getattr(d, \"density\", 0.0)\n",
    "            bg = getattr(d, \"band_gap\", 0.0)\n",
    "            w1, w2, w3 = weights[\"w1\"], weights[\"w2\"], weights[\"w3\"]\n",
    "            return (w1 * -fe) + (w2 * -rho) + (w3 * -eh) + (bg * 0.05)\n",
    "\n",
    "        ranked = sorted(clean, key=_score_one, reverse=True)[:12]\n",
    "        base_candidates = [{\n",
    "            \"material_id\": getattr(d, \"material_id\", None),\n",
    "            \"formula\": getattr(d, \"formula_pretty\", None),\n",
    "            \"density\": getattr(d, \"density\", None),\n",
    "            \"formation_energy_per_atom\": getattr(d, \"formation_energy_per_atom\", None),\n",
    "            \"energy_above_hull\": getattr(d, \"energy_above_hull\", None),\n",
    "            \"band_gap\": getattr(d, \"band_gap\", None),\n",
    "            \"elements\": list(getattr(d, \"elements\", []) or []),\n",
    "            \"score_single\": _score_one(d),\n",
    "        } for d in ranked]\n",
    "\n",
    "        payload = {\n",
    "            \"agent\": \"MaterialsAgent\",\n",
    "            \"status\": \"ok\",\n",
    "            \"region\": region,\n",
    "            \"weights\": weights,\n",
    "            \"base_candidates\": base_candidates,\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "\n",
    "        _log(f\"MaterialsAgent: {len(base_candidates)} base candidates (real, filtered)\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.5) MaterialsCombinerAgent — binary/ternary synergy computation\n",
    "# -----------------------------------------------------------------------------\n",
    "class MaterialsCombinerAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        js = _safe_json(text)\n",
    "        base = js.get(\"base_candidates\") or []\n",
    "        weights = js.get(\"weights\") or {\"w1\": 0.4, \"w2\": 0.3, \"w3\": 0.3}\n",
    "        if not base:\n",
    "            raise RuntimeError(\"No base candidates provided for combination.\")\n",
    "\n",
    "        def _prop(d, k):\n",
    "            v = d.get(k, None)\n",
    "            return float(v) if isinstance(v, (int, float, np.floating)) else np.nan\n",
    "\n",
    "        def _chem_penalty(elems: set) -> float:\n",
    "            tox = {\"Pb\", \"As\", \"Cd\", \"Hg\"}\n",
    "            return 0.5 if (elems & tox) else 0.0\n",
    "\n",
    "        combos = []\n",
    "        for i, j in itertools.combinations(range(len(base)), 2):\n",
    "            a, b = base[i], base[j]\n",
    "            elems = set((a.get(\"elements\") or []) + (b.get(\"elements\") or []))\n",
    "            fe = np.nanmean([_prop(a, \"formation_energy_per_atom\"), _prop(b, \"formation_energy_per_atom\")])\n",
    "            rho = np.nanmean([_prop(a, \"density\"), _prop(b, \"density\")])\n",
    "            eh = np.nanmean([_prop(a, \"energy_above_hull\"), _prop(b, \"energy_above_hull\")])\n",
    "            bg = np.nanmean([_prop(a, \"band_gap\"), _prop(b, \"band_gap\")])\n",
    "            w1, w2, w3 = weights[\"w1\"], weights[\"w2\"], weights[\"w3\"]\n",
    "            s = (w1 * -fe) + (w2 * -rho) + (w3 * -eh) + (bg * 0.05) - _chem_penalty(elems)\n",
    "            combos.append({\n",
    "                \"type\": \"binary\",\n",
    "                \"members\": [a[\"formula\"], b[\"formula\"]],\n",
    "                \"props\": {\"formation_energy_per_atom\": fe, \"density\": rho, \"energy_above_hull\": eh, \"band_gap\": bg},\n",
    "                \"synergy_score\": float(s),\n",
    "            })\n",
    "\n",
    "        for i, j, k in itertools.combinations(range(len(base)), 3):\n",
    "            a, b, c = base[i], base[j], base[k]\n",
    "            elems = set((a.get(\"elements\") or []) + (b.get(\"elements\") or []) + (c.get(\"elements\") or []))\n",
    "            fe = np.nanmean([_prop(a, \"formation_energy_per_atom\"), _prop(b, \"formation_energy_per_atom\"), _prop(c, \"formation_energy_per_atom\")])\n",
    "            rho = np.nanmean([_prop(a, \"density\"), _prop(b, \"density\"), _prop(c, \"density\")])\n",
    "            eh = np.nanmean([_prop(a, \"energy_above_hull\"), _prop(b, \"energy_above_hull\"), _prop(c, \"energy_above_hull\")])\n",
    "            bg = np.nanmean([_prop(a, \"band_gap\"), _prop(b, \"band_gap\"), _prop(c, \"band_gap\")])\n",
    "            w1, w2, w3 = weights[\"w1\"], weights[\"w2\"], weights[\"w3\"]\n",
    "            s = (w1 * -fe) + (w2 * -rho) + (w3 * -eh) + (bg * 0.05) - _chem_penalty(elems)\n",
    "            combos.append({\n",
    "                \"type\": \"ternary\",\n",
    "                \"members\": [a[\"formula\"], b[\"formula\"], c[\"formula\"]],\n",
    "                \"props\": {\"formation_energy_per_atom\": fe, \"density\": rho, \"energy_above_hull\": eh, \"band_gap\": bg},\n",
    "                \"synergy_score\": float(s),\n",
    "            })\n",
    "\n",
    "        combos_sorted = sorted(combos, key=lambda x: x[\"synergy_score\"], reverse=True)\n",
    "        payload = {\n",
    "            \"agent\": \"MaterialsCombinerAgent\",\n",
    "            \"status\": \"ok\",\n",
    "            \"weights\": weights,\n",
    "            \"num_combinations\": len(combos_sorted),\n",
    "            \"top_combinations\": combos_sorted[:12],\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "        _log(f\"Combiner: {len(combos_sorted)} combos → top {len(payload['top_combinations'])}\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) SwarmPlannerAgent — multi-drone cooperative planning\n",
    "# -----------------------------------------------------------------------------\n",
    "class SwarmPlannerAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        rng = np.random.default_rng()\n",
    "        n = int(rng.integers(3, 10))\n",
    "        area = rng.choice([\"north_sector\", \"south_sector\", \"ridge_zone\", \"valley_edge\"])\n",
    "        plan = [{\"drone_id\": f\"UAV-{i+1}\", \"sector\": area, \"altitude_m\": round(float(rng.uniform(80, 150)), 1)}\n",
    "                for i in range(n)]\n",
    "        payload = {\"agent\": \"SwarmPlannerAgent\", \"status\": \"ok\", \"num_drones\": n, \"assignments\": plan, \"timestamp\": _utcnow()}\n",
    "        _log(f\"Swarm plan created for {n} drones in {area}\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) SimAgent — containment efficiency simulation\n",
    "# -----------------------------------------------------------------------------\n",
    "class SimAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        rng = np.random.default_rng()\n",
    "        baseline = round(float(rng.uniform(1000, 5000)), 1)\n",
    "        mitigated = round(baseline * float(rng.uniform(0.3, 0.8)), 1)\n",
    "        eff = round((baseline - mitigated) / baseline, 3)\n",
    "        payload = {\"agent\": \"SimAgent\", \"status\": \"ok\", \"baseline_area_ha\": baseline, \"mitigated_area_ha\": mitigated, \"efficiency\": eff, \"timestamp\": _utcnow()}\n",
    "        _log(f\"Simulation: efficiency={eff}, saved={baseline - mitigated:.1f} ha\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) ActuationAgent — mission dispatch\n",
    "# -----------------------------------------------------------------------------\n",
    "class ActuationAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        payload = {\"agent\": \"ActuationAgent\", \"status\": \"executed\", \"timestamp\": _utcnow()}\n",
    "        _log(\"Actuation dispatched\")\n",
    "        return json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) CommAgent — summaries\n",
    "# -----------------------------------------------------------------------------\n",
    "class CommAgent(TextAgent):\n",
    "    def handle(self, text: str = \"\") -> str:\n",
    "        js = _safe_json(text)\n",
    "        agent = js.get(\"agent\")\n",
    "        if agent == \"MaterialsAgent\":\n",
    "            return f\"[Materials] {len(js.get('base_candidates', []))} materials ({js.get('status')})\"\n",
    "        if agent == \"MaterialsCombinerAgent\":\n",
    "            return f\"[Combiner] {js.get('num_combinations')} combos\"\n",
    "        if agent == \"SimAgent\":\n",
    "            return f\"[Simulation] eff={js.get('efficiency')} saved {js.get('baseline_area_ha')}→{js.get('mitigated_area_ha')} ha\"\n",
    "        if agent == \"SwarmPlannerAgent\":\n",
    "            sector = (js.get('assignments') or [{}])[0].get('sector', 'unknown')\n",
    "            return f\"[Swarm] {js.get('num_drones')} drones in {sector}\"\n",
    "        if agent == \"ActuationAgent\":\n",
    "            return \"[Actuation] mission executed successfully\"\n",
    "        return \"[Comm] unrecognized payload\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Instantiate agents\n",
    "# -----------------------------------------------------------------------------\n",
    "materials_context_builder = MaterialsContextBuilder()\n",
    "materials_agent           = MaterialsAgent()\n",
    "materials_combiner_agent  = MaterialsCombinerAgent()\n",
    "swarm_planner_agent       = SwarmPlannerAgent()\n",
    "sim_agent                 = SimAgent()\n",
    "actuation_agent           = ActuationAgent()\n",
    "comm_agent                = CommAgent()\n",
    "\n",
    "_log(\"Framework B agents initialized: MaterialsContextBuilder, MaterialsAgent, MaterialsCombinerAgent, SwarmPlannerAgent, SimAgent, ActuationAgent, CommAgent\")\n",
    "print(\"Framework B ready — operational layer initialized successfully.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Core entrypoint — run_material_framework\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_material_framework(summary_A_text: Union[str, dict], origin: str = \"manual\") -> str:\n",
    "    try:\n",
    "        js = _safe_json(summary_A_text)\n",
    "        region = js.get(\"region\", \"unknown\")\n",
    "        slope  = _get_signal(js, \"slope_mean_deg\", 10.0)\n",
    "\n",
    "        ctx  = json.loads(materials_context_builder.handle(js))\n",
    "        mats = json.loads(materials_agent.handle(ctx))\n",
    "        comb = json.loads(materials_combiner_agent.handle(mats))\n",
    "        plan = json.loads(swarm_planner_agent.handle(comb))\n",
    "        sim  = json.loads(sim_agent.handle(plan))\n",
    "        act  = json.loads(actuation_agent.handle(sim))\n",
    "\n",
    "        summary_text = \" | \".join([\n",
    "            comm_agent.handle(mats),\n",
    "            comm_agent.handle(comb),\n",
    "            comm_agent.handle(plan),\n",
    "            comm_agent.handle(sim),\n",
    "            comm_agent.handle(act),\n",
    "        ])\n",
    "\n",
    "        payload = {\n",
    "            \"agent\": \"MaterialsFramework\",\n",
    "            \"status\": \"ok\",\n",
    "            \"origin\": origin,\n",
    "            \"wildfire_context\": {\"region\": region, \"slope_mean_deg\": slope},\n",
    "            \"weights\": ctx.get(\"context\", {}).get(\"weights\"),\n",
    "            \"num_base_candidates\": len(mats.get(\"base_candidates\") or []),\n",
    "            \"num_combinations\": comb.get(\"num_combinations\"),\n",
    "            \"top_combinations\": comb.get(\"top_combinations\")[:6],\n",
    "            \"summary_text\": summary_text,\n",
    "            \"summary_hash\": hashlib.md5(json.dumps(comb.get(\"top_combinations\")[:6], ensure_ascii=False).encode(\"utf-8\")).hexdigest()[:10],\n",
    "            \"timestamp\": _utcnow()\n",
    "        }\n",
    "\n",
    "        text = \"[Framework B] \" + json.dumps(payload, ensure_ascii=False)\n",
    "        print(f\"Framework B executed successfully for region={region}\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        _log(f\"Framework B error: {type(e).__name__} → {e}\", origin)\n",
    "        traceback.print_exc()\n",
    "        return json.dumps({\"agent\": \"MaterialsFramework\", \"status\": \"error\", \"error_type\": type(e).__name__, \"message\": str(e), \"timestamp\": _utcnow()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3649b-312b-4def-a5fb-153491e88c2e",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 4.5 — AG² System Test & Validation ====\n",
    "\n",
    "Executes a **complete end-to-end validation** of the AG² system to ensure all components  \n",
    "(**Framework A → Coordinator → Framework B**) are operational, synchronized, and producing consistent outputs.\n",
    "\n",
    "It performs:\n",
    "1. Full pipeline execution using the current environment (`WF_REGION`, `WF_DATE_FROM`, `WF_DATE_TO`).\n",
    "2. Validation of structured outputs (`summary_A`, `summary_B`) via Pydantic models.\n",
    "3. Generation of key performance metrics (fire counts, slope mean, materials suggested, efficiency, runtime).\n",
    "4. Tail preview of Coordinator and Framework B logs for rapid debugging.\n",
    "\n",
    "All validation artifacts are automatically versioned and stored under `/reports/history/`.\n",
    "# -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651b76e8-897a-4b79-a54f-2060a4a31da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 AG² System Validation initialized.\n",
      " Using environment region=EUROPE\n",
      "[2025-11-04T19:11:45+00:00][FrameworkB] ══════════ AG² PIPELINE START ══════════\n",
      "[2025-11-04T19:11:45+00:00][FrameworkB] Running Framework A (Wildfire Intelligence)…\n",
      "[2025-11-04T19:11:45+00:00][FrameworkB] Pipeline failed: run_wildfire_framework() got an unexpected keyword argument 'region'\n",
      "[2025-11-04T19:11:45+00:00][FrameworkB] Traceback (most recent call last):\n",
      "  File \"/var/folders/wn/mpnj0v3s1_1fd88s9l4c9pth0000gn/T/ipykernel_1114/2100843501.py\", line 214, in pipeline\n",
      "    A = self.run_A(**kwargs)\n",
      "  File \"/var/folders/wn/mpnj0v3s1_1fd88s9l4c9pth0000gn/T/ipykernel_1114/2100843501.py\", line 137, in run_A\n",
      "    text = globals()[\"run_wildfire_framework\"](**kwargs)\n",
      "TypeError: run_wildfire_framework() got an unexpected keyword argument 'region'\n",
      "[2025-11-04T19:11:45+00:00][FrameworkB] ══════════ AG² PIPELINE END ══════════\n",
      " AG² pipeline failed: run_wildfire_framework() got an unexpected keyword argument 'region'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run_wildfire_framework() got an unexpected keyword argument 'region'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     26\u001b[39m     start = dt.datetime.now(dt.UTC)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     summary_A, summary_B = \u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWF_REGION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGLOBAL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWF_DATE_FROM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown_from\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWF_DATE_TO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown_to\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     duration = (dt.datetime.now(dt.UTC) - start).total_seconds()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m AG² pipeline executed successfully in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mCoordinator.pipeline\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m _log(\u001b[33m\"\u001b[39m\u001b[33m══════════ AG² PIPELINE START ══════════\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     A = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m     B = \u001b[38;5;28mself\u001b[39m.run_B(A)\n\u001b[32m    216\u001b[39m     _log(\u001b[33m\"\u001b[39m\u001b[33mAG² pipeline completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mCoordinator.run_A\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrun_wildfire_framework\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFunction run_wildfire_framework() not found — execute Cell 4.1 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m text = \u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_wildfire_framework\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m data = _extract_json_from_text(text)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m HAS_PYDANTIC:\n",
      "\u001b[31mTypeError\u001b[39m: run_wildfire_framework() got an unexpected keyword argument 'region'"
     ]
    }
   ],
   "source": [
    "# ==== WildfiresAI — Cell 4.5: AG² System Test & Validation ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Environment setup\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "HISTORY_DIR  = REPORTS_DIR / \"history\"\n",
    "LOG_DIR      = REPORTS_DIR / \"logs\"\n",
    "\n",
    "for d in (REPORTS_DIR, HISTORY_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"🔍 AG² System Validation initialized.\")\n",
    "print(f\" Using environment region={os.getenv('WF_REGION', 'GLOBAL')}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) Execute full A→B pipeline\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    start = dt.datetime.now(dt.UTC)\n",
    "    summary_A, summary_B = coordinator.pipeline(\n",
    "        region=os.getenv(\"WF_REGION\", \"GLOBAL\"),\n",
    "        date_from=os.getenv(\"WF_DATE_FROM\", \"unknown_from\"),\n",
    "        date_to=os.getenv(\"WF_DATE_TO\", \"unknown_to\"),\n",
    "    )\n",
    "    duration = (dt.datetime.now(dt.UTC) - start).total_seconds()\n",
    "    print(f\"\\n AG² pipeline executed successfully in {duration:.2f} seconds.\")\n",
    "except Exception as e:\n",
    "    print(f\" AG² pipeline failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) Validate structured outputs\n",
    "# ---------------------------------------------------------------------------\n",
    "def validate_summary(summary_obj: dict, expected_keys: list[str], name: str):\n",
    "    \"\"\"Check presence of expected keys in the summary object.\"\"\"\n",
    "    missing = [k for k in expected_keys if k not in summary_obj]\n",
    "    if missing:\n",
    "        print(f\"  Missing fields in {name}: {missing}\")\n",
    "    else:\n",
    "        print(f\"  {name} structure OK ({len(summary_obj)} keys).\")\n",
    "\n",
    "A_dict = summary_A.model_dump()\n",
    "B_dict = summary_B.model_dump()\n",
    "\n",
    "validate_summary(A_dict, [\"agent\", \"region\", \"window\", \"signals\"], \"Summary A\")\n",
    "validate_summary(B_dict, [\"agent\", \"status\", \"candidates_top3\"], \"Summary B\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Generate validation report\n",
    "# ---------------------------------------------------------------------------\n",
    "fires = (A_dict.get(\"counts\") or {}).get(\"fires\", \"?\")\n",
    "slope = (A_dict.get(\"signals\") or {}).get(\"slope_mean_deg\", \"?\")\n",
    "mats  = len(B_dict.get(\"candidates_top3\") or [])\n",
    "eff   = (B_dict.get(\"wildfire_context\") or {}).get(\"efficiency\", \"?\")\n",
    "\n",
    "summary_report = {\n",
    "    \"run_id\": coordinator.run_id,\n",
    "    \"region\": A_dict.get(\"region\", \"?\"),\n",
    "    \"fires_detected\": fires,\n",
    "    \"slope_mean_deg\": slope,\n",
    "    \"materials_suggested\": mats,\n",
    "    \"efficiency\": eff,\n",
    "    \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "    \"execution_seconds\": duration,\n",
    "}\n",
    "\n",
    "report_path = HISTORY_DIR / f\"{coordinator.run_id}_validation_summary.json\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary_report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n🧾 Validation summary written to: {report_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Display condensed results\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
    "print(\" AG² Validation Summary\")\n",
    "print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
    "print(f\" Region: {summary_report['region']}\")\n",
    "print(f\" Fires detected: {summary_report['fires_detected']}\")\n",
    "print(f\" Mean slope: {summary_report['slope_mean_deg']}°\")\n",
    "print(f\" Materials suggested: {summary_report['materials_suggested']}\")\n",
    "print(f\" Containment efficiency: {summary_report['efficiency']}\")\n",
    "print(f\" Runtime: {summary_report['execution_seconds']:.2f}s\")\n",
    "print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6) Log tail preview\n",
    "# ---------------------------------------------------------------------------\n",
    "def tail_log(path: Path, n: int = 10):\n",
    "    \"\"\"Show last N lines of a log file (if exists).\"\"\"\n",
    "    if path.exists():\n",
    "        lines = path.read_text(encoding=\"utf-8\").splitlines()[-n:]\n",
    "        print(f\"\\n Last {n} lines from {path.name}:\")\n",
    "        for line in lines:\n",
    "            print(\" \", line)\n",
    "    else:\n",
    "        print(f\"(No log found at {path})\")\n",
    "\n",
    "tail_log(LOG_DIR / \"coordinator.log\")\n",
    "tail_log(LOG_DIR / \"framework_b.log\")\n",
    "\n",
    "print(\"\\n AG² System Validation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac60bb1-632b-476d-b6aa-70c682e0354e",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------\n",
    "## ==== Cell 5 — AG² Interactive Command Console ====\n",
    "\n",
    "Implements the interactive human–AI interface for the AG² System.\n",
    "\n",
    "It bridges human operators with the dual frameworks:\n",
    "- **Framework A (Wildfire Intelligence)** — environmental and risk analytics.  \n",
    "- **Framework B (Materials Intelligence)** — material response and containment planning.  \n",
    "- **LLM Orchestrator (Cell 4.3)** — interprets natural-language intent and activates the appropriate framework(s).\n",
    "\n",
    "### Operational Flow\n",
    "Human → Query → LLM Assistant → Orchestrator → Coordinator → Framework A → Framework B → Report\n",
    "\n",
    "### Capabilities\n",
    "- Context-aware interpretation via GPT-5.  \n",
    "- Automatic region and timeframe detection.  \n",
    "- Persistent audit logging in `/reports/chat_log.json`.  \n",
    "- Fully functional for field or command-center deployment.\n",
    "\n",
    "All interactions, logs, and reports are versioned and traceable.\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379e767-e6b8-4a89-8312-5ee2ad438db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 5 — AG² Interactive Command Console (LLM + Map, Field-Ready) ====\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# GIS (map rendering)\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1) Environment & directories\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path.cwd())\n",
    "REPORTS_DIR  = PROJECT_ROOT / \"reports\"\n",
    "MAPS_DIR     = REPORTS_DIR / \"maps\"\n",
    "CHAT_LOG     = REPORTS_DIR / \"chat_log.json\"\n",
    "for d in (REPORTS_DIR, MAPS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# These should already be defined in previous cells (4.1/4.2), but default safely:\n",
    "WF_REGION     = globals().get(\"WF_REGION\", os.getenv(\"WF_REGION\", \"GLOBAL\")).upper()\n",
    "WF_DATE_FROM  = globals().get(\"WF_DATE_FROM\", os.getenv(\"WF_DATE_FROM\", \"unknown_from\"))\n",
    "WF_DATE_TO    = globals().get(\"WF_DATE_TO\", os.getenv(\"WF_DATE_TO\", \"unknown_to\"))\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USE_LLM = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2) Region / timeframe detection (fallback heuristics if LLM is off)\n",
    "# ---------------------------------------------------------------------------\n",
    "def detect_region(question: str) -> str:\n",
    "    \"\"\"Heuristic region extractor for offline mode.\"\"\"\n",
    "    q = question.lower()\n",
    "    mapping = {\n",
    "        \"spain\": \"SPAIN\", \"españa\": \"SPAIN\", \"portugal\": \"PORTUGAL\",\n",
    "        \"france\": \"FRANCE\", \"italy\": \"ITALY\", \"usa\": \"USA\",\n",
    "        \"united states\": \"USA\", \"canada\": \"CANADA\",\n",
    "        \"australia\": \"AUSTRALIA\", \"europe\": \"EUROPE\",\n",
    "        \"africa\": \"AFRICA\", \"asia\": \"ASIA\", \"global\": \"GLOBAL\"\n",
    "    }\n",
    "    for k, v in mapping.items():\n",
    "        if k in q:\n",
    "            return v\n",
    "    return \"GLOBAL\"\n",
    "\n",
    "def detect_days(question: str) -> int:\n",
    "    \"\"\"Approximate time window in days from natural language cues.\"\"\"\n",
    "    q = question.lower()\n",
    "    if \"today\" in q or \"hoy\" in q: return 1\n",
    "    if \"yesterday\" in q or \"ayer\" in q: return 2\n",
    "    if \"week\" in q or \"semana\" in q: return 7\n",
    "    if \"month\" in q or \"mes\" in q: return 30\n",
    "    if \"year\" in q or \"año\" in q: return 365\n",
    "    return 7\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3) LLM assistant (uses GPT-5 when available)\n",
    "# ---------------------------------------------------------------------------\n",
    "def llm_refine_query(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use GPT-5 to convert user's natural language question into a structured command.\n",
    "    Returns JSON: {intent, region, period_days, parameters?}\n",
    "    \"\"\"\n",
    "    if not USE_LLM:\n",
    "        return {\n",
    "            \"intent\": \"analyze_wildfires\",\n",
    "            \"region\": detect_region(question),\n",
    "            \"period_days\": detect_days(question),\n",
    "            \"context\": \"heuristic\",\n",
    "            \"confidence\": 0.6,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        system_prompt = (\n",
    "            \"You are AG² Command LLM for WildfiresAI. \"\n",
    "            \"Given a human query, output ONLY JSON with keys: \"\n",
    "            \"{intent, region, period_days, parameters}. \"\n",
    "            \"intent∈{analyze_wildfires, materials_query, both}. \"\n",
    "            \"Return valid JSON only.\"\n",
    "        )\n",
    "        user_prompt = f\"Query: {question}\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        parsed = json.loads(text)\n",
    "        parsed[\"context\"] = \"llm\"\n",
    "        parsed[\"confidence\"] = 0.95\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        print(f\"LLM failed ({e}); using heuristic fallback.\")\n",
    "        return {\n",
    "            \"intent\": \"analyze_wildfires\",\n",
    "            \"region\": detect_region(question),\n",
    "            \"period_days\": detect_days(question),\n",
    "            \"context\": \"fallback\",\n",
    "            \"confidence\": 0.5,\n",
    "        }\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4) Data loading + map rendering (robust to schema variants)\n",
    "# ---------------------------------------------------------------------------\n",
    "def _find_processed_file(region: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Locate a plausible processed parquet: strict name first, then newest match.\n",
    "    \"\"\"\n",
    "    strict = PROCESSED_DIR / f\"fires_terrain_{region}_{WF_DATE_FROM}_{WF_DATE_TO}.parquet\"\n",
    "    if strict.exists():\n",
    "        return strict\n",
    "\n",
    "    candidates = sorted(\n",
    "        PROCESSED_DIR.glob(f\"fires_terrain_{region}*.parquet\"),\n",
    "        key=lambda p: p.stat().st_mtime,\n",
    "        reverse=True,\n",
    "    )\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def _normalize_latlon(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Normalize lat/lon columns to ['lat','lon'] and drop NA rows for mapping.\n",
    "    Accepts columns {lat/lon} or {latitude/longitude}.\n",
    "    \"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    lat = cols.get(\"lat\") or cols.get(\"latitude\")\n",
    "    lon = cols.get(\"lon\") or cols.get(\"longitude\")\n",
    "    if not lat or not lon:\n",
    "        return None\n",
    "    out = df.rename(columns={lat: \"lat\", lon: \"lon\"})\n",
    "    out = out[[\"lat\", \"lon\"]].dropna()\n",
    "    return out\n",
    "\n",
    "def render_heatmap_from_processed(region: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Build a folium heatmap from the latest processed parquet. Returns HTML path.\n",
    "    If no data is available, returns None.\n",
    "    \"\"\"\n",
    "    path = _find_processed_file(region)\n",
    "    if not path or not path.exists():\n",
    "        print(\"Map: no processed parquet found.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        points = _normalize_latlon(df)\n",
    "        if points is None or points.empty:\n",
    "            print(\"Map: no suitable lat/lon columns for rendering.\")\n",
    "            return None\n",
    "\n",
    "        m = folium.Map(location=[points[\"lat\"].mean(), points[\"lon\"].mean()], zoom_start=5, control_scale=True)\n",
    "        HeatMap(points[[\"lat\", \"lon\"]].values.tolist(), radius=9, blur=15, max_zoom=8).add_to(m)\n",
    "\n",
    "        ts = dt.datetime.now(dt.UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_html = MAPS_DIR / f\"ag2_map_{region}_{ts}.html\"\n",
    "        m.save(out_html)\n",
    "        print(f\"Map saved to {out_html}\")\n",
    "        return out_html\n",
    "    except Exception as e:\n",
    "        print(f\"Map rendering failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5) Human-readable summarizer for A→B results\n",
    "# ---------------------------------------------------------------------------\n",
    "def summarize_agents(A: dict, B: dict) -> str:\n",
    "    slope = (A.get(\"signals\") or {}).get(\"slope_mean_deg\", \"?\")\n",
    "    fires = (A.get(\"counts\")  or {}).get(\"fires\", \"?\")\n",
    "    mats  = len(B.get(\"candidates_top3\", []))\n",
    "    # Efficiency is optional; only include if present in B\n",
    "    eff = (B.get(\"wildfire_context\") or {}).get(\"efficiency\")\n",
    "    base = f\"[AG2] Region={A.get('region','?')} | Fires={fires} | Slope≈{slope}° | Materials={mats}\"\n",
    "    return base if eff is None else f\"{base} | Efficiency={eff}\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6) Interactive loop — LLM-enhanced + map\n",
    "# ---------------------------------------------------------------------------\n",
    "def ag2_chat():\n",
    "    \"\"\"Interactive console with LLM orchestration and map output.\"\"\"\n",
    "    print(\"AG2 Interactive Console (LLM + Map) ready.\")\n",
    "    print(\"Type your question (e.g., 'Fires in Spain last month') or 'exit' to quit.\")\n",
    "\n",
    "    chat_history = []\n",
    "    while True:\n",
    "        q = input(\"\\nAsk WildfiresAI: \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Session ended.\")\n",
    "            break\n",
    "\n",
    "        # 6.1 Interpret query with LLM (or heuristics)\n",
    "        print(\"Interpreting intent via LLM Orchestrator...\")\n",
    "        intent_info = llm_refine_query(q)\n",
    "        region = (intent_info.get(\"region\") or WF_REGION).upper()\n",
    "        period = int(intent_info.get(\"period_days\", 7))\n",
    "        intent = intent_info.get(\"intent\", \"analyze_wildfires\")\n",
    "        print(f\"→ Intent: {intent} | Region: {region} | Period: {period} days\")\n",
    "\n",
    "        # 6.2 Compute window from period_days\n",
    "        end_dt = dt.datetime.now(dt.UTC).date()\n",
    "        start_dt = end_dt - dt.timedelta(days=period)\n",
    "        date_from = start_dt.isoformat()\n",
    "        date_to   = end_dt.isoformat()\n",
    "\n",
    "        # 6.3 Run AG² pipeline end-to-end (Coordinator → A → B)\n",
    "        print(\"Running AG2 pipeline...\")\n",
    "        t0 = dt.datetime.now(dt.UTC)\n",
    "        try:\n",
    "            A, B = coordinator.pipeline(region=region, date_from=date_from, date_to=date_to)\n",
    "            summary = summarize_agents(A.model_dump(), B.model_dump())\n",
    "            print(f\"\\n{summary}\")\n",
    "\n",
    "            # 6.4 Render map (best effort)\n",
    "            map_path = render_heatmap_from_processed(region)\n",
    "\n",
    "            # 6.5 Build response object\n",
    "            record = {\n",
    "                \"question\": q,\n",
    "                \"intent\": intent,\n",
    "                \"region\": region,\n",
    "                \"period_days\": period,\n",
    "                \"llm_context\": intent_info.get(\"context\", \"none\"),\n",
    "                \"confidence\": intent_info.get(\"confidence\", 0),\n",
    "                \"answer\": summary,\n",
    "                \"map_html\": str(map_path) if map_path else None,\n",
    "                \"date_from\": date_from,\n",
    "                \"date_to\": date_to,\n",
    "                \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "            }\n",
    "            chat_history.append(record)\n",
    "\n",
    "            if map_path:\n",
    "                print(\"If the map does not render, trust this Notebook: File → Trust Notebook\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline error: {e}\")\n",
    "            chat_history.append({\n",
    "                \"question\": q,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n",
    "            })\n",
    "\n",
    "        # 6.6 Timing\n",
    "        t1 = dt.datetime.now(dt.UTC)\n",
    "        print(f\"Completed in {(t1 - t0).total_seconds():.2f}s.\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "    # 6.7 Persist full conversation\n",
    "    with open(CHAT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chat_history, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Chat history saved to {CHAT_LOG}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7) Launch console\n",
    "# ---------------------------------------------------------------------------\n",
    "ag2_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4739c-cf02-4ac2-b4b1-591ab2372987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WildfiresAI)",
   "language": "python",
   "name": "wildfiresai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
